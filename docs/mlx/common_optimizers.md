Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Usage Pattern (curated) \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx import mlx.nn as nn from mlx.optimizers import AdamW, clip\\\\\\\_grad\\\\\\\_norm, cosine\\\\\\\_decay net = nn.Sequential(nn.Linear(32, 64), nn.GELU(), nn.Linear(64, 10)) params = net.parameters() opt = AdamW(cosine\\\\\\\_decay(3e-4, total\\\\\\\_steps=10000)) def loss\\\\\\\_fn(p, x, y): logits = net.apply(p, x) return mx.mean((logits - y) \\\\\\\*\\\\\\\* 2) x = mx.random.normal((128, 32)); y = mx.random.normal((128, 10)) loss, grads = mx.value\\\\\\\_and\\\\\\\_grad(loss\\\\\\\_fn)(params, x, y) grads, \\\\\\\_ = clip\\\\\\\_grad\\\\\\\_norm(grads, 1.0) # unpack tuple params = opt.update(params, grads) \\\\\\\`\\\\\\\`\\\\\\\` ### More Examples \\\\\\\`\\\\\\\`\\\\\\\`python # SGD with momentum and weight decay from mlx.optimizers import SGD opt = SGD(1e-2, momentum=0.9, weight\\\\\\\_decay=1e-4) # Freezing a subtree (e.g., encoder) by zeroing grads from mlx.utils import tree\\\\\\\_map\\\\\\\_with\\\\\\\_path def zero\\\\\\\_frozen(path, g): return mx.zeros\\\\\\\_like(g) if path and path\\\\\\\[0\\\\\\\] == "encoder" else g loss, grads = mx.value\\\\\\\_and\\\\\\\_grad(loss\\\\\\\_fn)(params, x, y) grads = tree\\\\\\\_map\\\\\\\_with\\\\\\\_path(zero\\\\\\\_frozen, grads) params = opt.update(params, grads) \\\\\\\`\\\\\\\`\\\\\\\` \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers/common\\\_optimizers.rst "Download source file") - .pdf # Common Optimizers # Common Optimizers | | | |----|----| | \\\[\\\`\\\`SGD\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.SGD.html#mlx.optimizers.SGD "mlx.optimizers.SGD")(learning\\\\\\\_rate\\\\\\\\\\\\\\\[, momentum, weight\\\\\\\_decay, ...\\\\\\\\\\\\\\\]) | The stochastic gradient descent optimizer. | | \\\[\\\`\\\`RMSprop\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.RMSprop.html#mlx.optimizers.RMSprop "mlx.optimizers.RMSprop")(learning\\\\\\\_rate\\\\\\\\\\\\\\\[, alpha, eps\\\\\\\\\\\\\\\]) | The RMSprop optimizer \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\]. | | \\\[\\\`\\\`Adagrad\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adagrad.html#mlx.optimizers.Adagrad "mlx.optimizers.Adagrad")(learning\\\\\\\_rate\\\\\\\\\\\\\\\[, eps\\\\\\\\\\\\\\\]) | The Adagrad optimizer \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\]. | | \\\[\\\`\\\`Adafactor\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adafactor.html#mlx.optimizers.Adafactor "mlx.optimizers.Adafactor")(\\\\\\\\\\\\\\\[learning\\\\\\\_rate, eps, ...\\\\\\\\\\\\\\\]) | The Adafactor optimizer. | | \\\[\\\`\\\`AdaDelta\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.AdaDelta.html#mlx.optimizers.AdaDelta "mlx.optimizers.AdaDelta")(learning\\\\\\\_rate\\\\\\\\\\\\\\\[, rho, eps\\\\\\\\\\\\\\\]) | The AdaDelta optimizer with a learning rate \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\]. | | \\\[\\\`\\\`Adam\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adam.html#mlx.optimizers.Adam "mlx.optimizers.Adam")(learning\\\\\\\_rate\\\\\\\\\\\\\\\[, betas, eps, ...\\\\\\\\\\\\\\\]) | The Adam optimizer \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\]. | | \\\[\\\`\\\`AdamW\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.AdamW.html#mlx.optimizers.AdamW "mlx.optimizers.AdamW")(learning\\\\\\\_rate\\\\\\\\\\\\\\\[, betas, eps, ...\\\\\\\\\\\\\\\]) | The AdamW optimizer \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\]. | | \\\[\\\`\\\`Adamax\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adamax.html#mlx.optimizers.Adamax "mlx.optimizers.Adamax")(learning\\\\\\\_rate\\\\\\\\\\\\\\\[, betas, eps\\\\\\\\\\\\\\\]) | The Adamax optimizer, a variant of Adam based on the infinity norm \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\]. | | \\\[\\\`\\\`Lion\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Lion.html#mlx.optimizers.Lion "mlx.optimizers.Lion")(learning\\\\\\\_rate\\\\\\\\\\\\\\\[, betas, weight\\\\\\\_decay\\\\\\\\\\\\\\\]) | The Lion optimizer \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\]. | | \\\[\\\`\\\`MultiOptimizer\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.MultiOptimizer.html#mlx.optimizers.MultiOptimizer "mlx.optimizers.MultiOptimizer")(optimizers\\\\\\\\\\\\\\\[, filters\\\\\\\\\\\\\\\]) | Wraps a list of optimizers with corresponding weight predicates/filters to make it easy to use different optimizers for different weights. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.update.html "previous page") previous mlx.optimizers.Optimizer.update \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.SGD.html "next page") next mlx.optimizers.SGD By MLX Contributors © Copyright 2023, MLX Contributors.
