Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - Many activations have both module and function forms; choose by preference (no parameters -> function is convenient). - \\\\\\\`glu(x, axis=...)\\\\\\\`: validate that the chosen \\\\\\\`axis\\\\\\\` can be split evenly into two gates. - Approximate variants (e.g., \\\\\\\`gelu\\\\\\\_approx\\\\\\\`) trade accuracy for speed; verify on your domain. ### Examples \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx import mlx.nn as nn x = mx.random.normal((2, 6)) print(nn.relu(x)) # GLU splits features along axis and gates half by sigmoid(other half) g = nn.glu(x, axis=-1) # requires last dim even (splits into 3+3) print(g.shape) # Function vs Module forms relu\\\\\\\_mod = nn.ReLU() print(relu\\\\\\\_mod(x)) \\\\\\\`\\\\\\\`\\\\\\\` \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/nn/functions.rst "Download source file") - .pdf # Functions # Functions Layers without parameters (e.g. activation functions) are also provided as simple functions. | | | |----|----| | \\\[\\\`\\\`elu\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.elu.html#mlx.nn.elu "mlx.nn.elu") | elu(x, alpha=1.0) | | \\\[\\\`\\\`celu\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.celu.html#mlx.nn.celu "mlx.nn.celu") | celu(x, alpha=1.0) | | \\\[\\\`\\\`gelu\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.gelu.html#mlx.nn.gelu "mlx.nn.gelu") | gelu(x) -\\\\\\\\> mlx.core.array | | \\\[\\\`\\\`gelu\\\_approx\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.gelu\\\_approx.html#mlx.nn.gelu\\\_approx "mlx.nn.gelu\\\_approx") | gelu\\\\\\\_approx(x) | | \\\[\\\`\\\`gelu\\\_fast\\\_approx\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.gelu\\\_fast\\\_approx.html#mlx.nn.gelu\\\_fast\\\_approx "mlx.nn.gelu\\\_fast\\\_approx") | gelu\\\\\\\_fast\\\\\\\_approx(x) | | \\\[\\\`\\\`glu\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.glu.html#mlx.nn.glu "mlx.nn.glu")(x\\\\\\\\\\\\\\\[, axis\\\\\\\\\\\\\\\]) | Applies the gated linear unit function. | | \\\[\\\`\\\`hard\\\_shrink\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.hard\\\_shrink.html#mlx.nn.hard\\\_shrink "mlx.nn.hard\\\_shrink") | hard\\\\\\\_shrink(x, lambd=0.5) | | \\\[\\\`\\\`hard\\\_tanh\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.hard\\\_tanh.html#mlx.nn.hard\\\_tanh "mlx.nn.hard\\\_tanh") | hard\\\\\\\_tanh(x, min\\\\\\\_val=-1.0, max\\\\\\\_val=1.0) | | \\\[\\\`\\\`hardswish\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.hardswish.html#mlx.nn.hardswish "mlx.nn.hardswish") | hardswish(x) | | \\\[\\\`\\\`leaky\\\_relu\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.leaky\\\_relu.html#mlx.nn.leaky\\\_relu "mlx.nn.leaky\\\_relu") | leaky\\\\\\\_relu(x, negative\\\\\\\_slope=0.01) | | \\\[\\\`\\\`log\\\_sigmoid\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.log\\\_sigmoid.html#mlx.nn.log\\\_sigmoid "mlx.nn.log\\\_sigmoid") | log\\\\\\\_sigmoid(x) | | \\\[\\\`\\\`log\\\_softmax\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.log\\\_softmax.html#mlx.nn.log\\\_softmax "mlx.nn.log\\\_softmax") | log\\\\\\\_softmax(x, axis=-1) | | \\\[\\\`\\\`mish\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.mish.html#mlx.nn.mish "mlx.nn.mish") | mlx.core.array) -\\\\\\\\> mlx.core.array | | \\\[\\\`\\\`prelu\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.prelu.html#mlx.nn.prelu "mlx.nn.prelu") | mlx.core.array) -\\\\\\\\> mlx.core.array | | \\\[\\\`\\\`relu\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.relu.html#mlx.nn.relu "mlx.nn.relu") | relu(x) | | \\\[\\\`\\\`relu6\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.relu6.html#mlx.nn.relu6 "mlx.nn.relu6") | relu6(x) | | \\\[\\\`\\\`selu\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.selu.html#mlx.nn.selu "mlx.nn.selu") | selu(x) | | \\\[\\\`\\\`sigmoid\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.sigmoid.html#mlx.nn.sigmoid "mlx.nn.sigmoid") | sigmoid(x) | | \\\[\\\`\\\`silu\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.silu.html#mlx.nn.silu "mlx.nn.silu") | silu(x) | | \\\[\\\`\\\`softmax\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.softmax.html#mlx.nn.softmax "mlx.nn.softmax") | softmax(x, axis=-1) | | \\\[\\\`\\\`softmin\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.softmin.html#mlx.nn.softmin "mlx.nn.softmin") | softmin(x, axis=-1) | | \\\[\\\`\\\`softplus\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.softplus.html#mlx.nn.softplus "mlx.nn.softplus") | softplus(x) | | \\\[\\\`\\\`softshrink\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.softshrink.html#mlx.nn.softshrink "mlx.nn.softshrink") | float = 0.5) | | \\\[\\\`\\\`step\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.step.html#mlx.nn.step "mlx.nn.step") | float = 0.0) | | \\\[\\\`\\\`tanh\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.tanh.html#mlx.nn.tanh "mlx.nn.tanh")(x) | Applies the hyperbolic tangent function. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Upsample.html "previous page") previous mlx.nn.Upsample \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.elu.html "next page") next mlx.nn.elu By MLX Contributors © Copyright 2023, MLX Contributors.
