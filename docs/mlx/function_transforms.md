Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - \\\\\\\`mx.value\\\\\\\_and\\\\\\\_grad(fn)\\\\\\\` is the workhorse for training: returns \\\\\\\`(loss, grads)\\\\\\\` in one pass. - \\\\\\\`mx.vmap(fn, in\\\\\\\_axes=..., out\\\\\\\_axes=...)\\\\\\\` helps batch a pure function without writing loops; align axes explicitly. - Compose transforms: \\\\\\\`mx.vmap(mx.grad(fn))\\\\\\\` is valid when \\\\\\\`fn\\\\\\\` is scalar‑valued per input. ### Examples \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx # value\\\\\\\_and\\\\\\\_grad for training def f(w, x): return mx.mean((x @ w) \\\\\\\*\\\\\\\* 2) w = mx.random.normal((16,)) x = mx.random.normal((32, 16)) loss, grad = mx.value\\\\\\\_and\\\\\\\_grad(lambda w: f(w, x))(w) # vmap over a scalar-valued function def scalar\\\\\\\_fn(a): return mx.sum(a \\\\\\\* a) batched = mx.vmap(scalar\\\\\\\_fn) A = mx.random.normal((8, 4)) vals = batched(A) # shape (8,) # Compose vmap and grad df = mx.grad(scalar\\\\\\\_fn) df\\\\\\\_batched = mx.vmap(df) grads = df\\\\\\\_batched(A) \\\\\\\`\\\\\\\`\\\\\\\` ## Function Basics (lazy, device‑agnostic) - Define plain Python functions that operate on \\\\\\\`mx.array\\\\\\\` values; MLX records a graph lazily and executes on eval. - No \\\\\\\`device=\\\\\\\` keyword on ops; control placement with default device/\\\\\\\`stream\\\\\\\`. \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx def my\\\\\\\_sum(x, y): return x + y a = mx.array(\\\\\\\[1, 2, 3\\\\\\\]) b = mx.array(\\\\\\\[4, 5, 6\\\\\\\]) c = my\\\\\\\_sum(a, b) mx.eval(c) print(c) # array(\\\\\\\[5, 7, 9\\\\\\\], dtype=int32) \\\\\\\`\\\\\\\`\\\\\\\` ## \\\\\\\`mx.grad\\\\\\\` Returns a new function computing the gradient w.r.t. the first argument by default. \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx def mse\\\\\\\_loss(w, x, y): return mx.mean((w \\\\\\\* x - y) \\\\\\\*\\\\\\\* 2) grad\\\\\\\_fn = mx.grad(mse\\\\\\\_loss) w = mx.array(1.0) x = mx.array(\\\\\\\[0.5, -0.5\\\\\\\]) y = mx.array(\\\\\\\[1.5, -1.5\\\\\\\]) g = grad\\\\\\\_fn(w, x, y) mx.eval(g) print(g) \\\\\\\`\\\\\\\`\\\\\\\` ## \\\\\\\`mx.value\\\\\\\_and\\\\\\\_grad\\\\\\\` Computes value and gradient in one pass for efficiency. \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx def mse\\\\\\\_loss(w, x, y): return mx.mean((w \\\\\\\* x - y) \\\\\\\*\\\\\\\* 2) vag = mx.value\\\\\\\_and\\\\\\\_grad(mse\\\\\\\_loss) w = mx.array(1.0) x = mx.array(\\\\\\\[0.5, -0.5\\\\\\\]) y = mx.array(\\\\\\\[1.5, -1.5\\\\\\\]) loss, grad\\\\\\\_w = vag(w, x, y) mx.eval(loss, grad\\\\\\\_w) print(loss, grad\\\\\\\_w) \\\\\\\`\\\\\\\`\\\\\\\` ## \\\\\\\`mx.vmap\\\\\\\` Vectorizes a function across batch dimensions without Python loops. \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx def my\\\\\\\_function(x): return x \\\\\\\* 2 batched\\\\\\\_x = mx.array(\\\\\\\[\\\\\\\[1, 2\\\\\\\], \\\\\\\[3, 4\\\\\\\]\\\\\\\]) vf = mx.vmap(my\\\\\\\_function) out = vf(batched\\\\\\\_x) mx.eval(out) print(out) \\\\\\\`\\\\\\\`\\\\\\\` ## \\\\\\\`mx.compile\\\\\\\` JIT‑compiles a function; first call traces+compiles, subsequent calls with same signature are fast. See the Compile guide for benchmarks and caveats. \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx import time def fun(x, y): return mx.exp(-x) + y x, y = mx.array(1.0), mx.array(2.0) t0 = time.time(); r0 = fun(x, y); mx.eval(r0); t1 = time.time() cf = mx.compile(fun) t2 = time.time(); r1 = cf(x, y); mx.eval(r1); t3 = time.time() t4 = time.time(); r2 = cf(x, y); mx.eval(r2); t5 = time.time() print(f"regular={t1-t0:.6f}s first={t3-t2:.6f}s cached={t5-t4:.6f}s") \\\\\\\`\\\\\\\`\\\\\\\` ## \\\\\\\`mx.custom\\\\\\\_function\\\\\\\` (advanced) Wraps a function with custom differentiation rules (e.g., custom VJP/JVP) when the analytic reverse/forward is cheaper than the default. See Custom Extensions for end‑to‑end examples. \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/usage/function\\\_transforms.rst "Download source file") - .pdf # Function Transforms ## Contents \\\\- \\\[Automatic Differentiation\\\](https://ml-explore.github.io/mlx/build/html/#automatic-differentiation) - \\\[Automatic Vectorization\\\](https://ml-explore.github.io/mlx/build/html/#automatic-vectorization) # Function Transforms MLX uses composable function transformations for automatic differentiation, vectorization, and compute graph optimizations. To see the complete list of function transformations check-out the \\\[API documentation\\\](https://ml-explore.github.io/mlx/build/html/python/transforms.html#transforms). The key idea behind composable function transformations is that every transformation returns a function which can be further transformed. Here is a simple example: \\\`\\\`\\\`shell > dfdx = mx.grad(mx.sin) > dfdx(mx.array(mx.pi)) array(-1, dtype=float32) > mx.cos(mx.array(mx.pi)) array(-1, dtype=float32) \\\`\\\`\\\` The output of \\\[\\\`\\\`grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.grad.html#mlx.core.grad "mlx.core.grad") on \\\[\\\`\\\`sin()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.sin.html#mlx.core.sin "mlx.core.sin") is simply another function. In this case it is the gradient of the sine function which is exactly the cosine function. To get the second derivative you can do: \\\`\\\`\\\`shell > d2fdx2 = mx.grad(mx.grad(mx.sin)) > d2fdx2(mx.array(mx.pi / 2)) array(-1, dtype=float32) > mx.sin(mx.array(mx.pi / 2)) array(1, dtype=float32) \\\`\\\`\\\` Using \\\[\\\`\\\`grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.grad.html#mlx.core.grad "mlx.core.grad") on the output of \\\[\\\`\\\`grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.grad.html#mlx.core.grad "mlx.core.grad") is always ok. You keep getting higher order derivatives. Any of the MLX function transformations can be composed in any order to any depth. See the following sections for more information on \\\[automatic differentiation\\\](https://ml-explore.github.io/mlx/build/html/#auto-diff) and \\\[automatic vectorization\\\](https://ml-explore.github.io/mlx/build/html/#vmap). For more information on \\\[\\\`\\\`compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html#mlx.core.compile "mlx.core.compile") see the \\\[compile documentation\\\](https://ml-explore.github.io/mlx/build/html/usage/compile.html#compile). ## Automatic Differentiation Automatic differentiation in MLX works on functions rather than on implicit graphs. Note If you are coming to MLX from PyTorch, you no longer need functions like \\\`\\\\\\\`backward\\\\\\\`\\\`, \\\`\\\\\\\`zero\\\\\\\_grad\\\\\\\`\\\`, and \\\`\\\\\\\`detach\\\\\\\`\\\`, or properties like \\\`\\\\\\\`requires\\\\\\\_grad\\\\\\\`\\\`. The most basic example is taking the gradient of a scalar-valued function as we saw above. You can use the \\\[\\\`\\\`grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.grad.html#mlx.core.grad "mlx.core.grad") and \\\[\\\`\\\`value\\\_and\\\_grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.value\\\_and\\\_grad.html#mlx.core.value\\\_and\\\_grad "mlx.core.value\\\_and\\\_grad") function to compute gradients of more complex functions. By default these functions compute the gradient with respect to the first argument: \\\`\\\`\\\`python def loss\\\_fn(w, x, y): return mx.mean(mx.square(w \\\* x - y)) w = mx.array(1.0) x = mx.array(\\\[0.5, -0.5\\\]) y = mx.array(\\\[1.5, -1.5\\\]) # Computes the gradient of loss\\\_fn with respect to w: grad\\\_fn = mx.grad(loss\\\_fn) dloss\\\_dw = grad\\\_fn(w, x, y) # Prints array(-1, dtype=float32) print(dloss\\\_dw) # To get the gradient with respect to x we can do: grad\\\_fn = mx.grad(loss\\\_fn, argnums=1) dloss\\\_dx = grad\\\_fn(w, x, y) # Prints array(\\\[-1, 1\\\], dtype=float32) print(dloss\\\_dx) \\\`\\\`\\\` One way to get the loss and gradient is to call \\\`\\\\\\\`loss\\\\\\\_fn\\\\\\\`\\\` followed by \\\`\\\\\\\`grad\\\\\\\_fn\\\\\\\`\\\`, but this can result in a lot of redundant work. Instead, you should use \\\[\\\`\\\`value\\\_and\\\_grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.value\\\_and\\\_grad.html#mlx.core.value\\\_and\\\_grad "mlx.core.value\\\_and\\\_grad"). Continuing the above example: \\\`\\\`\\\`python # Computes the gradient of loss\\\_fn with respect to w: loss\\\_and\\\_grad\\\_fn = mx.value\\\_and\\\_grad(loss\\\_fn) loss, dloss\\\_dw = loss\\\_and\\\_grad\\\_fn(w, x, y) # Prints array(1, dtype=float32) print(loss) # Prints array(-1, dtype=float32) print(dloss\\\_dw) \\\`\\\`\\\` You can also take the gradient with respect to arbitrarily nested Python containers of arrays (specifically any of \\\[\\\`\\\`list\\\`\\\`\\\](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)"), \\\[\\\`\\\`tuple\\\`\\\`\\\](https://docs.python.org/3/library/stdtypes.html#tuple "(in Python v3.13)"), or \\\[\\\`\\\`dict\\\`\\\`\\\](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")). Suppose we wanted a weight and a bias parameter in the above example. A nice way to do that is the following: \\\`\\\`\\\`python def loss\\\_fn(params, x, y): w, b = params\\\["weight"\\\], params\\\["bias"\\\] h = w \\\* x + b return mx.mean(mx.square(h - y)) params = {"weight": mx.array(1.0), "bias": mx.array(0.0)} x = mx.array(\\\[0.5, -0.5\\\]) y = mx.array(\\\[1.5, -1.5\\\]) # Computes the gradient of loss\\\_fn with respect to both the # weight and bias: grad\\\_fn = mx.grad(loss\\\_fn) grads = grad\\\_fn(params, x, y) # Prints # {'weight': array(-1, dtype=float32), 'bias': array(0, dtype=float32)} print(grads) \\\`\\\`\\\` Notice the tree structure of the parameters is preserved in the gradients. In some cases you may want to stop gradients from propagating through a part of the function. You can use the \\\[\\\`\\\`stop\\\_gradient()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.stop\\\_gradient.html#mlx.core.stop\\\_gradient "mlx.core.stop\\\_gradient") for that. ## Automatic Vectorization Use \\\[\\\`\\\`vmap()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.vmap.html#mlx.core.vmap "mlx.core.vmap") to automate vectorizing complex functions. Here we’ll go through a basic and contrived example for the sake of clarity, but \\\[\\\`\\\`vmap()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.vmap.html#mlx.core.vmap "mlx.core.vmap") can be quite powerful for more complex functions which are difficult to optimize by hand. Warning Some operations are not yet supported with \\\[\\\`\\\`vmap()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.vmap.html#mlx.core.vmap "mlx.core.vmap"). If you encounter an error like: \\\`\\\\\\\`ValueError:\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`Primitive's\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`vmap\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`not\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`implemented.\\\\\\\`\\\` file an \\\[issue\\\](https://github.com/ml-explore/mlx/issues) and include your function. We will prioritize including it. A naive way to add the elements from two sets of vectors is with a loop: \\\`\\\`\\\`python xs = mx.random.uniform(shape=(4096, 100)) ys = mx.random.uniform(shape=(100, 4096)) def naive\\\_add(xs, ys): return \\\[xs\\\[i\\\] + ys\\\[:, i\\\] for i in range(xs.shape\\\[0\\\])\\\] \\\`\\\`\\\` Instead you can use \\\[\\\`\\\`vmap()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.vmap.html#mlx.core.vmap "mlx.core.vmap") to automatically vectorize the addition: \\\`\\\`\\\`python # Vectorize over the second dimension of x and the # first dimension of y vmap\\\_add = mx.vmap(lambda x, y: x + y, in\\\_axes=(0, 1)) \\\`\\\`\\\` The \\\`\\\\\\\`in\\\\\\\_axes\\\\\\\`\\\` parameter can be used to specify which dimensions of the corresponding input to vectorize over. Similarly, use \\\`\\\\\\\`out\\\\\\\_axes\\\\\\\`\\\` to specify where the vectorized axes should be in the outputs. Let’s time these two different versions: \\\`\\\`\\\`python import timeit print(timeit.timeit(lambda: mx.eval(naive\\\_add(xs, ys)), number=100)) print(timeit.timeit(lambda: mx.eval(vmap\\\_add(xs, ys)), number=100)) \\\`\\\`\\\` On an M1 Max the naive version takes in total \\\`\\\\\\\`5.639\\\\\\\`\\\` seconds whereas the vectorized version takes only \\\`\\\\\\\`0.024\\\\\\\`\\\` seconds, more than 200 times faster. Of course, this operation is quite contrived. A better approach is to simply do \\\`\\\\\\\`xs\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`+\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`ys.T\\\\\\\`\\\`, but for more complex functions \\\[\\\`\\\`vmap()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.vmap.html#mlx.core.vmap "mlx.core.vmap") can be quite handy. \\\[\\\](https://ml-explore.github.io/mlx/build/html/usage/saving\\\_and\\\_loading.html "previous page") previous Saving and Loading Arrays \\\[\\\](https://ml-explore.github.io/mlx/build/html/usage/compile.html "next page") next Compilation Contents \\\\- \\\[Automatic Differentiation\\\](https://ml-explore.github.io/mlx/build/html/#automatic-differentiation) - \\\[Automatic Vectorization\\\](https://ml-explore.github.io/mlx/build/html/#automatic-vectorization) By MLX Contributors © Copyright 2023, MLX Contributors.
