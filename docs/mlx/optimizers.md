Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers.rst "Download source file") - .pdf # Optimizers ## Contents \\\\- \\\[Saving and Loading\\\](https://ml-explore.github.io/mlx/build/html/#saving-and-loading) # Optimizers The optimizers in MLX can be used both with \\\`\\\\\\\`mlx.nn\\\\\\\`\\\` but also with pure \\\`\\\\\\\`mlx.core\\\\\\\`\\\` functions. A typical example involves calling \\\[\\\`\\\`Optimizer.update()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.update.html#mlx.optimizers.Optimizer.update "mlx.optimizers.Optimizer.update") to update a model’s parameters based on the loss gradients and subsequently calling \\\[\\\`\\\`mlx.core.eval()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.eval.html#mlx.core.eval "mlx.core.eval") to evaluate both the model’s parameters and the \\\\\\\*\\\\\\\*optimizer state\\\\\\\*\\\\\\\*. \\\`\\\`\\\`python # Create a model model = MLP(num\\\_layers, train\\\_images.shape\\\[-1\\\], hidden\\\_dim, num\\\_classes) mx.eval(model.parameters()) # Create the gradient function and the optimizer loss\\\_and\\\_grad\\\_fn = nn.value\\\_and\\\_grad(model, loss\\\_fn) optimizer = optim.SGD(learning\\\_rate=learning\\\_rate) for e in range(num\\\_epochs): for X, y in batch\\\_iterate(batch\\\_size, train\\\_images, train\\\_labels): loss, grads = loss\\\_and\\\_grad\\\_fn(model, X, y) # Update the model with the gradients. So far no computation has happened. optimizer.update(model, grads) # Compute the new parameters but also the optimizer state. mx.eval(model.parameters(), optimizer.state) \\\`\\\`\\\` ## Saving and Loading To serialize an optimizer, save its state. To load an optimizer, load and set the saved state. Here’s a simple example: \\\`\\\`\\\`python import mlx.core as mx from mlx.utils import tree\\\_flatten, tree\\\_unflatten import mlx.optimizers as optim optimizer = optim.Adam(learning\\\_rate=1e-2) # Perform some updates with the optimizer model = {"w" : mx.zeros((5, 5))} grads = {"w" : mx.ones((5, 5))} optimizer.update(model, grads) # Save the state state = tree\\\_flatten(optimizer.state) mx.save\\\_safetensors("optimizer.safetensors", dict(state)) # Later on, for example when loading from a checkpoint, # recreate the optimizer and load the state optimizer = optim.Adam(learning\\\_rate=1e-2) state = tree\\\_unflatten(list(mx.load("optimizer.safetensors").items())) optimizer.state = state \\\`\\\`\\\` Note, not every optimizer configuation parameter is saved in the state. For example, for Adam the learning rate is saved but the \\\`\\\\\\\`betas\\\\\\\`\\\` and \\\`\\\\\\\`eps\\\\\\\`\\\` parameters are not. A good rule of thumb is if the parameter can be scheduled then it will be included in the optimizer state. \\\\- \\\[Optimizer\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/optimizer.html) - \\\[\\\`\\\`Optimizer\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/optimizer.html#mlx.optimizers.Optimizer) - \\\[mlx.optimizers.Optimizer.state\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.state.html) - \\\[\\\`\\\`Optimizer.state\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.state.html#mlx.optimizers.Optimizer.state) - \\\[mlx.optimizers.Optimizer.apply\\\\\\\_gradients\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.apply\\\_gradients.html) - \\\[\\\`\\\`Optimizer.apply\\\_gradients()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.apply\\\_gradients.html#mlx.optimizers.Optimizer.apply\\\_gradients) - \\\[mlx.optimizers.Optimizer.init\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.init.html) - \\\[\\\`\\\`Optimizer.init()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.init.html#mlx.optimizers.Optimizer.init) - \\\[mlx.optimizers.Optimizer.update\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.update.html) - \\\[\\\`\\\`Optimizer.update()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.update.html#mlx.optimizers.Optimizer.update) - \\\[Common Optimizers\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/common\\\_optimizers.html) - \\\[mlx.optimizers.SGD\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.SGD.html) - \\\[\\\`\\\`SGD\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.SGD.html#mlx.optimizers.SGD) - \\\[mlx.optimizers.RMSprop\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.RMSprop.html) - \\\[\\\`\\\`RMSprop\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.RMSprop.html#mlx.optimizers.RMSprop) - \\\[mlx.optimizers.Adagrad\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adagrad.html) - \\\[\\\`\\\`Adagrad\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adagrad.html#mlx.optimizers.Adagrad) - \\\[mlx.optimizers.Adafactor\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adafactor.html) - \\\[\\\`\\\`Adafactor\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adafactor.html#mlx.optimizers.Adafactor) - \\\[mlx.optimizers.AdaDelta\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.AdaDelta.html) - \\\[\\\`\\\`AdaDelta\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.AdaDelta.html#mlx.optimizers.AdaDelta) - \\\[mlx.optimizers.Adam\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adam.html) - \\\[\\\`\\\`Adam\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adam.html#mlx.optimizers.Adam) - \\\[mlx.optimizers.AdamW\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.AdamW.html) - \\\[\\\`\\\`AdamW\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.AdamW.html#mlx.optimizers.AdamW) - \\\[mlx.optimizers.Adamax\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adamax.html) - \\\[\\\`\\\`Adamax\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adamax.html#mlx.optimizers.Adamax) - \\\[mlx.optimizers.Lion\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Lion.html) - \\\[\\\`\\\`Lion\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Lion.html#mlx.optimizers.Lion) - \\\[mlx.optimizers.MultiOptimizer\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.MultiOptimizer.html) - \\\[\\\`\\\`MultiOptimizer\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.MultiOptimizer.html#mlx.optimizers.MultiOptimizer) - \\\[Schedulers\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/schedulers.html) - \\\[mlx.optimizers.cosine\\\\\\\_decay\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.cosine\\\_decay.html) - \\\[\\\`\\\`cosine\\\_decay()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.cosine\\\_decay.html#mlx.optimizers.cosine\\\_decay) - \\\[mlx.optimizers.exponential\\\\\\\_decay\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.exponential\\\_decay.html) - \\\[\\\`\\\`exponential\\\_decay()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.exponential\\\_decay.html#mlx.optimizers.exponential\\\_decay) - \\\[mlx.optimizers.join\\\\\\\_schedules\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.join\\\_schedules.html) - \\\[\\\`\\\`join\\\_schedules()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.join\\\_schedules.html#mlx.optimizers.join\\\_schedules) - \\\[mlx.optimizers.linear\\\\\\\_schedule\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.linear\\\_schedule.html) - \\\[\\\`\\\`linear\\\_schedule()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.linear\\\_schedule.html#mlx.optimizers.linear\\\_schedule) - \\\[mlx.optimizers.step\\\\\\\_decay\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.step\\\_decay.html) - \\\[\\\`\\\`step\\\_decay()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.step\\\_decay.html#mlx.optimizers.step\\\_decay) | | | |----|----| | \\\[\\\`\\\`clip\\\_grad\\\_norm\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.optimizers.clip\\\_grad\\\_norm.html#mlx.optimizers.clip\\\_grad\\\_norm "mlx.optimizers.clip\\\_grad\\\_norm")(grads, max\\\\\\\_norm) | Clips the global norm of the gradients. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.init.he\\\_uniform.html "previous page") previous mlx.nn.init.he\\\\\\\_uniform \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/optimizer.html "next page") next Optimizer Contents \\\\- \\\[Saving and Loading\\\](https://ml-explore.github.io/mlx/build/html/#saving-and-loading) By MLX Contributors © Copyright 2023, MLX Contributors.
