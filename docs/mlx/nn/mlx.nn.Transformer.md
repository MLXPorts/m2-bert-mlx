Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/nn/\\\_autosummary/mlx.nn.Transformer.rst "Download source file") - .pdf # mlx.nn.Transformer ## Contents \\\\- \\\[\\\`\\\`Transformer\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.Transformer) # mlx.nn.Transformer \\\\\\\*\\\`class\\\` \\\\\\\*\\\`Transformer\\\`(\\\\\\\*\\\`dims:\\\` \\\`int\\\` \\\`\\\\=\\\` \\\`512,\\\` \\\`num\\\\\\\_heads:\\\` \\\`int\\\` \\\`\\\\=\\\` \\\`8,\\\` \\\`num\\\\\\\_encoder\\\\\\\_layers:\\\` \\\`int\\\` \\\`\\\\=\\\` \\\`6,\\\` \\\`num\\\\\\\_decoder\\\\\\\_layers:\\\` \\\`int\\\` \\\`\\\\=\\\` \\\`6,\\\` \\\`mlp\\\\\\\_dims:\\\` \\\`int\\\` \\\`\\\\\\\\|\\\` \\\`None\\\` \\\`\\\\=\\\` \\\`None,\\\` \\\`dropout:\\\` \\\`float\\\` \\\`\\\\=\\\` \\\`0.0,\\\` \\\`activation:\\\` \\\`~typing.Callable\\\\\\\\\\\\\\\[\\\\\\\\\\\\\\\[~typing.Any\\\\\\\\\\\\\\\],\\\` \\\`~typing.Any\\\\\\\\\\\\\\\]\\\` \\\`\\\\=\\\` \\\`\\\\\\\\ \\\`object\\\\\\\\>,\\\` \\\`custom\\\\\\\_encoder:\\\` \\\`~typing.Any\\\` \\\`\\\\\\\\|\\\` \\\`None\\\` \\\`\\\\=\\\` \\\`None,\\\` \\\`custom\\\\\\\_decoder:\\\` \\\`~typing.Any\\\` \\\`\\\\\\\\|\\\` \\\`None\\\` \\\`\\\\=\\\` \\\`None,\\\` \\\`norm\\\\\\\_first:\\\` \\\`bool\\\` \\\`\\\\=\\\` \\\`True,\\\` \\\`checkpoint:\\\` \\\`bool\\\` \\\`\\\\=\\\` \\\`False\\\`\\\\\\\*) Implements a standard Transformer model. The implementation is based on \\\[Attention Is All You Need\\\](https://arxiv.org/abs/1706.03762). The Transformer model contains an encoder and a decoder. The encoder processes the input sequence and the decoder generates the output sequence. The interaction between encoder and decoder happens through the attention mechanism. Parameters: - \\\\\\\*\\\\\\\*dims\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The number of expected features in the encoder/decoder inputs. Default: \\\`\\\\\\\`512\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*num\\\\\\\_heads\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The number of attention heads. Default: \\\`\\\\\\\`8\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*num\\\\\\\_encoder\\\\\\\_layers\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The number of encoder layers in the Transformer encoder. Default: \\\`\\\\\\\`6\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*num\\\\\\\_decoder\\\\\\\_layers\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The number of decoder layers in the Transformer decoder. Default: \\\`\\\\\\\`6\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*mlp\\\\\\\_dims\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The hidden dimension of the MLP block in each Transformer layer. Defaults to \\\`\\\\\\\`4\\\\\\\*dims\\\\\\\`\\\` if not provided. Default: \\\`\\\\\\\`None\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*dropout\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The dropout value for the Transformer encoder and decoder. Dropout is used after each attention layer and the activation in the MLP layer. Default: \\\`\\\\\\\`0.0\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*activation\\\\\\\*\\\\\\\* (\\\\\\\*function,\\\\\\\* \\\\\\\*optional\\\\\\\*) – the activation function for the MLP hidden layer. Default: \\\[\\\`\\\`mlx.nn.relu()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.relu.html#mlx.nn.relu "mlx.nn.relu"). - \\\\\\\*\\\\\\\*custom\\\\\\\_encoder\\\\\\\*\\\\\\\* (\\\[\\\*Module\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.Module")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – A custom encoder to replace the standard Transformer encoder. Default: \\\`\\\\\\\`None\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*custom\\\\\\\_decoder\\\\\\\*\\\\\\\* (\\\[\\\*Module\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.Module")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – A custom decoder to replace the standard Transformer decoder. Default: \\\`\\\\\\\`None\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*norm\\\\\\\_first\\\\\\\*\\\\\\\* (\\\[\\\*bool\\\*\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – if \\\`\\\\\\\`True\\\\\\\`\\\`, encoder and decoder layers will perform layer normalization before attention and MLP operations, otherwise after. Default: \\\`\\\\\\\`True\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*checkpoint\\\\\\\*\\\\\\\* (\\\[\\\*bool\\\*\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – if \\\`\\\\\\\`True\\\\\\\`\\\` perform gradient checkpointing to reduce the memory usage at the expense of more computation. Default: \\\`\\\\\\\`False\\\\\\\`\\\`. Methods\\\` \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Tanh.html "previous page") previous mlx.nn.Tanh \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Upsample.html "next page") next mlx.nn.Upsample Contents \\\\- \\\[\\\`\\\`Transformer\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.Transformer) By MLX Contributors © Copyright 2023, MLX Contributors.
