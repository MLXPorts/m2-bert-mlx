Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/nn/\\\_autosummary/mlx.nn.QuantizedLinear.rst "Download source file") - .pdf # mlx.nn.QuantizedLinear ## Contents \\\\- \\\[\\\`\\\`QuantizedLinear\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.QuantizedLinear) # mlx.nn.QuantizedLinear \\\\\\\*\\\`class\\\` \\\\\\\*\\\`QuantizedLinear\\\`(\\\\\\\*\\\`input\\\\\\\_dims\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*, \\\\\\\*\\\`output\\\\\\\_dims\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*, \\\\\\\*\\\`bias\\\`\\\`:\\\` \\\[\\\`bool\\\`\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") \\\`\\\\=\\\` \\\`True\\\`\\\\\\\*, \\\\\\\*\\\`group\\\\\\\_size\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") \\\`\\\\=\\\` \\\`64\\\`\\\\\\\*, \\\\\\\*\\\`bits\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") \\\`\\\\=\\\` \\\`4\\\`\\\\\\\*) Applies an affine transformation to the input using a quantized weight matrix. It is the quantized equivalent of \\\[\\\`\\\`mlx.nn.Linear\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Linear.html#mlx.nn.Linear "mlx.nn.Linear"). For now its parameters are frozen and will not be included in any gradient computation but this will probably change in the future. \\\[\\\`\\\`QuantizedLinear\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.QuantizedLinear "mlx.nn.QuantizedLinear") also provides a classmethod \\\`\\\\\\\`from\\\\\\\_linear()\\\\\\\`\\\` to convert linear layers to \\\[\\\`\\\`QuantizedLinear\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.QuantizedLinear "mlx.nn.QuantizedLinear") layers. Parameters: - \\\\\\\*\\\\\\\*input\\\\\\\_dims\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The dimensionality of the input features. - \\\\\\\*\\\\\\\*output\\\\\\\_dims\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The dimensionality of the output features. - \\\\\\\*\\\\\\\*bias\\\\\\\*\\\\\\\* (\\\[\\\*bool\\\*\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – If set to \\\`\\\\\\\`False\\\\\\\`\\\` then the layer will not use a bias. Default: \\\`\\\\\\\`True\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*group\\\\\\\_size\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The group size to use for the quantized weight. See \\\[\\\`\\\`quantize()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.quantize.html#mlx.core.quantize "mlx.core.quantize"). Default: \\\`\\\\\\\`64\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*bits\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The bit width to use for the quantized weight. See \\\[\\\`\\\`quantize()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.quantize.html#mlx.core.quantize "mlx.core.quantize"). Default: \\\`\\\\\\\`4\\\\\\\`\\\`. Methods | | | |----|----| | \\\`\\\\\\\`from\\\\\\\_linear\\\\\\\`\\\`(linear\\\\\\\_layer\\\\\\\\\\\\\\\[, group\\\\\\\_size, bits\\\\\\\\\\\\\\\]) | Create a \\\[\\\`\\\`QuantizedLinear\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.QuantizedLinear "mlx.nn.QuantizedLinear") layer from a \\\[\\\`\\\`Linear\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Linear.html#mlx.nn.Linear "mlx.nn.Linear") layer. | | \\\`\\\\\\\`unfreeze\\\\\\\`\\\`(\\\\\\\\\\\\\\\*args, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\*kwargs) | Wrap unfreeze so that we unfreeze any layers we might contain but our parameters will remain frozen. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.QuantizedEmbedding.html "previous page") previous mlx.nn.QuantizedEmbedding \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.RMSNorm.html "next page") next mlx.nn.RMSNorm Contents \\\\- \\\[\\\`\\\`QuantizedLinear\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.QuantizedLinear) By MLX Contributors © Copyright 2023, MLX Contributors.
