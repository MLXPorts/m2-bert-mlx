Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/\\\_autosummary/mlx.nn.quantize.rst "Download source file") - .pdf # mlx.nn.quantize ## Contents \\\\- \\\[\\\`\\\`quantize()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.quantize) # mlx.nn.quantize \\\`quantize\\\`(\\\\\\\*\\\`model\\\`\\\`:\\\` \\\[\\\`Module\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.layers.base.Module")\\\\\\\*, \\\\\\\*\\\`group\\\\\\\_size\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") \\\`\\\\=\\\` \\\`64\\\`\\\\\\\*, \\\\\\\*\\\`bits\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") \\\`\\\\=\\\` \\\`4\\\`\\\\\\\*, \\\\\\\*\\\`class\\\\\\\_predicate\\\`\\\`:\\\` \\\[\\\`Callable\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\[\\\`\\\`\\\\\\\\\\\\\\\[\\\`\\\[\\\`str\\\`\\\](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")\\\`,\\\` \\\[\\\`Module\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.layers.base.Module")\\\`\\\\\\\\\\\\\\\]\\\`\\\`,\\\` \\\[\\\`bool\\\`\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") \\\`\\\\\\\\|\\\` \\\[\\\`dict\\\`\\\](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\]\\\` \\\`\\\\\\\\|\\\` \\\[\\\`None\\\`\\\](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") \\\`\\\\=\\\` \\\`None\\\`\\\\\\\*) Quantize the sub-modules of a module according to a predicate. By default all layers that define a \\\`\\\\\\\`to\\\\\\\_quantized(group\\\\\\\_size,\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`bits)\\\\\\\`\\\` method will be quantized. Both \\\[\\\`\\\`Linear\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Linear.html#mlx.nn.Linear "mlx.nn.Linear") and \\\[\\\`\\\`Embedding\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Embedding.html#mlx.nn.Embedding "mlx.nn.Embedding") layers will be quantized. Note also, the module is updated in-place. Parameters: - \\\\\\\*\\\\\\\*model\\\\\\\*\\\\\\\* (\\\[\\\*Module\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.Module")) – The model whose leaf modules may be quantized. - \\\\\\\*\\\\\\\*group\\\\\\\_size\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The quantization group size (see \\\[\\\`\\\`mlx.core.quantize()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.quantize.html#mlx.core.quantize "mlx.core.quantize")). Default: \\\`\\\\\\\`64\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*bits\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – The number of bits per parameter (see \\\[\\\`\\\`mlx.core.quantize()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.quantize.html#mlx.core.quantize "mlx.core.quantize")). Default: \\\`\\\\\\\`4\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*class\\\\\\\_predicate\\\\\\\*\\\\\\\* (\\\\\\\*Optional\\\\\\\\\\\\\\\[Callable\\\\\\\\\\\\\\\]\\\\\\\*) – A callable which receives the \\\[\\\`\\\`Module\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.Module") path and \\\[\\\`\\\`Module\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.Module") itself and returns \\\`\\\\\\\`True\\\\\\\`\\\` or a dict of params for to\\\\\\\_quantized if it should be quantized and \\\`\\\\\\\`False\\\\\\\`\\\` otherwise. If \\\`\\\\\\\`None\\\\\\\`\\\`, then all layers that define a \\\`\\\\\\\`to\\\\\\\_quantized(group\\\\\\\_size,\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`bits)\\\\\\\`\\\` method are quantized. Default: \\\`\\\\\\\`None\\\\\\\`\\\`. \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.nn.value\\\_and\\\_grad.html "previous page") previous mlx.nn.value\\\\\\\_and\\\\\\\_grad \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.nn.average\\\_gradients.html "next page") next mlx.nn.average\\\\\\\_gradients Contents \\\\- \\\[\\\`\\\`quantize()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.quantize) By MLX Contributors © Copyright 2023, MLX Contributors.
