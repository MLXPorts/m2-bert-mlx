Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/nn/\\\_autosummary\\\_functions/mlx.nn.log\\\_softmax.rst "Download source file") - .pdf # mlx.nn.log\\\\\\\_softmax ## Contents \\\\- \\\[\\\`\\\`log\\\_softmax\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.log\\\_softmax) # mlx.nn.log\\\\\\\_softmax \\\\\\\*\\\`class\\\` \\\\\\\*\\\`log\\\\\\\_softmax\\\`(\\\\\\\*\\\`x\\\`\\\\\\\*, \\\\\\\*\\\`axis\\\`\\\`\\\\=\\\`\\\`\\\\-1\\\`\\\\\\\*) Applies the Log Softmax function. Applies \\\\\\\\\\\\\\\\x + \\\\\\\\log \\\\\\\\sum\\\\\\\_i e^{x\\\\\\\_i}\\\\\\\\\\\\\\\\ element wise. \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.log\\\_sigmoid.html "previous page") previous mlx.nn.log\\\\\\\_sigmoid \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.mish.html "next page") next mlx.nn.mish Contents \\\\- \\\[\\\`\\\`log\\\_softmax\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.log\\\_softmax) By MLX Contributors Â© Copyright 2023, MLX Contributors.
