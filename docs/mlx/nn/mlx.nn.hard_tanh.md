Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/nn/\\\_autosummary\\\_functions/mlx.nn.hard\\\_tanh.rst "Download source file") - .pdf # mlx.nn.hard\\\\\\\_tanh ## Contents \\\\- \\\[\\\`\\\`hard\\\_tanh\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.hard\\\_tanh) # mlx.nn.hard\\\\\\\_tanh \\\\\\\*\\\`class\\\` \\\\\\\*\\\`hard\\\\\\\_tanh\\\`(\\\\\\\*\\\`x\\\`\\\\\\\*, \\\\\\\*\\\`min\\\\\\\_val\\\`\\\`\\\\=\\\`\\\`\\\\-1.0\\\`\\\\\\\*, \\\\\\\*\\\`max\\\\\\\_val\\\`\\\`\\\\=\\\`\\\`1.0\\\`\\\\\\\*) Applies the HardTanh function. Applies \\\\\\\\\\\\\\\\\\\\\\\\max(\\\\\\\\min(x, \\\\\\\\text{max\\\\\\\\\\\\\\\\val}), \\\\\\\\text{min\\\\\\\\\\\\\\\\val})\\\\\\\\\\\\\\\\ element-wise. \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.hard\\\_shrink.html "previous page") previous mlx.nn.hard\\\\\\\_shrink \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.hardswish.html "next page") next mlx.nn.hardswish Contents \\\\- \\\[\\\`\\\`hard\\\_tanh\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.hard\\\_tanh) By MLX Contributors Â© Copyright 2023, MLX Contributors.
