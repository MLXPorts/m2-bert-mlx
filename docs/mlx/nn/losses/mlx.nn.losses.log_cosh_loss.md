Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.log\\\_cosh\\\_loss.rst "Download source file") - .pdf # mlx.nn.losses.log\\\\\\\_cosh\\\\\\\_loss ## Contents \\\\- \\\[\\\`\\\`log\\\_cosh\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.losses.log\\\_cosh\\\_loss) # mlx.nn.losses.log\\\\\\\_cosh\\\\\\\_loss \\\\\\\*\\\`class\\\` \\\\\\\*\\\`log\\\\\\\_cosh\\\\\\\_loss\\\`(\\\\\\\*\\\`inputs\\\`\\\`:\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\\\\\*, \\\\\\\*\\\`targets\\\`\\\`:\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\\\\\*, \\\\\\\*\\\`reduction\\\`\\\`:\\\` \\\[\\\`Literal\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\[\\\`\\\`'none'\\\`\\\`,\\\` \\\`'mean'\\\`\\\`,\\\` \\\`'sum'\\\`\\\`\\\\\\\\\\\\\\\]\\\` \\\`\\\\=\\\` \\\`'none'\\\`\\\\\\\*) Computes the log cosh loss between inputs and targets. Logcosh acts like L2 loss for small errors, ensuring stable gradients, and like the L1 loss for large errors, reducing sensitivity to outliers. This dual behavior offers a balanced, robust approach for regression tasks. \\\\\\\\\\\\\\\\\\\\\\\\text{logcosh}(y\\\\\\\\\\\\\\\_{\\\\\\\\text{true}}, y\\\\\\\\\\\\\\\_{\\\\\\\\text{pred}}) = \\\\\\\\frac{1}{n} \\\\\\\\sum\\\\\\\\\\\\\\\_{i=1}^{n} \\\\\\\\log(\\\\\\\\cosh(y\\\\\\\\\\\\\\\_{\\\\\\\\text{pred}}^{(i)} - y\\\\\\\\\\\\\\\_{\\\\\\\\text{true}}^{(i)}))\\\\\\\\\\\\\\\\ Parameters: - \\\\\\\*\\\\\\\*inputs\\\\\\\*\\\\\\\* (\\\[\\\*array\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")) – The predicted values. - \\\\\\\*\\\\\\\*targets\\\\\\\*\\\\\\\* (\\\[\\\*array\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")) – The target values. - \\\\\\\*\\\\\\\*reduction\\\\\\\*\\\\\\\* (\\\[\\\*str\\\*\\\](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – Specifies the reduction to apply to the output: \\\`\\\\\\\`'none'\\\\\\\`\\\` \\\\\\\\| \\\`\\\\\\\`'mean'\\\\\\\`\\\` \\\\\\\\| \\\`\\\\\\\`'sum'\\\\\\\`\\\`. Default: \\\`\\\\\\\`'none'\\\\\\\`\\\`. Returns: The computed log cosh loss. Return type: \\\[\\\*array\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array") \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.l1\\\_loss.html "previous page") previous mlx.nn.losses.l1\\\\\\\_loss \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.margin\\\_ranking\\\_loss.html "next page") next mlx.nn.losses.margin\\\\\\\_ranking\\\\\\\_loss Contents \\\\- \\\[\\\`\\\`log\\\_cosh\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.losses.log\\\_cosh\\\_loss) By MLX Contributors © Copyright 2023, MLX Contributors.
