Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.smooth\\\_l1\\\_loss.rst "Download source file") - .pdf # mlx.nn.losses.smooth\\\\\\\_l1\\\\\\\_loss ## Contents \\\\- \\\[\\\`\\\`smooth\\\_l1\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.losses.smooth\\\_l1\\\_loss) # mlx.nn.losses.smooth\\\\\\\_l1\\\\\\\_loss \\\\\\\*\\\`class\\\` \\\\\\\*\\\`smooth\\\\\\\_l1\\\\\\\_loss\\\`(\\\\\\\*\\\`predictions\\\`\\\`:\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\\\\\*, \\\\\\\*\\\`targets\\\`\\\`:\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\\\\\*, \\\\\\\*\\\`beta\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\=\\\` \\\`1.0\\\`\\\\\\\*, \\\\\\\*\\\`reduction\\\`\\\`:\\\` \\\[\\\`Literal\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\[\\\`\\\`'none'\\\`\\\`,\\\` \\\`'mean'\\\`\\\`,\\\` \\\`'sum'\\\`\\\`\\\\\\\\\\\\\\\]\\\` \\\`\\\\=\\\` \\\`'mean'\\\`\\\\\\\*) Computes the smooth L1 loss. The smooth L1 loss is a variant of the L1 loss which replaces the absolute difference with a squared difference when the absolute difference is less than \\\`\\\\\\\`beta\\\\\\\`\\\`. The formula for the smooth L1 Loss is: \\\\\\\\\\\\\\\\\\\\\\\\begin{split}l = \\\\\\\\begin{cases} 0.5 (x - y)^2 / \\\\\\\\beta, & \\\\\\\\text{if } \\\\\\\\|x - y\\\\\\\\| \\\\\\\\< \\\\\\\\beta \\\\\\\\\\\\\\\\ \\\\\\\\|x - y\\\\\\\\| - 0.5 \\\\\\\\beta, & \\\\\\\\text{otherwise} \\\\\\\\end{cases}\\\\\\\\end{split}\\\\\\\\\\\\\\\\ Parameters: - \\\\\\\*\\\\\\\*predictions\\\\\\\*\\\\\\\* (\\\[\\\*array\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")) – Predicted values. - \\\\\\\*\\\\\\\*targets\\\\\\\*\\\\\\\* (\\\[\\\*array\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")) – Ground truth values. - \\\\\\\*\\\\\\\*beta\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The threshold after which the loss changes from the squared to the absolute difference. Default: \\\`\\\\\\\`1.0\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*reduction\\\\\\\*\\\\\\\* (\\\[\\\*str\\\*\\\](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – Specifies the reduction to apply to the output: \\\`\\\\\\\`'none'\\\\\\\`\\\` \\\\\\\\| \\\`\\\\\\\`'mean'\\\\\\\`\\\` \\\\\\\\| \\\`\\\\\\\`'sum'\\\\\\\`\\\`. Default: \\\`\\\\\\\`'mean'\\\\\\\`\\\`. Returns: The computed smooth L1 loss. Return type: \\\[\\\*array\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array") \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.nll\\\_loss.html "previous page") previous mlx.nn.losses.nll\\\\\\\_loss \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.triplet\\\_loss.html "next page") next mlx.nn.losses.triplet\\\\\\\_loss Contents \\\\- \\\[\\\`\\\`smooth\\\_l1\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.losses.smooth\\\_l1\\\_loss) By MLX Contributors © Copyright 2023, MLX Contributors.
