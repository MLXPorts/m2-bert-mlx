Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.kl\\\_div\\\_loss.rst "Download source file") - .pdf # mlx.nn.losses.kl\\\\\\\_div\\\\\\\_loss ## Contents \\\\- \\\[\\\`\\\`kl\\\_div\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.losses.kl\\\_div\\\_loss) # mlx.nn.losses.kl\\\\\\\_div\\\\\\\_loss \\\\\\\*\\\`class\\\` \\\\\\\*\\\`kl\\\\\\\_div\\\\\\\_loss\\\`(\\\\\\\*\\\`inputs\\\`\\\`:\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\\\\\*, \\\\\\\*\\\`targets\\\`\\\`:\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\\\\\*, \\\\\\\*\\\`axis\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") \\\`\\\\=\\\` \\\`\\\\-1\\\`\\\\\\\*, \\\\\\\*\\\`reduction\\\`\\\`:\\\` \\\[\\\`Literal\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Literal "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\[\\\`\\\`'none'\\\`\\\`,\\\` \\\`'mean'\\\`\\\`,\\\` \\\`'sum'\\\`\\\`\\\\\\\\\\\\\\\]\\\` \\\`\\\\=\\\` \\\`'none'\\\`\\\\\\\*) Computes the Kullback-Leibler divergence loss. Computes the following when \\\`\\\\\\\`reduction\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`==\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`'none'\\\\\\\`\\\`: \\\`\\\`\\\`python mx.exp(targets) \\\* (targets - inputs).sum(axis) \\\`\\\`\\\` Parameters: - \\\\\\\*\\\\\\\*inputs\\\\\\\*\\\\\\\* (\\\[\\\*array\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")) – Log probabilities for the predicted distribution. - \\\\\\\*\\\\\\\*targets\\\\\\\*\\\\\\\* (\\\[\\\*array\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")) – Log probabilities for the target distribution. - \\\\\\\*\\\\\\\*axis\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The distribution axis. Default: \\\`\\\\\\\`-1\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*reduction\\\\\\\*\\\\\\\* (\\\[\\\*str\\\*\\\](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – Specifies the reduction to apply to the output: \\\`\\\\\\\`'none'\\\\\\\`\\\` \\\\\\\\| \\\`\\\\\\\`'mean'\\\\\\\`\\\` \\\\\\\\| \\\`\\\\\\\`'sum'\\\\\\\`\\\`. Default: \\\`\\\\\\\`'none'\\\\\\\`\\\`. Returns: The computed Kullback-Leibler divergence loss. Return type: \\\[\\\*array\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array") \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.huber\\\_loss.html "previous page") previous mlx.nn.losses.huber\\\\\\\_loss \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.l1\\\_loss.html "next page") next mlx.nn.losses.l1\\\\\\\_loss Contents \\\\- \\\[\\\`\\\`kl\\\_div\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.losses.kl\\\_div\\\_loss) By MLX Contributors © Copyright 2023, MLX Contributors.
