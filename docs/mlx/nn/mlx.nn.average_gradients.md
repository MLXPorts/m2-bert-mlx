Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/\\\_autosummary/mlx.nn.average\\\_gradients.rst "Download source file") - .pdf # mlx.nn.average\\\\\\\_gradients ## Contents \\\\- \\\[\\\`\\\`average\\\_gradients()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.average\\\_gradients) # mlx.nn.average\\\\\\\_gradients \\\`average\\\\\\\_gradients\\\`(\\\\\\\*\\\`gradients\\\`\\\`:\\\` \\\[\\\`Any\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.13)")\\\\\\\*, \\\\\\\*\\\`group\\\`\\\`:\\\` \\\[\\\`Group\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.distributed.Group.html#mlx.core.distributed.Group "mlx.core.distributed.Group") \\\`\\\\\\\\|\\\` \\\[\\\`None\\\`\\\](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") \\\`\\\\=\\\` \\\`None\\\`\\\\\\\*, \\\\\\\*\\\`all\\\\\\\_reduce\\\\\\\_size\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") \\\`\\\\=\\\` \\\`33554432\\\`\\\\\\\*, \\\\\\\*\\\`communication\\\\\\\_type\\\`\\\`:\\\` \\\[\\\`Dtype\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.Dtype.html#mlx.core.Dtype "mlx.core.Dtype") \\\`\\\\\\\\|\\\` \\\[\\\`None\\\`\\\](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") \\\`\\\\=\\\` \\\`None\\\`\\\\\\\*) Average the gradients across the distributed processes in the passed group. This helper enables concatenating several gradients of small arrays to one big all reduce call for better networking performance. Parameters: - \\\\\\\*\\\\\\\*gradients\\\\\\\*\\\\\\\* (\\\\\\\*Any\\\\\\\*) – The Python tree containing the gradients (it should have the same structure across processes) - \\\\\\\*\\\\\\\*group\\\\\\\*\\\\\\\* (\\\\\\\*Optional\\\\\\\\\\\\\\\[\\\\\\\*\\\[\\\*Group\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.distributed.Group.html#mlx.core.distributed.Group "mlx.core.distributed.Group")\\\\\\\*\\\\\\\\\\\\\\\]\\\\\\\*) – The group of processes to average the gradients. If set to \\\`\\\\\\\`None\\\\\\\`\\\` the global group is used. Default: \\\`\\\\\\\`None\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*all\\\\\\\_reduce\\\\\\\_size\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – Group arrays until their size in bytes exceeds this number. Perform one communication step per group of arrays. If less or equal to 0 array grouping is disabled. Default: \\\`\\\\\\\`32MiB\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*communication\\\\\\\_type\\\\\\\*\\\\\\\* (\\\\\\\*Optional\\\\\\\\\\\\\\\[\\\\\\\*\\\[\\\*Dtype\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.Dtype.html#mlx.core.Dtype "mlx.core.Dtype")\\\\\\\*\\\\\\\\\\\\\\\]\\\\\\\*) – If provided cast to this type before performing the communication. Typically cast to a smaller float to reduce the communication size. Default: \\\`\\\\\\\`None\\\\\\\`\\\`. \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.nn.quantize.html "previous page") previous mlx.nn.quantize \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html "next page") next Module Contents \\\\- \\\[\\\`\\\`average\\\_gradients()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.average\\\_gradients) By MLX Contributors © Copyright 2023, MLX Contributors.
