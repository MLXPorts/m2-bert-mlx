Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/nn/\\\_autosummary/mlx.nn.PReLU.rst "Download source file") - .pdf # mlx.nn.PReLU ## Contents \\\\- \\\[\\\`\\\`PReLU\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.PReLU) # mlx.nn.PReLU \\\\\\\*\\\`class\\\` \\\\\\\*\\\`PReLU\\\`(\\\\\\\*\\\`num\\\\\\\_parameters\\\`\\\`\\\\=\\\`\\\`1\\\`\\\\\\\*, \\\\\\\*\\\`init\\\`\\\`\\\\=\\\`\\\`0.25\\\`\\\\\\\*) Applies the element-wise parametric ReLU. Applies \\\\\\\\\\\\\\\\\\\\\\\\max(0, x) + a \\\\\\\\\\\\\\\* \\\\\\\\min(0, x)\\\\\\\\\\\\\\\\ element wise, where \\\\\\\\\\\\\\\\a\\\\\\\\\\\\\\\\ is an array. See \\\[\\\`\\\`prelu()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.prelu.html#mlx.nn.prelu "mlx.nn.prelu") for the functional equivalent. Parameters: - \\\\\\\*\\\\\\\*num\\\\\\\_parameters\\\\\\\*\\\\\\\* – number of \\\\\\\\\\\\\\\\a\\\\\\\\\\\\\\\\ to learn. Default: \\\`\\\\\\\`1\\\\\\\`\\\` - \\\\\\\*\\\\\\\*init\\\\\\\*\\\\\\\* – the initial value of \\\\\\\\\\\\\\\\a\\\\\\\\\\\\\\\\. Default: \\\`\\\\\\\`0.25\\\\\\\`\\\` Methods \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.MultiHeadAttention.html "previous page") previous mlx.nn.MultiHeadAttention \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.QuantizedEmbedding.html "next page") next mlx.nn.QuantizedEmbedding Contents \\\\- \\\[\\\`\\\`PReLU\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.PReLU) By MLX Contributors © Copyright 2023, MLX Contributors.
