Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/nn/\\\_autosummary\\\_functions/mlx.nn.gelu\\\_approx.rst "Download source file") - .pdf # mlx.nn.gelu\\\\\\\_approx ## Contents \\\\- \\\[\\\`\\\`gelu\\\_approx\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.gelu\\\_approx) # mlx.nn.gelu\\\\\\\_approx \\\\\\\*\\\`class\\\` \\\\\\\*\\\`gelu\\\\\\\_approx\\\`(\\\\\\\*\\\`x\\\`\\\\\\\*) An approximation to Gaussian Error Linear Unit. See \\\[\\\`\\\`gelu()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.gelu.html#mlx.nn.gelu "mlx.nn.gelu") for the exact computation. This function approximates \\\`\\\\\\\`gelu\\\\\\\`\\\` with a maximum absolute error \\\\\\\\\\\\\\\\\\\\\\\\< 0.0005\\\\\\\\\\\\\\\\ in the range \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\[-6, 6\\\\\\\\\\\\\\\]\\\\\\\\\\\\\\\\ using the following \\\\\\\\\\\\\\\\x = 0.5 \\\\\\\\\\\\\\\* x \\\\\\\\\\\\\\\* \\\\\\\\left(1 + \\\\\\\\text{Tanh}\\\\\\\\left((\\\\\\\\sqrt{2 / \\\\\\\\pi} \\\\\\\\\\\\\\\* \\\\\\\\left(x + 0.044715 \\\\\\\\\\\\\\\* x^3\\\\\\\\right)\\\\\\\\right)\\\\\\\\right)\\\\\\\\\\\\\\\\ \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.gelu.html "previous page") previous mlx.nn.gelu \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.gelu\\\_fast\\\_approx.html "next page") next mlx.nn.gelu\\\\\\\_fast\\\\\\\_approx Contents \\\\- \\\[\\\`\\\`gelu\\\_approx\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.nn.gelu\\\_approx) By MLX Contributors Â© Copyright 2023, MLX Contributors.
