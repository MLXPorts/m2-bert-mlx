Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - Defaults: floats are \\\\\\\`float32\\\\\\\`, integers are \\\\\\\`int32\\\\\\\` unless specified. - GPU caveat: \\\\\\\`float64\\\\\\\` runs on CPU only; prefer \\\\\\\`float32\\\\\\\` or \\\\\\\`bfloat16\\\\\\\`/\\\\\\\`float16\\\\\\\` on GPU. - Mixed precision: keep numerically sensitive reductions (e.g., loss accumulation) in \\\\\\\`float32\\\\\\\` even if model weights use lower precision. - Casting: use \\\\\\\`x.astype(mx.float16)\\\\\\\` explicitly; avoid implicit Python casts on MLX arrays. ### Examples \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx # Inspect dtype and shape x = mx.ones((2, 3), dtype=mx.float16) print(x.dtype, x.shape) # Cast explicitly x32 = x.astype(mx.float32) # Mixed precision: keep reductions in float32 fp16\\\\\\\_vals = mx.random.normal((1024,), dtype=mx.float16) mean32 = mx.mean(fp16\\\\\\\_vals.astype(mx.float32)) # Complex dtype z = mx.array(\\\\\\\[1+2j, 3-4j\\\\\\\]) print(z.dtype) # Integer widths i16 = mx.arange(0, 5, dtype=mx.int16) i64 = i16.astype(mx.int64) \\\\\\\`\\\\\\\`\\\\\\\` ## Float64 (double precision) Support MLX has limited float64 support tied to the execution device. - CPU: float64 is supported on CPU and participates in automatic differentiation. - GPU: float64 is not supported by Metal; attempting to run float64 ops on GPU raises an exception. - NumPy interop: converting a NumPy float64 array to MLX may downcast to float32 unless you target CPU and request \\\\\\\`dtype=mx.float64\\\\\\\`. ### Using float64 on CPU \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx # Per-op routing to CPU a64 = mx.ones((4,), dtype=mx.float64) out = mx.add(a64, 2.0, stream=mx.cpu) # Or set default device globally to CPU for double-precision sections mx.set\\\\\\\_default\\\\\\\_device(mx.cpu) x = mx.random.normal((1024,), dtype=mx.float64) y = mx.random.normal((1024,), dtype=mx.float64) z = mx.sum(x \\\\\\\* y) # runs on CPU; retains float64 \\\\\\\`\\\\\\\`\\\\\\\` ### Limitations on GPU - Unsupported dtype: float64 ops on GPU will error at runtime. - Be explicit about dtypes when moving between frameworks; verify with \\\\\\\`arr.dtype\\\\\\\` after conversions. \\\\\\\`\\\\\\\`\\\\\\\`python import numpy as np import mlx.core as mx np\\\\\\\_arr = np.arange(5, dtype=np.float64) mlx\\\\\\\_arr = mx.array(np\\\\\\\_arr) # may be float32 if default device is GPU print(mlx\\\\\\\_arr.dtype) # Ensure float64 on CPU mlx\\\\\\\_arr\\\\\\\_cpu64 = mx.array(np\\\\\\\_arr, dtype=mx.float64) with mx.default\\\\\\\_device(mx.cpu): print(mlx\\\\\\\_arr\\\\\\\_cpu64.dtype) # float64 \\\\\\\`\\\\\\\`\\\\\\\` ### Precision‑Sensitive Workflows - CPU‑only segments: run double‑precision sections entirely on CPU with \\\\\\\`mx.float64\\\\\\\`. - Mixed precision: keep numerically sensitive reductions/solves in float64 on CPU and cast results to float32 for GPU‑accelerated parts. - Explicit conversions: insert \\\\\\\`astype(...)\\\\\\\` at the device/precision boundaries; validate numerics with small probes. - Advanced: double‑double techniques can emulate higher precision in software when float64 is unavailable on your target device. \\\\\\\`\\\\\\\`\\\\\\\`python # Example: high‑precision reduce on CPU, rest on default device mx.set\\\\\\\_default\\\\\\\_device(mx.gpu) # assume your default is GPU v = mx.random.normal((1\\\\\\\_000\\\\\\\_000,), dtype=mx.float32) sum64 = mx.sum(v.astype(mx.float64), stream=mx.cpu) # precise sum on CPU sum32 = sum64.astype(mx.float32) # cast back for GPU work result = mx.sqrt(sum32 + 1.0) # continue on default device \\\\\\\`\\\\\\\`\\\\\\\` \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/data\\\_types.rst "Download source file") - .pdf # Data Types # Data Types The default floating point type is \\\`\\\\\\\`float32\\\\\\\`\\\` and the default integer type is \\\`\\\\\\\`int32\\\\\\\`\\\`. The table below shows supported values for \\\[\\\`\\\`Dtype\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.Dtype.html#mlx.core.Dtype "mlx.core.Dtype"). | Type | Bytes | Description | |----|----|----| | \\\`\\\\\\\`bool\\\\\\\_\\\\\\\`\\\` | 1 | Boolean (\\\`\\\\\\\`True\\\\\\\`\\\`, \\\`\\\\\\\`False\\\\\\\`\\\`) data type | | \\\`\\\\\\\`uint8\\\\\\\`\\\` | 1 | 8-bit unsigned integer | | \\\`\\\\\\\`uint16\\\\\\\`\\\` | 2 | 16-bit unsigned integer | | \\\`\\\\\\\`uint32\\\\\\\`\\\` | 4 | 32-bit unsigned integer | | \\\`\\\\\\\`uint64\\\\\\\`\\\` | 8 | 64-bit unsigned integer | | \\\`\\\\\\\`int8\\\\\\\`\\\` | 1 | 8-bit signed integer | | \\\`\\\\\\\`int16\\\\\\\`\\\` | 2 | 16-bit signed integer | | \\\`\\\\\\\`int32\\\\\\\`\\\` | 4 | 32-bit signed integer | | \\\`\\\\\\\`int64\\\\\\\`\\\` | 8 | 64-bit signed integer | | \\\`\\\\\\\`bfloat16\\\\\\\`\\\` | 2 | 16-bit brain float (e8, m7) | | \\\`\\\\\\\`float16\\\\\\\`\\\` | 2 | 16-bit IEEE float (e5, m10) | | \\\`\\\\\\\`float32\\\\\\\`\\\` | 4 | 32-bit float | | \\\`\\\\\\\`float64\\\\\\\`\\\` | 4 | 64-bit double | | \\\`\\\\\\\`complex64\\\\\\\`\\\` | 8 | 64-bit complex float | Supported Data Types {#id2} Note Arrays with type \\\`\\\\\\\`float64\\\\\\\`\\\` only work with CPU operations. Using \\\`\\\\\\\`float64\\\\\\\`\\\` arrays on the GPU will result in an exception. Data type are aranged in a hierarchy. See the \\\[\\\`\\\`DtypeCategory\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.DtypeCategory.html#mlx.core.DtypeCategory "mlx.core.DtypeCategory") object documentation for more information. Use \\\[\\\`\\\`issubdtype()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.issubdtype.html#mlx.core.issubdtype "mlx.core.issubdtype") to determine if one \\\`\\\\\\\`dtype\\\\\\\`\\\` (or category) is a subtype of another category. | | | |----|----| | \\\[\\\`\\\`Dtype\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.Dtype.html#mlx.core.Dtype "mlx.core.Dtype") | An object to hold the type of a \\\[\\\`\\\`array\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array"). | | \\\[\\\`\\\`DtypeCategory\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.DtypeCategory.html#mlx.core.DtypeCategory "mlx.core.DtypeCategory")(value) | Type to hold categories of \\\[\\\`\\\`dtypes\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.Dtype.html#mlx.core.Dtype "mlx.core.Dtype"). | | \\\[\\\`\\\`issubdtype\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.issubdtype.html#mlx.core.issubdtype "mlx.core.issubdtype")(arg1, arg2) | Check if a \\\[\\\`\\\`Dtype\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.Dtype.html#mlx.core.Dtype "mlx.core.Dtype") or \\\[\\\`\\\`DtypeCategory\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.DtypeCategory.html#mlx.core.DtypeCategory "mlx.core.DtypeCategory") is a subtype of another. | | \\\[\\\`\\\`finfo\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.finfo.html#mlx.core.finfo "mlx.core.finfo") | Get information on floating-point types. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.view.html "previous page") previous mlx.core.array.view \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.Dtype.html "next page") next mlx.core.Dtype By MLX Contributors © Copyright 2023, MLX Contributors.
