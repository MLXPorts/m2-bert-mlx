Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - Verify native Python: \\\\\\\`python -c "import platform; print(platform.processor())"\\\\\\\` should print \\\\\\\`arm\\\\\\\` on Apple Silicon. - Prefer a clean virtual environment (venv/conda) per project; avoid mixing system and user site‑packages. - If Homebrew installed Python, ensure your shell picks the ARM binary (e.g., avoid running under Rosetta/x86 shells). \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/install.rst "Download source file") - .pdf # Build and Install ## Contents \\\\- \\\[Python Installation\\\](https://ml-explore.github.io/mlx/build/html/#python-installation) - \\\[Troubleshooting\\\](https://ml-explore.github.io/mlx/build/html/#troubleshooting) - \\\[Build from source\\\](https://ml-explore.github.io/mlx/build/html/#build-from-source) - \\\[Build Requirements\\\](https://ml-explore.github.io/mlx/build/html/#build-requirements) - \\\[Python API\\\](https://ml-explore.github.io/mlx/build/html/#python-api) - \\\[C++ API\\\](https://ml-explore.github.io/mlx/build/html/#c-api) - \\\[Binary Size Minimization\\\](https://ml-explore.github.io/mlx/build/html/#binary-size-minimization) - \\\[Troubleshooting\\\](https://ml-explore.github.io/mlx/build/html/#id3) - \\\[Metal not found\\\](https://ml-explore.github.io/mlx/build/html/#metal-not-found) - \\\[x86 Shell\\\](https://ml-explore.github.io/mlx/build/html/#x86-shell) # Build and Install ## Python Installation MLX is available on PyPI. All you have to do to use MLX with your own Apple silicon computer is \\\`\\\`\\\`shell pip install mlx \\\`\\\`\\\` To install from PyPI you must meet the following requirements: - Using an M series chip (Apple silicon) - Using a native Python \\\\\\\\>= 3.9 - macOS \\\\\\\\>= 13.5 Note MLX is only available on devices running macOS \\\\\\\\>= 13.5 It is highly recommended to use macOS 14 (Sonoma) MLX is also available on conda-forge. To install MLX with conda do: \\\`\\\`\\\`shell conda install conda-forge::mlx \\\`\\\`\\\` ### Troubleshooting \\\\\\\*My OS and Python versions are in the required range but pip still does not find a matching distribution.\\\\\\\* Probably you are using a non-native Python. The output of \\\`\\\`\\\`shell python -c "import platform; print(platform.processor())" \\\`\\\`\\\` should be \\\`\\\\\\\`arm\\\\\\\`\\\`. If it is \\\`\\\\\\\`i386\\\\\\\`\\\` (and you have M series machine) then you are using a non-native Python. Switch your Python to a native Python. A good way to do this is with \\\[Conda\\\](https://stackoverflow.com/q/65415996). ## Build from source ### Build Requirements - A C++ compiler with C++17 support (e.g. Clang \\\\\\\\>= 5.0) - \\\[cmake\\\](https://cmake.org/) – version 3.25 or later, and \\\`\\\\\\\`make\\\\\\\`\\\` - Xcode \\\\\\\\>= 15.0 and macOS SDK \\\\\\\\>= 14.0 Note Ensure your shell environment is native \\\`\\\\\\\`arm\\\\\\\`\\\`, not \\\`\\\\\\\`x86\\\\\\\`\\\` via Rosetta. If the output of \\\`\\\\\\\`uname\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`-p\\\\\\\`\\\` is \\\`\\\\\\\`x86\\\\\\\`\\\`, see the \\\[troubleshooting section\\\](https://ml-explore.github.io/mlx/build/html/#build-shell) below. ### Python API To build and install the MLX python library from source, first, clone MLX from \\\[its GitHub repo\\\](https://github.com/ml-explore/mlx): \\\`\\\`\\\`shell git clone git@github.com:ml-explore/mlx.git mlx && cd mlx \\\`\\\`\\\` Then simply build and install MLX using pip: \\\`\\\`\\\`shell CMAKE\\\_BUILD\\\_PARALLEL\\\_LEVEL=8 pip install . \\\`\\\`\\\` For developing, install the package with development dependencies, and use an editable install: \\\`\\\`\\\`shell CMAKE\\\_BUILD\\\_PARALLEL\\\_LEVEL=8 pip install -e ".\\\[dev\\\]" \\\`\\\`\\\` Once the development dependencies are installed, you can build faster with: \\\`\\\`\\\`shell CMAKE\\\_BUILD\\\_PARALLEL\\\_LEVEL=8 python setup.py build\\\_ext --inplace \\\`\\\`\\\` Run the tests with: \\\`\\\`\\\`shell python -m unittest discover python/tests \\\`\\\`\\\` Optional: Install stubs to enable auto completions and type checking from your IDE: \\\`\\\`\\\`shell python setup.py generate\\\_stubs \\\`\\\`\\\` ### C++ API Currently, MLX must be built and installed from source. Similarly to the python library, to build and install the MLX C++ library start by cloning MLX from \\\[its GitHub repo\\\](https://github.com/ml-explore/mlx): \\\`\\\`\\\`shell git clone git@github.com:ml-explore/mlx.git mlx && cd mlx \\\`\\\`\\\` Create a build directory and run CMake and make: \\\`\\\`\\\`shell mkdir -p build && cd build cmake .. && make -j \\\`\\\`\\\` Run tests with: \\\`\\\`\\\`shell make test \\\`\\\`\\\` Install with: \\\`\\\`\\\`shell make install \\\`\\\`\\\` Note that the built \\\`\\\\\\\`mlx.metallib\\\\\\\`\\\` file should be either at the same directory as the executable statically linked to \\\`\\\\\\\`libmlx.a\\\\\\\`\\\` or the preprocessor constant \\\`\\\\\\\`METAL\\\\\\\_PATH\\\\\\\`\\\` should be defined at build time and it should point to the path to the built metal library. | Option | Default | |---------------------------|---------| | MLX\\\\\\\_BUILD\\\\\\\_TESTS | ON | | MLX\\\\\\\_BUILD\\\\\\\_EXAMPLES | OFF | | MLX\\\\\\\_BUILD\\\\\\\_BENCHMARKS | OFF | | MLX\\\\\\\_BUILD\\\\\\\_METAL | ON | | MLX\\\\\\\_BUILD\\\\\\\_CPU | ON | | MLX\\\\\\\_BUILD\\\\\\\_PYTHON\\\\\\\_BINDINGS | OFF | | MLX\\\\\\\_METAL\\\\\\\_DEBUG | OFF | | MLX\\\\\\\_BUILD\\\\\\\_SAFETENSORS | ON | | MLX\\\\\\\_BUILD\\\\\\\_GGUF | ON | | MLX\\\\\\\_METAL\\\\\\\_JIT | OFF | Build Options {#id4} Note If you have multiple Xcode installations and wish to use a specific one while building, you can do so by adding the following environment variable before building \\\`\\\`\\\`shell export DEVELOPER\\\_DIR="/path/to/Xcode.app/Contents/Developer/" \\\`\\\`\\\` Further, you can use the following command to find out which macOS SDK will be used \\\`\\\`\\\`shell xcrun -sdk macosx --show-sdk-version \\\`\\\`\\\` #### Binary Size Minimization To produce a smaller binary use the CMake flags \\\`\\\\\\\`CMAKE\\\\\\\_BUILD\\\\\\\_TYPE=MinSizeRel\\\\\\\`\\\` and \\\`\\\\\\\`BUILD\\\\\\\_SHARED\\\\\\\_LIBS=ON\\\\\\\`\\\`. The MLX CMake build has several additional options to make smaller binaries. For example, if you don’t need the CPU backend or support for safetensors and GGUF, you can do: \\\`\\\`\\\`shell cmake .. \\\\ -DCMAKE\\\_BUILD\\\_TYPE=MinSizeRel \\\\ -DBUILD\\\_SHARED\\\_LIBS=ON \\\\ -DMLX\\\_BUILD\\\_CPU=OFF \\\\ -DMLX\\\_BUILD\\\_SAFETENSORS=OFF \\\\ -DMLX\\\_BUILD\\\_GGUF=OFF \\\\ -DMLX\\\_METAL\\\_JIT=ON \\\`\\\`\\\` THE \\\`\\\\\\\`MLX\\\\\\\_METAL\\\\\\\_JIT\\\\\\\`\\\` flag minimizes the size of the MLX Metal library which contains pre-built GPU kernels. This substantially reduces the size of the Metal library by run-time compiling kernels the first time they are used in MLX on a given machine. Note run-time compilation incurs a cold-start cost which can be anwywhere from a few hundred millisecond to a few seconds depending on the application. Once a kernel is compiled, it will be cached by the system. The Metal kernel cache persists across reboots. ### Troubleshooting #### Metal not found You see the following error when you try to build: \\\`\\\`\\\`shell error: unable to find utility "metal", not a developer tool or in PATH \\\`\\\`\\\` To fix this, first make sure you have Xcode installed: \\\`\\\`\\\`shell xcode-select --install \\\`\\\`\\\` Then set the active developer directory: \\\`\\\`\\\`shell sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer \\\`\\\`\\\` #### x86 Shell If the output of \\\`\\\\\\\`uname\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`-p\\\\\\\`\\\` is \\\`\\\\\\\`x86\\\\\\\`\\\` then your shell is running as x86 via Rosetta instead of natively. To fix this, find the application in Finder (\\\`\\\\\\\`/Applications\\\\\\\`\\\` for iTerm, \\\`\\\\\\\`/Applications/Utilities\\\\\\\`\\\` for Terminal), right-click, and click “Get Info”. Uncheck “Open using Rosetta”, close the “Get Info” window, and restart your terminal. Verify the terminal is now running natively the following command: \\\`\\\`\\\`shell $ uname -p arm \\\`\\\`\\\` Also check that cmake is using the correct architecture: \\\`\\\`\\\`shell $ cmake --system-information | grep CMAKE\\\_HOST\\\_SYSTEM\\\_PROCESSOR CMAKE\\\_HOST\\\_SYSTEM\\\_PROCESSOR "arm64" \\\`\\\`\\\` If you see \\\`\\\\\\\`"x86\\\\\\\_64"\\\\\\\`\\\`, try re-installing \\\`\\\\\\\`cmake\\\\\\\`\\\`. If you see \\\`\\\\\\\`"arm64"\\\\\\\`\\\` but the build errors out with “Building for x86\\\\\\\_64 on macOS is not supported.” wipe your build cache with \\\`\\\\\\\`rm\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`-rf\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`build/\\\\\\\`\\\` and try again. \\\[\\\](https://ml-explore.github.io/mlx/build/html/index.html "previous page") previous MLX \\\[\\\](https://ml-explore.github.io/mlx/build/html/usage/quick\\\_start.html "next page") next Quick Start Guide Contents \\\\- \\\[Python Installation\\\](https://ml-explore.github.io/mlx/build/html/#python-installation) - \\\[Troubleshooting\\\](https://ml-explore.github.io/mlx/build/html/#troubleshooting) - \\\[Build from source\\\](https://ml-explore.github.io/mlx/build/html/#build-from-source) - \\\[Build Requirements\\\](https://ml-explore.github.io/mlx/build/html/#build-requirements) - \\\[Python API\\\](https://ml-explore.github.io/mlx/build/html/#python-api) - \\\[C++ API\\\](https://ml-explore.github.io/mlx/build/html/#c-api) - \\\[Binary Size Minimization\\\](https://ml-explore.github.io/mlx/build/html/#binary-size-minimization) - \\\[Troubleshooting\\\](https://ml-explore.github.io/mlx/build/html/#id3) - \\\[Metal not found\\\](https://ml-explore.github.io/mlx/build/html/#metal-not-found) - \\\[x86 Shell\\\](https://ml-explore.github.io/mlx/build/html/#x86-shell) By MLX Contributors © Copyright 2023, MLX Contributors.
