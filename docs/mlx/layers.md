Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - Input convention: most layers expect \\\\\\\`(N, C, ...)\\\\\\\` (e.g., Conv2d: \\\\\\\`(N, C, H, W)\\\\\\\`). - After spatial layers, \\\\\\\`nn.Flatten()\\\\\\\` helps build MLP heads: verify \\\\\\\`C\\\\\\\*H\\\\\\\*W\\\\\\\` matches the next \\\\\\\`Linear\\\\\\\`. - For depthwise conv: \\\\\\\`groups=in\\\\\\\_channels\\\\\\\` with \\\\\\\`out\\\\\\\_channels=in\\\\\\\_channels\\\\\\\`. ### Examples \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx import mlx.nn as nn # NCHW input x = mx.random.normal((8, 3, 32, 32)) conv = nn.Conv2d(3, 16, kernel\\\\\\\_size=3, stride=1, padding=1) y = conv(x) # (8, 16, 32, 32) pool = nn.AvgPool2d(2) p = pool(y) # (8, 16, 16, 16) flat = nn.Flatten() f = flat(p) # (8, 16\\\\\\\*16\\\\\\\*16) head = nn.Linear(16\\\\\\\*16\\\\\\\*16, 10) logits = head(f) # (8, 10) # Depthwise conv (per-channel) dw = nn.Conv2d(16, 16, kernel\\\\\\\_size=3, padding=1, groups=16) z = dw(y) \\\\\\\`\\\\\\\`\\\\\\\` \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/nn/layers.rst "Download source file") - .pdf # Layers # Layers | | | |----|----| | \\\[\\\`\\\`ALiBi\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.ALiBi.html#mlx.nn.ALiBi "mlx.nn.ALiBi")() | | | \\\[\\\`\\\`AvgPool1d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.AvgPool1d.html#mlx.nn.AvgPool1d "mlx.nn.AvgPool1d")(kernel\\\\\\\_size\\\\\\\\\\\\\\\[, stride, padding\\\\\\\\\\\\\\\]) | Applies 1-dimensional average pooling. | | \\\[\\\`\\\`AvgPool2d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.AvgPool2d.html#mlx.nn.AvgPool2d "mlx.nn.AvgPool2d")(kernel\\\\\\\_size\\\\\\\\\\\\\\\[, stride, padding\\\\\\\\\\\\\\\]) | Applies 2-dimensional average pooling. | | \\\[\\\`\\\`AvgPool3d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.AvgPool3d.html#mlx.nn.AvgPool3d "mlx.nn.AvgPool3d")(kernel\\\\\\\_size\\\\\\\\\\\\\\\[, stride, padding\\\\\\\\\\\\\\\]) | Applies 3-dimensional average pooling. | | \\\[\\\`\\\`BatchNorm\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.BatchNorm.html#mlx.nn.BatchNorm "mlx.nn.BatchNorm")(num\\\\\\\_features\\\\\\\\\\\\\\\[, eps, momentum, ...\\\\\\\\\\\\\\\]) | Applies Batch Normalization over a 2D or 3D input. | | \\\[\\\`\\\`CELU\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.CELU.html#mlx.nn.CELU "mlx.nn.CELU")(\\\\\\\\\\\\\\\[alpha\\\\\\\\\\\\\\\]) | Applies the Continuously Differentiable Exponential Linear Unit. | | \\\[\\\`\\\`Conv1d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Conv1d.html#mlx.nn.Conv1d "mlx.nn.Conv1d")(in\\\\\\\_channels, out\\\\\\\_channels, kernel\\\\\\\_size) | Applies a 1-dimensional convolution over the multi-channel input sequence. | | \\\[\\\`\\\`Conv2d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Conv2d.html#mlx.nn.Conv2d "mlx.nn.Conv2d")(in\\\\\\\_channels, out\\\\\\\_channels, kernel\\\\\\\_size) | Applies a 2-dimensional convolution over the multi-channel input image. | | \\\[\\\`\\\`Conv3d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Conv3d.html#mlx.nn.Conv3d "mlx.nn.Conv3d")(in\\\\\\\_channels, out\\\\\\\_channels, kernel\\\\\\\_size) | Applies a 3-dimensional convolution over the multi-channel input image. | | \\\[\\\`\\\`ConvTranspose1d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.ConvTranspose1d.html#mlx.nn.ConvTranspose1d "mlx.nn.ConvTranspose1d")(in\\\\\\\_channels, out\\\\\\\_channels, ...) | Applies a 1-dimensional transposed convolution over the multi-channel input sequence. | | \\\[\\\`\\\`ConvTranspose2d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.ConvTranspose2d.html#mlx.nn.ConvTranspose2d "mlx.nn.ConvTranspose2d")(in\\\\\\\_channels, out\\\\\\\_channels, ...) | Applies a 2-dimensional transposed convolution over the multi-channel input image. | | \\\[\\\`\\\`ConvTranspose3d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.ConvTranspose3d.html#mlx.nn.ConvTranspose3d "mlx.nn.ConvTranspose3d")(in\\\\\\\_channels, out\\\\\\\_channels, ...) | Applies a 3-dimensional transposed convolution over the multi-channel input image. | | \\\[\\\`\\\`Dropout\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Dropout.html#mlx.nn.Dropout "mlx.nn.Dropout")(\\\\\\\\\\\\\\\[p\\\\\\\\\\\\\\\]) | Randomly zero a portion of the elements during training. | | \\\[\\\`\\\`Dropout2d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Dropout2d.html#mlx.nn.Dropout2d "mlx.nn.Dropout2d")(\\\\\\\\\\\\\\\[p\\\\\\\\\\\\\\\]) | Apply 2D channel-wise dropout during training. | | \\\[\\\`\\\`Dropout3d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Dropout3d.html#mlx.nn.Dropout3d "mlx.nn.Dropout3d")(\\\\\\\\\\\\\\\[p\\\\\\\\\\\\\\\]) | Apply 3D channel-wise dropout during training. | | \\\[\\\`\\\`Embedding\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Embedding.html#mlx.nn.Embedding "mlx.nn.Embedding")(num\\\\\\\_embeddings, dims) | Implements a simple lookup table that maps each input integer to a high-dimensional vector. | | \\\[\\\`\\\`ELU\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.ELU.html#mlx.nn.ELU "mlx.nn.ELU")(\\\\\\\\\\\\\\\[alpha\\\\\\\\\\\\\\\]) | Applies the Exponential Linear Unit. | | \\\[\\\`\\\`GELU\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.GELU.html#mlx.nn.GELU "mlx.nn.GELU")(\\\\\\\\\\\\\\\[approx\\\\\\\\\\\\\\\]) | Applies the Gaussian Error Linear Units. | | \\\[\\\`\\\`GLU\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.GLU.html#mlx.nn.GLU "mlx.nn.GLU")(\\\\\\\\\\\\\\\[axis\\\\\\\\\\\\\\\]) | Applies the gated linear unit function. | | \\\[\\\`\\\`GroupNorm\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.GroupNorm.html#mlx.nn.GroupNorm "mlx.nn.GroupNorm")(num\\\\\\\_groups, dims\\\\\\\\\\\\\\\[, eps, affine, ...\\\\\\\\\\\\\\\]) | Applies Group Normalization \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\] to the inputs. | | \\\[\\\`\\\`GRU\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.GRU.html#mlx.nn.GRU "mlx.nn.GRU")(input\\\\\\\_size, hidden\\\\\\\_size\\\\\\\\\\\\\\\[, bias\\\\\\\\\\\\\\\]) | A gated recurrent unit (GRU) RNN layer. | | \\\[\\\`\\\`HardShrink\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.HardShrink.html#mlx.nn.HardShrink "mlx.nn.HardShrink")() | Applies the HardShrink function. | | \\\[\\\`\\\`HardTanh\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.HardTanh.html#mlx.nn.HardTanh "mlx.nn.HardTanh")() | Applies the HardTanh function. | | \\\[\\\`\\\`Hardswish\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Hardswish.html#mlx.nn.Hardswish "mlx.nn.Hardswish")() | Applies the hardswish function, element-wise. | | \\\[\\\`\\\`InstanceNorm\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.InstanceNorm.html#mlx.nn.InstanceNorm "mlx.nn.InstanceNorm")(dims\\\\\\\\\\\\\\\[, eps, affine\\\\\\\\\\\\\\\]) | Applies instance normalization \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\] on the inputs. | | \\\[\\\`\\\`LayerNorm\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.LayerNorm.html#mlx.nn.LayerNorm "mlx.nn.LayerNorm")(dims\\\\\\\\\\\\\\\[, eps, affine, bias\\\\\\\\\\\\\\\]) | Applies layer normalization \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\] on the inputs. | | \\\[\\\`\\\`LeakyReLU\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.LeakyReLU.html#mlx.nn.LeakyReLU "mlx.nn.LeakyReLU")(\\\\\\\\\\\\\\\[negative\\\\\\\_slope\\\\\\\\\\\\\\\]) | Applies the Leaky Rectified Linear Unit. | | \\\[\\\`\\\`Linear\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Linear.html#mlx.nn.Linear "mlx.nn.Linear")(input\\\\\\\_dims, output\\\\\\\_dims\\\\\\\\\\\\\\\[, bias\\\\\\\\\\\\\\\]) | Applies an affine transformation to the input. | | \\\[\\\`\\\`LogSigmoid\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.LogSigmoid.html#mlx.nn.LogSigmoid "mlx.nn.LogSigmoid")() | Applies the Log Sigmoid function. | | \\\[\\\`\\\`LogSoftmax\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.LogSoftmax.html#mlx.nn.LogSoftmax "mlx.nn.LogSoftmax")() | Applies the Log Softmax function. | | \\\[\\\`\\\`LSTM\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.LSTM.html#mlx.nn.LSTM "mlx.nn.LSTM")(input\\\\\\\_size, hidden\\\\\\\_size\\\\\\\\\\\\\\\[, bias\\\\\\\\\\\\\\\]) | An LSTM recurrent layer. | | \\\[\\\`\\\`MaxPool1d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.MaxPool1d.html#mlx.nn.MaxPool1d "mlx.nn.MaxPool1d")(kernel\\\\\\\_size\\\\\\\\\\\\\\\[, stride, padding\\\\\\\\\\\\\\\]) | Applies 1-dimensional max pooling. | | \\\[\\\`\\\`MaxPool2d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.MaxPool2d.html#mlx.nn.MaxPool2d "mlx.nn.MaxPool2d")(kernel\\\\\\\_size\\\\\\\\\\\\\\\[, stride, padding\\\\\\\\\\\\\\\]) | Applies 2-dimensional max pooling. | | \\\[\\\`\\\`MaxPool3d\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.MaxPool3d.html#mlx.nn.MaxPool3d "mlx.nn.MaxPool3d")(kernel\\\\\\\_size\\\\\\\\\\\\\\\[, stride, padding\\\\\\\\\\\\\\\]) | Applies 3-dimensional max pooling. | | \\\[\\\`\\\`Mish\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Mish.html#mlx.nn.Mish "mlx.nn.Mish")() | Applies the Mish function, element-wise. | | \\\[\\\`\\\`MultiHeadAttention\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.MultiHeadAttention.html#mlx.nn.MultiHeadAttention "mlx.nn.MultiHeadAttention")(dims, num\\\\\\\_heads\\\\\\\\\\\\\\\[, ...\\\\\\\\\\\\\\\]) | Implements the scaled dot product attention with multiple heads. | | \\\[\\\`\\\`PReLU\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.PReLU.html#mlx.nn.PReLU "mlx.nn.PReLU")(\\\\\\\\\\\\\\\[num\\\\\\\_parameters, init\\\\\\\\\\\\\\\]) | Applies the element-wise parametric ReLU. | | \\\[\\\`\\\`QuantizedEmbedding\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.QuantizedEmbedding.html#mlx.nn.QuantizedEmbedding "mlx.nn.QuantizedEmbedding")(num\\\\\\\_embeddings, dims\\\\\\\\\\\\\\\[, ...\\\\\\\\\\\\\\\]) | The same as \\\[\\\`\\\`Embedding\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Embedding.html#mlx.nn.Embedding "mlx.nn.Embedding") but with a quantized weight matrix. | | \\\[\\\`\\\`QuantizedLinear\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.QuantizedLinear.html#mlx.nn.QuantizedLinear "mlx.nn.QuantizedLinear")(input\\\\\\\_dims, output\\\\\\\_dims\\\\\\\\\\\\\\\[, ...\\\\\\\\\\\\\\\]) | Applies an affine transformation to the input using a quantized weight matrix. | | \\\[\\\`\\\`RMSNorm\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.RMSNorm.html#mlx.nn.RMSNorm "mlx.nn.RMSNorm")(dims\\\\\\\\\\\\\\\[, eps\\\\\\\\\\\\\\\]) | Applies Root Mean Square normalization \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\] to the inputs. | | \\\[\\\`\\\`ReLU\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.ReLU.html#mlx.nn.ReLU "mlx.nn.ReLU")() | Applies the Rectified Linear Unit. | | \\\[\\\`\\\`ReLU6\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.ReLU6.html#mlx.nn.ReLU6 "mlx.nn.ReLU6")() | Applies the Rectified Linear Unit 6. | | \\\[\\\`\\\`RNN\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.RNN.html#mlx.nn.RNN "mlx.nn.RNN")(input\\\\\\\_size, hidden\\\\\\\_size\\\\\\\\\\\\\\\[, bias, ...\\\\\\\\\\\\\\\]) | An Elman recurrent layer. | | \\\[\\\`\\\`RoPE\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.RoPE.html#mlx.nn.RoPE "mlx.nn.RoPE")(dims\\\\\\\\\\\\\\\[, traditional, base, scale\\\\\\\\\\\\\\\]) | Implements the rotary positional encoding. | | \\\[\\\`\\\`SELU\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.SELU.html#mlx.nn.SELU "mlx.nn.SELU")() | Applies the Scaled Exponential Linear Unit. | | \\\[\\\`\\\`Sequential\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Sequential.html#mlx.nn.Sequential "mlx.nn.Sequential")(\\\\\\\\\\\\\\\*modules) | A layer that calls the passed callables in order. | | \\\[\\\`\\\`Sigmoid\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Sigmoid.html#mlx.nn.Sigmoid "mlx.nn.Sigmoid")() | Applies the sigmoid function, element-wise. | | \\\[\\\`\\\`SiLU\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.SiLU.html#mlx.nn.SiLU "mlx.nn.SiLU")() | Applies the Sigmoid Linear Unit. | | \\\[\\\`\\\`SinusoidalPositionalEncoding\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.SinusoidalPositionalEncoding.html#mlx.nn.SinusoidalPositionalEncoding "mlx.nn.SinusoidalPositionalEncoding")(dims\\\\\\\\\\\\\\\[, ...\\\\\\\\\\\\\\\]) | Implements sinusoidal positional encoding. | | \\\[\\\`\\\`Softmin\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Softmin.html#mlx.nn.Softmin "mlx.nn.Softmin")() | Applies the Softmin function. | | \\\[\\\`\\\`Softshrink\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Softshrink.html#mlx.nn.Softshrink "mlx.nn.Softshrink")(\\\\\\\\\\\\\\\[lambd\\\\\\\\\\\\\\\]) | Applies the Softshrink function. | | \\\[\\\`\\\`Softsign\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Softsign.html#mlx.nn.Softsign "mlx.nn.Softsign")() | Applies the Softsign function. | | \\\[\\\`\\\`Softmax\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Softmax.html#mlx.nn.Softmax "mlx.nn.Softmax")() | Applies the Softmax function. | | \\\[\\\`\\\`Softplus\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Softplus.html#mlx.nn.Softplus "mlx.nn.Softplus")() | Applies the Softplus function. | | \\\[\\\`\\\`Step\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Step.html#mlx.nn.Step "mlx.nn.Step")(\\\\\\\\\\\\\\\[threshold\\\\\\\\\\\\\\\]) | Applies the Step Activation Function. | | \\\[\\\`\\\`Tanh\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Tanh.html#mlx.nn.Tanh "mlx.nn.Tanh")() | Applies the hyperbolic tangent function. | | \\\[\\\`\\\`Transformer\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Transformer.html#mlx.nn.Transformer "mlx.nn.Transformer")(dims, num\\\\\\\_heads, ...) | Implements a standard Transformer model. | | \\\[\\\`\\\`Upsample\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Upsample.html#mlx.nn.Upsample "mlx.nn.Upsample")(scale\\\\\\\_factor\\\\\\\\\\\\\\\[, mode, align\\\\\\\_corners\\\\\\\\\\\\\\\]) | Upsample the input signal spatially. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Module.update\\\_modules.html "previous page") previous mlx.nn.Module.update\\\\\\\_modules \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.ALiBi.html "next page") next mlx.nn.ALiBi By MLX Contributors © Copyright 2023, MLX Contributors.
