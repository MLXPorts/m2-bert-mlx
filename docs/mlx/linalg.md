Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - Prefer \\\\\\\`mx.linalg.solve\\\\\\\` over \\\\\\\`inv(A) @ b\\\\\\\` for stability and performance. - Add small diagonal jitter when decomposing nearly singular matrices: \\\\\\\`A @ A.T + 1e-3 \\\\\\\* mx.eye(n)\\\\\\\`. - When batching solves, broadcast shapes explicitly and check that leading dimensions align. - No \\\\\\\`device=\\\\\\\` argument: control placement via default device or per‑op \\\\\\\`stream\\\\\\\` when supported. - Global: \\\\\\\`mx.set\\\\\\\_default\\\\\\\_device(mx.cpu)\\\\\\\` (or \\\\\\\`mx.gpu\\\\\\\`) - Scoped: \\\\\\\`with mx.default\\\\\\\_device(mx.cpu): U, S, Vt = mx.linalg.svd(A)\\\\\\\` - Per‑op: some linalg ops accept \\\\\\\`stream=mx.cpu\\\\\\\` ### Cholesky on Metal (Tiled Pattern) - Built‑in \\\\\\\`mx.linalg.cholesky\\\\\\\` often runs on CPU; for GPU, a custom tiled kernel can deliver speedups on large, well‑conditioned matrices. - Strategy: compute the diagonal block with a numerically stable loop (single thread), then update trailing blocks in parallel; synchronize with \\\\\\\`threadgroup\\\\\\\_barrier\\\\\\\` between phases. \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx def block\\\\\\\_cholesky(A, block\\\\\\\_size=16): # See Custom Metal Kernels for a full kernel; this shows the call pattern nthreads = min(32, A.shape\\\\\\\[0\\\\\\\]) k = mx.fast.metal\\\\\\\_kernel( name="blk\\\\\\\_chol", input\\\\\\\_names=\\\\\\\["A","blk","nth"\\\\\\\], output\\\\\\\_names=\\\\\\\["out"\\\\\\\], source="/\\\\\\\* body omitted for brevity; diagonal then trailing updates with barriers \\\\\\\*/", header="#include \\\\\\\\nusing namespace metal;\\\\\\\\n", ) return k( inputs=\\\\\\\[A, mx.array(\\\\\\\[block\\\\\\\_size\\\\\\\], dtype=mx.uint32), mx.array(\\\\\\\[nthreads\\\\\\\], dtype=mx.uint32)\\\\\\\], output\\\\\\\_shapes=\\\\\\\[A.shape\\\\\\\], output\\\\\\\_dtypes=\\\\\\\[A.dtype\\\\\\\], grid=(nthreads,1,1), threadgroup=(nthreads,1,1), )\\\\\\\[0\\\\\\\] # Verify correctness against A ≈ L @ L.T A = mx.random.normal((256, 256)); A = (A + A.T)/2 + 1e-1\\\\\\\*mx.eye(256) L = block\\\\\\\_cholesky(A, block\\\\\\\_size=16) ok = mx.allclose(A, L @ L.T, rtol=1e-4, atol=1e-4) \\\\\\\`\\\\\\\`\\\\\\\` Notes: - Keep block sizes modest; prefer stability over maximal parallelism. - Fall back to CPU \\\\\\\`mx.linalg.cholesky\\\\\\\` for ill‑conditioned matrices or when precision is critical. ### QR on Metal (Tiled + Guards) - Built‑in \\\\\\\`mx.linalg.qr\\\\\\\` may run on CPU depending on dtype/backend; large GPU QR benefits from block/panel updates. - Strategy: compute Householder transforms in panels, update trailing matrix in tiles; add stability guards and fallbacks. \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx # Reduced QR example and validation m, n = 512, 256 A = mx.random.normal((m, n)) Q, R = mx.linalg.qr(A) # reduced by default if m >= n # Orthogonality check: ||I - Q^T Q||\\\\\\\_F / n I = mx.eye(Q.shape\\\\\\\[1\\\\\\\]) orth\\\\\\\_err = mx.linalg.norm(I - (Q.T @ Q)) / Q.shape\\\\\\\[1\\\\\\\] # Triangularity check: lower part of R should be ~0 lower\\\\\\\_mask = mx.tril(mx.ones\\\\\\\_like(R), k=-1) tri\\\\\\\_err = mx.linalg.norm(lower\\\\\\\_mask \\\\\\\* R) print("orth\\\\\\\_err:", float(orth\\\\\\\_err.item()), "tri\\\\\\\_err:", float(tri\\\\\\\_err.item())) # CPU fallback with float64 for tough problems with mx.default\\\\\\\_device(mx.cpu): Q64, R64 = mx.linalg.qr(A.astype(mx.float64)) \\\\\\\`\\\\\\\`\\\\\\\` Notes and guards (for custom kernels): - Use modest panel widths and threadgroup sizes; synchronize phases with \\\\\\\`threadgroup\\\\\\\_barrier\\\\\\\`. - Add diagonal jitter or pivot guards when norms are tiny; detect NaN/Inf and fall back to CPU. - Re‑orthogonalize a panel if \\\\\\\`||I - Q^T Q||\\\\\\\` exceeds a threshold. - Consider limb‑based accumulation (see HPC16x8) for inner products to maintain orthogonality on GPU. - Thread planning (Apple GPUs): use execution width 32; cap threadgroup size ≤1024; launch ≤ one thread per element and round counts to a multiple of the execution width. #### Limb‑based vᵀv accumulation (Metal pattern) - For improved stability, accumulate \\\\\\\`v^T v\\\\\\\` in a wider fixed‑point format using 16‑bit limbs. - Typical setup: \\\\\\\`NUM\\\\\\\_LIMBS = 8\\\\\\\` with radix \\\\\\\`2^16\\\\\\\` gives a ~128‑bit accumulator across threads. \\\\\\\`\\\\\\\`\\\\\\\`c // Each thread accumulates partial limbs for its slice threadgroup uint thread\\\\\\\_limbs\\\\\\\[WARP\\\\\\\_SIZE \\\\\\\* NUM\\\\\\\_LIMBS\\\\\\\]; uint local\\\\\\\_limb\\\\\\\[NUM\\\\\\\_LIMBS\\\\\\\] = {0u}; for (uint i = k + tid; i < m; i += grid\\\\\\\_sz) { uint bits = as\\\\\\\_type(R\\\\\\\_out\\\\\\\[i\\\\\\\*n + k\\\\\\\]); ushort lo = bits & 0xFFFFu; ushort hi = (bits >> 16) & 0xFFFFu; uint p0 = uint(lo\\\\\\\*lo), p1 = uint(hi\\\\\\\*hi), pc = uint(lo\\\\\\\*hi) << 1; local\\\\\\\_limb\\\\\\\[0\\\\\\\] += p0 & 0xFFFFu; local\\\\\\\_limb\\\\\\\[1\\\\\\\] += (p0 >> 16) + (pc & 0xFFFFu); local\\\\\\\_limb\\\\\\\[2\\\\\\\] += (pc >> 16) + (p1 & 0xFFFFu); local\\\\\\\_limb\\\\\\\[3\\\\\\\] += p1 >> 16; } // Write to shared, then reduce + carry propagate to get vtv as float \\\\\\\`\\\\\\\`\\\\\\\` Notes: - Reduce across threads, propagate carries, then convert limbs back to float via radix expansion. - Use this for \\\\\\\`v^T v\\\\\\\` and optionally for dot products in reflector application when drift is observed. #### Running the QR Metal kernel (wrapper) \\\\\\\`\\\\\\\`\\\\\\\`python import signal, time import mlx.core as mx def run\\\\\\\_qr\\\\\\\_kernel(A: mx.array, kernel, debug=False, timeout\\\\\\\_seconds=1.0): m, n = A.shape exec\\\\\\\_width = 32 tgroup = exec\\\\\\\_width # one thread per element, conservatively capped total\\\\\\\_elems = m\\\\\\\*m + m\\\\\\\*n + m\\\\\\\*n max\\\\\\\_total = 1024 nthreads = min(max\\\\\\\_total, total\\\\\\\_elems) # round up to multiple of threadgroup nthreads = ((nthreads + tgroup - 1) // tgroup) \\\\\\\* tgroup grid = (nthreads, 1, 1) threadgroup = (tgroup, 1, 1) # small timeout guard def timeout\\\\\\\_handler(signum, frame): raise TimeoutError(f"QR kernel timed out after {timeout\\\\\\\_seconds}s") orig = signal.getsignal(signal.SIGALRM) signal.signal(signal.SIGALRM, timeout\\\\\\\_handler) signal.alarm(int(timeout\\\\\\\_seconds)) try: Q, R, dbg = kernel( inputs=\\\\\\\[A\\\\\\\], output\\\\\\\_shapes=\\\\\\\[(m, m), (m, n), (16,)\\\\\\\], output\\\\\\\_dtypes=\\\\\\\[mx.float32, mx.float32, mx.float32\\\\\\\], grid=grid, threadgroup=threadgroup, verbose=debug ) finally: signal.alarm(0); signal.signal(signal.SIGALRM, orig) return Q, R, dbg \\\\\\\`\\\\\\\`\\\\\\\` #### QR debug codes (dbg) - \\\\\\\`1\\\\\\\`: matrix too large - \\\\\\\`2\\\\\\\`: workload too large - \\\\\\\`3\\\\\\\`: too many iterations - \\\\\\\`4\\\\\\\`: numerical instability in norm calculation - \\\\\\\`5\\\\\\\`: numerical instability in vᵀv calculation #### Apple GPU sizing (heuristics) - Execution width 32; threadgroup size ≤ 1024. - Tiny (≤8): single threadgroup of 32. - Small (≤32): ≈ one thread per element, cap ≤ 256; round to width. - Medium+: cap total launched threads ≤ 1024 and round to width. ### SVD Notes (Shapes, Batching, Backend) - Supports rectangular and batched inputs: for array shape \\\\\\\`(..., m, n)\\\\\\\`, SVD is applied to the last two dims for each leading batch index. - Returns \\\\\\\`U, S, Vt\\\\\\\` when \\\\\\\`compute\\\\\\\_uv=True\\\\\\\` and \\\\\\\`S\\\\\\\` only when \\\\\\\`compute\\\\\\\_uv=False\\\\\\\`, satisfying \\\\\\\`A = U @ diag(S) @ Vt\\\\\\\`. - If you hit backend errors on a given device, scope to CPU for SVD and continue the rest of the pipeline on GPU as needed. ### Examples \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx # Solve vs inv(A) @ b A = mx.random.normal((4, 4)) b = mx.random.normal((4,)) x = mx.linalg.solve(A, b) # Batched solve: (B, M, M) @ (B, M) Ab = mx.random.normal((8, 4, 4)) bb = mx.random.normal((8, 4)) xb = mx.linalg.solve(Ab, bb) # SVD (optionally on CPU if your GPU backend errors) with mx.default\\\\\\\_device(mx.cpu): U, S, Vt = mx.linalg.svd(A) print("square A shapes:", U.shape, S.shape, Vt.shape) # Rectangular SVD Arect = mx.random.normal((5, 3)) with mx.default\\\\\\\_device(mx.cpu): U2, S2, Vt2 = mx.linalg.svd(Arect) print("rect A shapes:", U2.shape, S2.shape, Vt2.shape) # Singular values only with mx.default\\\\\\\_device(mx.cpu): S\\\\\\\_only = mx.linalg.svd(Arect, compute\\\\\\\_uv=False) print("S only:", S\\\\\\\_only.shape) # Cholesky with jitter for PSD matrices M = mx.random.normal((4, 4)) PSD = M @ M.T L = mx.linalg.cholesky(PSD + 1e-4 \\\\\\\* mx.eye(4)) \\\\\\\`\\\\\\\`\\\\\\\` \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/linalg.rst "Download source file") - .pdf # Linear Algebra # Linear Algebra | | | |----|----| | \\\[\\\`\\\`inv\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.inv.html#mlx.core.linalg.inv "mlx.core.linalg.inv")(a, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, stream\\\\\\\\\\\\\\\]) | Compute the inverse of a square matrix. | | \\\[\\\`\\\`tri\\\_inv\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.tri\\\_inv.html#mlx.core.linalg.tri\\\_inv "mlx.core.linalg.tri\\\_inv")(a\\\\\\\\\\\\\\\[, upper, stream\\\\\\\\\\\\\\\]) | Compute the inverse of a triangular square matrix. | | \\\[\\\`\\\`norm\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.norm.html#mlx.core.linalg.norm "mlx.core.linalg.norm")(a, /\\\\\\\\\\\\\\\[, ord, axis, keepdims, stream\\\\\\\\\\\\\\\]) | Matrix or vector norm. | | \\\[\\\`\\\`cholesky\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.cholesky.html#mlx.core.linalg.cholesky "mlx.core.linalg.cholesky")(a\\\\\\\\\\\\\\\[, upper, stream\\\\\\\\\\\\\\\]) | Compute the Cholesky decomposition of a real symmetric positive semi-definite matrix. | | \\\[\\\`\\\`cholesky\\\_inv\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.cholesky\\\_inv.html#mlx.core.linalg.cholesky\\\_inv "mlx.core.linalg.cholesky\\\_inv")(L\\\\\\\\\\\\\\\[, upper, stream\\\\\\\\\\\\\\\]) | Compute the inverse of a real symmetric positive semi-definite matrix using it's Cholesky decomposition. | | \\\[\\\`\\\`cross\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.cross.html#mlx.core.linalg.cross "mlx.core.linalg.cross")(a, b\\\\\\\\\\\\\\\[, axis, stream\\\\\\\\\\\\\\\]) | Compute the cross product of two arrays along a specified axis. | | \\\[\\\`\\\`qr\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.qr.html#mlx.core.linalg.qr "mlx.core.linalg.qr")(a, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, stream\\\\\\\\\\\\\\\]) | The QR factorization of the input matrix. | | \\\[\\\`\\\`svd\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.svd.html#mlx.core.linalg.svd "mlx.core.linalg.svd")(a\\\\\\\\\\\\\\\[, compute\\\\\\\_uv, stream\\\\\\\\\\\\\\\]) | The Singular Value Decomposition (SVD) of the input matrix. | | \\\[\\\`\\\`eigvalsh\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.eigvalsh.html#mlx.core.linalg.eigvalsh "mlx.core.linalg.eigvalsh")(a\\\\\\\\\\\\\\\[, UPLO, stream\\\\\\\\\\\\\\\]) | Compute the eigenvalues of a complex Hermitian or real symmetric matrix. | | \\\[\\\`\\\`eigh\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.eigh.html#mlx.core.linalg.eigh "mlx.core.linalg.eigh")(a\\\\\\\\\\\\\\\[, UPLO, stream\\\\\\\\\\\\\\\]) | Compute the eigenvalues and eigenvectors of a complex Hermitian or real symmetric matrix. | | \\\[\\\`\\\`lu\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.lu.html#mlx.core.linalg.lu "mlx.core.linalg.lu")(a, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, stream\\\\\\\\\\\\\\\]) | Compute the LU factorization of the given matrix \\\`\\\\\\\`A\\\\\\\`\\\`. | | \\\[\\\`\\\`lu\\\_factor\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.lu\\\_factor.html#mlx.core.linalg.lu\\\_factor "mlx.core.linalg.lu\\\_factor")(a, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, stream\\\\\\\\\\\\\\\]) | Computes a compact representation of the LU factorization. | | \\\[\\\`\\\`pinv\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.pinv.html#mlx.core.linalg.pinv "mlx.core.linalg.pinv")(a, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, stream\\\\\\\\\\\\\\\]) | Compute the (Moore-Penrose) pseudo-inverse of a matrix. | | \\\[\\\`\\\`solve\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.solve.html#mlx.core.linalg.solve "mlx.core.linalg.solve")(a, b, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, stream\\\\\\\\\\\\\\\]) | Compute the solution to a system of linear equations \\\`\\\\\\\`AX\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`=\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`B\\\\\\\`\\\`. | | \\\[\\\`\\\`solve\\\_triangular\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.solve\\\_triangular.html#mlx.core.linalg.solve\\\_triangular "mlx.core.linalg.solve\\\_triangular")(a, b, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, upper, stream\\\\\\\\\\\\\\\]) | Computes the solution of a triangular system of linear equations \\\`\\\\\\\`AX\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`=\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`B\\\\\\\`\\\`. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.fft.irfftn.html "previous page") previous mlx.core.fft.irfftn \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.linalg.inv.html "next page") next mlx.core.linalg.inv By MLX Contributors © Copyright 2023, MLX Contributors.
