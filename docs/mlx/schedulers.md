Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers/schedulers.rst "Download source file") - .pdf # Schedulers # Schedulers | | | |----|----| | \\\[\\\`\\\`cosine\\\_decay\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.cosine\\\_decay.html#mlx.optimizers.cosine\\\_decay "mlx.optimizers.cosine\\\_decay")(init, decay\\\\\\\_steps\\\\\\\\\\\\\\\[, end\\\\\\\\\\\\\\\]) | Make a cosine decay scheduler. | | \\\[\\\`\\\`exponential\\\_decay\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.exponential\\\_decay.html#mlx.optimizers.exponential\\\_decay "mlx.optimizers.exponential\\\_decay")(init, decay\\\\\\\_rate) | Make an exponential decay scheduler. | | \\\[\\\`\\\`join\\\_schedules\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.join\\\_schedules.html#mlx.optimizers.join\\\_schedules "mlx.optimizers.join\\\_schedules")(schedules, boundaries) | Join multiple schedules to create a new schedule. | | \\\[\\\`\\\`linear\\\_schedule\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.linear\\\_schedule.html#mlx.optimizers.linear\\\_schedule "mlx.optimizers.linear\\\_schedule")(init, end, steps) | Make a linear scheduler. | | \\\[\\\`\\\`step\\\_decay\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.step\\\_decay.html#mlx.optimizers.step\\\_decay "mlx.optimizers.step\\\_decay")(init, decay\\\\\\\_rate, step\\\\\\\_size) | Make a step decay scheduler. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.MultiOptimizer.html "previous page") previous mlx.optimizers.MultiOptimizer \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.cosine\\\_decay.html "next page") next mlx.optimizers.cosine\\\\\\\_decay By MLX Contributors © Copyright 2023, MLX Contributors.
