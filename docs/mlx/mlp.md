Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes \\\\\\\`\\\\\\\`\\\\\\\`python from mlx.optimizers import AdamW model = MLP(num\\\\\\\_layers=2, input\\\\\\\_dim=784, hidden\\\\\\\_dim=256, output\\\\\\\_dim=10) params = model.parameters() opt = AdamW(3e-4) def loss\\\\\\\_fn(p, x, y): logits = model.apply(p, x) return nn.losses.cross\\\\\\\_entropy(logits, y) for step, (xb, yb) in enumerate(train\\\\\\\_loader): loss, grads = mx.value\\\\\\\_and\\\\\\\_grad(loss\\\\\\\_fn)(params, xb, yb) params = opt.update(params, grads) if step % 100 == 0: print(step, loss.item()) model.eval() \\\\\\\`\\\\\\\`\\\\\\\` Switch to \\\\\\\`model.eval()\\\\\\\` for validation to disable dropout and training‑mode behavior in norms. \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/examples/mlp.rst "Download source file") - .pdf # Multi-Layer Perceptron # Multi-Layer Perceptron In this example we’ll learn to use \\\`\\\\\\\`mlx.nn\\\\\\\`\\\` by implementing a simple multi-layer perceptron to classify MNIST. As a first step import the MLX packages we need: \\\`\\\`\\\`python import mlx.core as mx import mlx.nn as nn import mlx.optimizers as optim import numpy as np \\\`\\\`\\\` The model is defined as the \\\`\\\\\\\`MLP\\\\\\\`\\\` class which inherits from \\\[\\\`\\\`mlx.nn.Module\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.Module"). We follow the standard idiom to make a new module: 1. Define an \\\`\\\\\\\`\\\\\\\_\\\\\\\_init\\\\\\\_\\\\\\\_\\\\\\\`\\\` where the parameters and/or submodules are setup. See the \\\[Module class docs\\\](https://ml-explore.github.io/mlx/build/html/python/nn.html#module-class) for more information on how \\\[\\\`\\\`mlx.nn.Module\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.Module") registers parameters. 2. Define a \\\`\\\\\\\`\\\\\\\_\\\\\\\_call\\\\\\\_\\\\\\\_\\\\\\\`\\\` where the computation is implemented. \\\`\\\`\\\`python class MLP(nn.Module): def \\\_\\\_init\\\_\\\_( self, num\\\_layers: int, input\\\_dim: int, hidden\\\_dim: int, output\\\_dim: int ): super().\\\_\\\_init\\\_\\\_() layer\\\_sizes = \\\[input\\\_dim\\\] + \\\[hidden\\\_dim\\\] \\\* num\\\_layers + \\\[output\\\_dim\\\] self.layers = \\\[ nn.Linear(idim, odim) for idim, odim in zip(layer\\\_sizes\\\[:-1\\\], layer\\\_sizes\\\[1:\\\]) \\\] def \\\_\\\_call\\\_\\\_(self, x): for l in self.layers\\\[:-1\\\]: x = mx.maximum(l(x), 0.0) return self.layers\\\[-1\\\](x) \\\`\\\`\\\` We define the loss function which takes the mean of the per-example cross entropy loss. The \\\`\\\\\\\`mlx.nn.losses\\\\\\\`\\\` sub-package has implementations of some commonly used loss functions. \\\`\\\`\\\`python def loss\\\_fn(model, X, y): return mx.mean(nn.losses.cross\\\_entropy(model(X), y)) \\\`\\\`\\\` We also need a function to compute the accuracy of the model on the validation set: \\\`\\\`\\\`python def eval\\\_fn(model, X, y): return mx.mean(mx.argmax(model(X), axis=1) == y) \\\`\\\`\\\` Next, setup the problem parameters and load the data. To load the data, you need our \\\[mnist data loader\\\](https://github.com/ml-explore/mlx-examples/blob/main/mnist/mnist.py), which we will import as \\\`\\\\\\\`mnist\\\\\\\`\\\`. \\\`\\\`\\\`python num\\\_layers = 2 hidden\\\_dim = 32 num\\\_classes = 10 batch\\\_size = 256 num\\\_epochs = 10 learning\\\_rate = 1e-1 # Load the data import mnist train\\\_images, train\\\_labels, test\\\_images, test\\\_labels = map( mx.array, mnist.mnist() ) \\\`\\\`\\\` Since we’re using SGD, we need an iterator which shuffles and constructs minibatches of examples in the training set: \\\`\\\`\\\`python def batch\\\_iterate(batch\\\_size, X, y): perm = mx.array(np.random.permutation(y.size)) for s in range(0, y.size, batch\\\_size): ids = perm\\\[s : s + batch\\\_size\\\] yield X\\\[ids\\\], y\\\[ids\\\] \\\`\\\`\\\` Finally, we put it all together by instantiating the model, the \\\[\\\`\\\`mlx.optimizers.SGD\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.SGD.html#mlx.optimizers.SGD "mlx.optimizers.SGD") optimizer, and running the training loop: \\\`\\\`\\\`python # Load the model model = MLP(num\\\_layers, train\\\_images.shape\\\[-1\\\], hidden\\\_dim, num\\\_classes) mx.eval(model.parameters()) # Get a function which gives the loss and gradient of the # loss with respect to the model's trainable parameters loss\\\_and\\\_grad\\\_fn = nn.value\\\_and\\\_grad(model, loss\\\_fn) # Instantiate the optimizer optimizer = optim.SGD(learning\\\_rate=learning\\\_rate) for e in range(num\\\_epochs): for X, y in batch\\\_iterate(batch\\\_size, train\\\_images, train\\\_labels): loss, grads = loss\\\_and\\\_grad\\\_fn(model, X, y) # Update the optimizer state and model parameters # in a single call optimizer.update(model, grads) # Force a graph evaluation mx.eval(model.parameters(), optimizer.state) accuracy = eval\\\_fn(model, test\\\_images, test\\\_labels) print(f"Epoch {e}: Test accuracy {accuracy.item():.3f}") \\\`\\\`\\\` Note The \\\[\\\`\\\`mlx.nn.value\\\_and\\\_grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.nn.value\\\_and\\\_grad.html#mlx.nn.value\\\_and\\\_grad "mlx.nn.value\\\_and\\\_grad") function is a convenience function to get the gradient of a loss with respect to the trainable parameters of a model. This should not be confused with \\\[\\\`\\\`mlx.core.value\\\_and\\\_grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.value\\\_and\\\_grad.html#mlx.core.value\\\_and\\\_grad "mlx.core.value\\\_and\\\_grad"). The model should train to a decent accuracy (about 95%) after just a few passes over the training set. The \\\[full example\\\](https://github.com/ml-explore/mlx-examples/tree/main/mnist) is available in the MLX GitHub repo. \\\[\\\](https://ml-explore.github.io/mlx/build/html/examples/linear\\\_regression.html "previous page") previous Linear Regression \\\[\\\](https://ml-explore.github.io/mlx/build/html/examples/llama-inference.html "next page") next LLM inference By MLX Contributors © Copyright 2023, MLX Contributors.
