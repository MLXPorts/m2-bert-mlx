Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - Norms (\\\\\\\`rms\\\\\\\_norm\\\\\\\`, \\\\\\\`layer\\\\\\\_norm\\\\\\\`): keep weights/bias in the same dtype as activations to avoid hidden casts. - Scaled dot-product attention: ensure mask broadcasting is correct; scale typically \\\\\\\`1/sqrt(d\\\\\\\_k)\\\\\\\`. - \\\\\\\`fast.metal\\\\\\\_kernel\\\\\\\`: prototype on tiny shapes; validate correctness vs a pure-MLX version before scaling up. ### Examples \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx # Scaled dot-product attention (single head) Q = mx.random.normal((2, 8, 16)) # (batch, seq\\\\\\\_q, d) K = mx.random.normal((2, 8, 16)) # (batch, seq\\\\\\\_k, d) V = mx.random.normal((2, 8, 32)) # (batch, seq\\\\\\\_k, dv) scale = 1.0 / mx.sqrt(mx.array(16.0)) # Optional mask: True for keep, False for mask out mask = mx.array(\\\\\\\[\\\\\\\[True\\\\\\\]\\\\\\\*8 + \\\\\\\[False\\\\\\\]\\\\\\\*0\\\\\\\]\\\\\\\*2)\\\\\\\[:, :8\\\\\\\] # (2, 8) mask = mx.reshape(mask, (2, 1, 8)) # broadcast to (B, 1, seq\\\\\\\_k) O = mx.fast.scaled\\\\\\\_dot\\\\\\\_product\\\\\\\_attention(Q, K, V, scale=scale) \\\\\\\`\\\\\\\`\\\\\\\` ### Metal Kernel Quick Reference - \\\\\\\`mx.fast.metal\\\\\\\_kernel(name, input\\\\\\\_names, output\\\\\\\_names, source, header="", ensure\\\\\\\_row\\\\\\\_contiguous=True, atomic\\\\\\\_outputs=False)\\\\\\\` returns a callable. - Call the returned kernel with: - \\\\\\\`inputs=\\\\\\\[...\\\\\\\]\\\\\\\`, \\\\\\\`template=\\\\\\\[("T", dtype), ("K", 3), ...\\\\\\\]\\\\\\\` - \\\\\\\`grid=(n,1,1)\\\\\\\`, \\\\\\\`threadgroup=(tg,1,1)\\\\\\\` - \\\\\\\`output\\\\\\\_shapes=\\\\\\\[...\\\\\\\]\\\\\\\`, \\\\\\\`output\\\\\\\_dtypes=\\\\\\\[...\\\\\\\]\\\\\\\` - Optional: \\\\\\\`ensure\\\\\\\_row\\\\\\\_contiguous=False\\\\\\\` (use \\\\\\\`elem\\\\\\\_to\\\\\\\_loc\\\\\\\`), \\\\\\\`init\\\\\\\_value=0\\\\\\\`, \\\\\\\`atomic\\\\\\\_outputs=True\\\\\\\`, \\\\\\\`verbose=True\\\\\\\` Header/verbose example: \\\\\\\`\\\\\\\`\\\\\\\`python src = """ // body uses helper declared in header uint elem = thread\\\\\\\_position\\\\\\\_in\\\\\\\_grid.x; out\\\\\\\[elem\\\\\\\] = helper(inp\\\\\\\[elem\\\\\\\]); """ hdr = """ inline T helper(T v) { return metal::exp(v); } """ k = mx.fast.metal\\\\\\\_kernel( name="myexp\\\\\\\_hdr", input\\\\\\\_names=\\\\\\\["inp"\\\\\\\], output\\\\\\\_names=\\\\\\\["out"\\\\\\\], source=src, header=hdr ) out = k(inputs=\\\\\\\[a\\\\\\\], template=\\\\\\\[("T", a.dtype)\\\\\\\], grid=(a.size,1,1), threadgroup=(256,1,1), output\\\\\\\_shapes=\\\\\\\[a.shape\\\\\\\], output\\\\\\\_dtypes=\\\\\\\[a.dtype\\\\\\\], verbose=True)\\\\\\\[0\\\\\\\] \\\\\\\`\\\\\\\`\\\\\\\` \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/fast.rst "Download source file") - .pdf # Fast # Fast | | | |----|----| | \\\[\\\`\\\`rms\\\_norm\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.fast.rms\\\_norm.html#mlx.core.fast.rms\\\_norm "mlx.core.fast.rms\\\_norm")(x, weight, eps, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, stream\\\\\\\\\\\\\\\]) | Root Mean Square normalization (RMS norm). | | \\\[\\\`\\\`layer\\\_norm\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.fast.layer\\\_norm.html#mlx.core.fast.layer\\\_norm "mlx.core.fast.layer\\\_norm")(x, weight, bias, eps, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, stream\\\\\\\\\\\\\\\]) | Layer normalization. | | \\\[\\\`\\\`rope\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.fast.rope.html#mlx.core.fast.rope "mlx.core.fast.rope")(a, dims, \\\\\\\\\\\\\\\*, traditional, base, scale, ...) | Apply rotary positional encoding to the input. | | \\\[\\\`\\\`scaled\\\_dot\\\_product\\\_attention\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.fast.scaled\\\_dot\\\_product\\\_attention.html#mlx.core.fast.scaled\\\_dot\\\_product\\\_attention "mlx.core.fast.scaled\\\_dot\\\_product\\\_attention")(q, k, v, \\\\\\\\\\\\\\\*, scale) | A fast implementation of multi-head attention: \\\`\\\\\\\`O\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`=\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`softmax(Q\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`@\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`K.T,\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`dim=-1)\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`@\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`V\\\\\\\`\\\`. | | \\\[\\\`\\\`metal\\\_kernel\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.fast.metal\\\_kernel.html#mlx.core.fast.metal\\\_kernel "mlx.core.fast.metal\\\_kernel")(name, input\\\\\\\_names, ...\\\\\\\\\\\\\\\[, ...\\\\\\\\\\\\\\\]) | A jit-compiled custom Metal kernel defined from a source string. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.vmap.html "previous page") previous mlx.core.vmap \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.fast.rms\\\_norm.html "next page") next mlx.core.fast.rms\\\\\\\_norm By MLX Contributors © Copyright 2023, MLX Contributors.
