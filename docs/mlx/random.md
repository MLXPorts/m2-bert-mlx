Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/random.rst "Download source file") - .pdf # Random # Random Random sampling functions in MLX use an implicit global PRNG state by default. However, all function take an optional \\\`\\\\\\\`key\\\\\\\`\\\` keyword argument for when more fine-grained control or explicit state management is needed. For example, you can generate random numbers with: \\\`\\\`\\\`python for \\\_ in range(3): print(mx.random.uniform()) \\\`\\\`\\\` which will print a sequence of unique pseudo random numbers. Alternatively you can explicitly set the key: \\\`\\\`\\\`python key = mx.random.key(0) for \\\_ in range(3): print(mx.random.uniform(key=key)) \\\`\\\`\\\` which will yield the same pseudo random number at each iteration. Following \\\[JAX’s PRNG design\\\](https://jax.readthedocs.io/en/latest/jep/263-prng.html) we use a splittable version of Threefry, which is a counter-based PRNG. ## Curated Notes - Reproducibility: - Global: \\\\\\\`mx.random.seed(n)\\\\\\\` seeds the global generator. - Local: pass \\\\\\\`key=mx.random.key(n)\\\\\\\` for explicit, deterministic draws. Reuse the same key to reproduce the same value; use \\\\\\\`mx.random.split(key, num=...)\\\\\\\` to create independent subkeys. - Device control: random ops do not take a \\\\\\\`device=\\\\\\\` argument; use default device or per‑op \\\\\\\`stream\\\\\\\`. - Dtypes/shapes: most random APIs accept \\\\\\\`shape=\\\\\\\` and \\\\\\\`dtype=\\\\\\\`; if omitted, defaults are used (often float32). Construct then \\\\\\\`astype(...)\\\\\\\` if an API lacks a \\\\\\\`dtype\\\\\\\` parameter. - Ranges/semantics: - \\\\\\\`randint(low, high, ...)\\\\\\\` is typically half‑open \\\\\\\[low, high). - \\\\\\\`permutation(x, axis=...)\\\\\\\` shuffles along an axis or returns a permuted copy for 1‑D inputs. - \\\\\\\`categorical(logits, axis=...)\\\\\\\` samples indices; ensure logits axis matches \\\\\\\`axis\\\\\\\`. ### Reproducible Patterns \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx # Global seed mx.random.seed(42) a = mx.random.normal(shape=(2, 3)) # Explicit keys key = mx.random.key(0) u1 = mx.random.uniform(shape=(2, 2), key=key) u2 = mx.random.uniform(shape=(2, 2), key=key) # identical to u1 # Independent subkeys k1, k2 = mx.random.split(mx.random.key(123), num=2) n1 = mx.random.normal(shape=(4,), key=k1) n2 = mx.random.normal(shape=(4,), key=k2) \\\\\\\`\\\\\\\`\\\\\\\` ### Common Distributions \\\\\\\`\\\\\\\`\\\\\\\`python # Uniform in \\\\\\\[low, high) u = mx.random.uniform(low=-1.0, high=1.0, shape=(3, 3)) # Normal with loc/scale n = mx.random.normal(loc=0.0, scale=2.0, shape=(1024,)) # Truncated normal in \\\\\\\[lower, upper\\\\\\\] tn = mx.random.truncated\\\\\\\_normal(lower=-2.0, upper=2.0, shape=(5,)) # Categorical over last axis by default logits = mx.random.normal(shape=(8, 10)) idx = mx.random.categorical(logits, axis=-1) # Integers: \\\\\\\[low, high) ri = mx.random.randint(low=0, high=10, shape=(4, 4), dtype=mx.int32) # Permutation p = mx.random.permutation(mx.arange(10)) \\\\\\\`\\\\\\\`\\\\\\\` ### Device/Stream Examples \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx # Route random draws to CPU explicitly cpu\\\\\\\_u = mx.random.uniform(shape=(2, 2), stream=mx.cpu) # Or set global default device (no device= on ops) mx.set\\\\\\\_default\\\\\\\_device(mx.cpu) cpu\\\\\\\_n = mx.random.normal(shape=(128,)) \\\\\\\`\\\\\\\`\\\\\\\` ### Vectorizing With Independent Randomness When using \\\\\\\`mx.vmap\\\\\\\`, split a single seed key into per‑example subkeys so each parallel instance has its own independent stream. \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx def my\\\\\\\_random\\\\\\\_function(key): # Generates a 5x5 normal draw using the provided key return mx.random.normal(shape=(5, 5), key=key) # 1) Start from a reproducible key initial\\\\\\\_key = mx.random.key(42) # 2) Split into subkeys for a batch batch\\\\\\\_size = 4 subkeys = mx.random.split(initial\\\\\\\_key, num=batch\\\\\\\_size) # 3) Vectorize: vmap maps over the first axis of subkeys vf = mx.vmap(my\\\\\\\_random\\\\\\\_function) parallel = vf(subkeys) # shape: (4, 5, 5) # Reproducibility: splitting again from the same initial key yields the same draws subkeys2 = mx.random.split(initial\\\\\\\_key, num=batch\\\\\\\_size) parallel2 = vf(subkeys2) print(mx.allclose(parallel, parallel2).item()) # True ## MLX vs PyTorch Random (Quick Comparison) - State model: - MLX: explicit, functional keys; pass \\\\\\\`key=\\\\\\\` to each op. Split keys with \\\\\\\`mx.random.split\\\\\\\` for independent streams. - PyTorch: implicit global state (\\\\\\\`torch.manual\\\\\\\_seed\\\\\\\`); finer control via stateful \\\\\\\`torch.Generator\\\\\\\`. - Reproducibility: - MLX: seed → key → split; deterministic across \\\\\\\`vmap\\\\\\\`/parallel code when subkeys are managed explicitly. - PyTorch: global/manual seeding works in simple cases; care needed across threads/devices. - APIs: - MLX \\\\\\\`normal(shape, loc, scale, key=...)\\\\\\\`; PyTorch \\\\\\\`randn(size) \\\\\\\* std + mean\\\\\\\`. - MLX \\\\\\\`uniform(low, high, shape, key=...)\\\\\\\`; PyTorch \\\\\\\`rand(size) \\\\\\\* (high-low) + low\\\\\\\`. - MLX \\\\\\\`randint(low, high, shape, key=...)\\\\\\\` (half‑open); PyTorch \\\\\\\`randint(low, high, size)\\\\\\\` similar. - MLX \\\\\\\`permutation(x, axis, key=...)\\\\\\\`; PyTorch \\\\\\\`randperm(n)\\\\\\\` returns indices for 1‑D. Example (MNIST‑style batch): \\\\\\\`\\\\\\\`\\\\\\\`python # MLX k = mx.random.key(0) k\\\\\\\_img, k\\\\\\\_lbl = mx.random.split(k, num=2) noise = mx.random.normal(shape=(64, 1, 28, 28), key=k\\\\\\\_img) labels = mx.random.randint(0, 10, shape=(64,), key=k\\\\\\\_lbl) # PyTorch (conceptually) # torch.manual\\\\\\\_seed(0) # noise = torch.randn(64, 1, 28, 28) # labels = torch.randint(0, 10, (64,)) \\\\\\\`\\\\\\\`\\\\\\\` ### Avoid Seeding Inside Transformed Code - Don’t call \\\\\\\`mx.random.seed(...)\\\\\\\` inside functions you pass to \\\\\\\`mx.vmap\\\\\\\`, \\\\\\\`mx.value\\\\\\\_and\\\\\\\_grad\\\\\\\`, or \\\\\\\`mx.compile\\\\\\\`. - Seeding mutates global state and can collapse independence across mapped/parallel instances or break caching. Bad (global seed inside mapped fn): \\\\\\\`\\\\\\\`\\\\\\\`python def f\\\\\\\_bad(x): mx.random.seed(0) # resets global state every call return mx.random.normal(shape=x.shape) vf = mx.vmap(f\\\\\\\_bad) vf(mx.ones((4, 8))) # rows likely identical \\\\\\\`\\\\\\\`\\\\\\\` Good (explicit subkeys): \\\\\\\`\\\\\\\`\\\\\\\`python def f\\\\\\\_good(key, x): return mx.random.normal(shape=x.shape, key=key) vf = mx.vmap(f\\\\\\\_good) keys = mx.random.split(mx.random.key(0), num=4) vf(keys, mx.ones((4, 8))) # independent rows \\\\\\\`\\\\\\\`\\\\\\\` ### Bridging Missing Distributions (Patterns) Exponential via inverse CDF: \\\\\\\`\\\\\\\`\\\\\\\`python def exponential(shape, scale=1.0, key=None, dtype=mx.float32): u = mx.random.uniform(shape=shape, key=key).astype(dtype) eps = mx.array(1e-9, dtype=dtype) return -mx.array(scale, dtype=dtype) \\\\\\\* mx.log(mx.maximum(1.0 - u, eps)) \\\\\\\`\\\\\\\`\\\\\\\` Poisson via Knuth for small λ and Normal approximation for large λ: \\\\\\\`\\\\\\\`\\\\\\\`python def poisson(shape, lam=1.0, key=None): lam = float(lam) if lam <= 0: return mx.zeros(shape, dtype=mx.int32) if lam > 15: n = mx.random.normal(shape=shape, loc=lam, scale=mx.sqrt(lam), key=key) return mx.maximum(mx.round(n), 0).astype(mx.int32) # Knuth L = mx.exp(-lam) k = mx.zeros(shape, dtype=mx.int32) p = mx.ones(shape) for \\\\\\\_ in range(max(100, int(lam\\\\\\\*5))): cont = p >= L if not mx.any(cont): break k = mx.where(cont, k + 1, k) u = mx.random.uniform(shape=shape, key=key) p = mx.where(cont, p \\\\\\\* u, p) return k \\\\\\\`\\\\\\\`\\\\\\\` Integer uniform (if \\\\\\\`randint\\\\\\\` is unavailable in your MLX version): \\\\\\\`\\\\\\\`\\\\\\\`python def randint(low, high, shape, key=None, dtype=mx.int32): u = mx.random.uniform(shape=shape, low=float(low), high=float(high), key=key) return mx.floor(u).astype(dtype) \\\\\\\`\\\\\\\`\\\\\\\` ### Shuffling and Permutations - 1‑D: \\\\\\\`mx.random.permutation(n, key=...)\\\\\\\` returns indices 0..n‑1 permuted. - Array: permute along an axis via advanced indexing. \\\\\\\`\\\\\\\`\\\\\\\`python def shuffle\\\\\\\_first\\\\\\\_dim(x, key=None): idx = mx.random.permutation(x.shape\\\\\\\[0\\\\\\\], key=key) return x\\\\\\\[idx\\\\\\\] \\\\\\\`\\\\\\\`\\\\\\\` Epoch/worker‑safe shuffles: \\\\\\\`\\\\\\\`\\\\\\\`python def epoch\\\\\\\_key(base\\\\\\\_seed, epoch, worker=0): return mx.random.key(int(base\\\\\\\_seed) ^ (epoch \\\\\\\* 0x9E3779B97F4A7C15) ^ worker) k = epoch\\\\\\\_key(0, epoch=5, worker=rank) idx = mx.random.permutation(N, key=k) \\\\\\\`\\\\\\\`\\\\\\\` ## PyTorch/NumPy‑Style RNG Shim (Compatibility) If you want a front‑end that feels like \\\\\\\`torch.Generator\\\\\\\`/global NumPy state without exposing keys to call sites, wrap MLX RNG in a tiny class. Internally it derives unique keys per call so parallel code stays reproducible, but the API looks familiar. \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx class RNG: def \\\\\\\_\\\\\\\_init\\\\\\\_\\\\\\\_(self, seed: int = 0): self.seed = int(seed) self.\\\\\\\_ctr = 0 def \\\\\\\_next\\\\\\\_key(self): # Derive a fresh key per call; no global state self.\\\\\\\_ctr += 1 return mx.random.key(self.seed ^ (0x9E3779B97F4A7C15 & 0xFFFFFFFFFFFF) ^ self.\\\\\\\_ctr) # Torch/NumPy‑like methods (no key arg exposed) def normal(self, shape, loc=0.0, scale=1.0, dtype=mx.float32): return mx.random.normal(shape=shape, loc=loc, scale=scale, key=self.\\\\\\\_next\\\\\\\_key()).astype(dtype) def uniform(self, shape, low=0.0, high=1.0, dtype=mx.float32): return mx.random.uniform(shape=shape, low=low, high=high, key=self.\\\\\\\_next\\\\\\\_key()).astype(dtype) def randint(self, low, high, shape, dtype=mx.int32): return mx.random.randint(low=low, high=high, shape=shape, dtype=dtype, key=self.\\\\\\\_next\\\\\\\_key()) def permutation(self, n\\\\\\\_or\\\\\\\_x): return mx.random.permutation(n\\\\\\\_or\\\\\\\_x, key=self.\\\\\\\_next\\\\\\\_key()) def categorical(self, logits, num\\\\\\\_samples): return mx.random.categorical(logits, num\\\\\\\_samples=num\\\\\\\_samples, key=self.\\\\\\\_next\\\\\\\_key()) # Usage g = RNG(seed=123) x = g.normal((2, 3)) idx = g.permutation(10) \\\\\\\`\\\\\\\`\\\\\\\` Notes: - Callers don’t pass keys; you get deterministic, independent draws per method call. - For strict global‑state emulation, expose \\\\\\\`set\\\\\\\_seed/get\\\\\\\_seed\\\\\\\` that mutate a module‑level RNG instance; prefer the instance API above for clarity and testability. ## Factory Helpers (rand/randn/like) MLX doesn’t ship NumPy/Torch‑style factory shorthands, but they’re easy to add: \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx def rand(shape, low=0.0, high=1.0, dtype=mx.float32, key=None): return mx.random.uniform(shape=shape, low=low, high=high, key=key).astype(dtype) def randn(shape, mean=0.0, std=1.0, dtype=mx.float32, key=None): return mx.random.normal(shape=shape, loc=mean, scale=std, key=key).astype(dtype) def randint(shape, low, high, dtype=mx.int32, key=None): return mx.random.randint(low=low, high=high, shape=shape, dtype=dtype, key=key) def rand\\\\\\\_like(x, low=0.0, high=1.0, key=None): return mx.random.uniform(shape=x.shape, low=low, high=high, key=key).astype(x.dtype) def randn\\\\\\\_like(x, mean=0.0, std=1.0, key=None): return mx.random.normal(shape=x.shape, loc=mean, scale=std, key=key).astype(x.dtype) def randint\\\\\\\_like(x, low, high, key=None): # Use x.dtype if it is an integer type; else default to int32 int\\\\\\\_dtype = x.dtype if str(x.dtype).startswith('int') else mx.int32 return mx.random.randint(low=low, high=high, shape=x.shape, dtype=int\\\\\\\_dtype, key=key) \\\\\\\`\\\\\\\`\\\\\\\` These mirror NumPy/Torch ergonomics and keep dtype/device consistent with a reference array when using the “\\\\\\\_like” variants. \\\\\\\`\\\\\\\`\\\\\\\` | | | |----|----| | \\\[\\\`\\\`bernoulli\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.bernoulli.html#mlx.core.random.bernoulli "mlx.core.random.bernoulli")(\\\\\\\\\\\\\\\[p, shape, key, stream\\\\\\\\\\\\\\\]) | Generate Bernoulli random values. | | \\\[\\\`\\\`categorical\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.categorical.html#mlx.core.random.categorical "mlx.core.random.categorical")(logits\\\\\\\\\\\\\\\[, axis, shape, ...\\\\\\\\\\\\\\\]) | Sample from a categorical distribution. | | \\\[\\\`\\\`gumbel\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.gumbel.html#mlx.core.random.gumbel "mlx.core.random.gumbel")(\\\\\\\\\\\\\\\[shape, dtype, key, stream\\\\\\\\\\\\\\\]) | Sample from the standard Gumbel distribution. | | \\\[\\\`\\\`key\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.key.html#mlx.core.random.key "mlx.core.random.key")(seed) | Get a PRNG key from a seed. | | \\\[\\\`\\\`normal\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.normal.html#mlx.core.random.normal "mlx.core.random.normal")(\\\\\\\\\\\\\\\[shape, dtype, loc, scale, key, stream\\\\\\\\\\\\\\\]) | Generate normally distributed random numbers. | | \\\[\\\`\\\`multivariate\\\_normal\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.multivariate\\\_normal.html#mlx.core.random.multivariate\\\_normal "mlx.core.random.multivariate\\\_normal")(mean, cov\\\\\\\\\\\\\\\[, shape, ...\\\\\\\\\\\\\\\]) | Generate jointly-normal random samples given a mean and covariance. | | \\\[\\\`\\\`randint\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.randint.html#mlx.core.random.randint "mlx.core.random.randint")(low, high\\\\\\\\\\\\\\\[, shape, dtype, key, stream\\\\\\\\\\\\\\\]) | Generate random integers from the given interval. | | \\\[\\\`\\\`seed\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.seed.html#mlx.core.random.seed "mlx.core.random.seed")(seed) | Seed the global PRNG. | | \\\[\\\`\\\`split\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.split.html#mlx.core.random.split "mlx.core.random.split")(key\\\\\\\\\\\\\\\[, num, stream\\\\\\\\\\\\\\\]) | Split a PRNG key into sub keys. | | \\\[\\\`\\\`truncated\\\_normal\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.truncated\\\_normal.html#mlx.core.random.truncated\\\_normal "mlx.core.random.truncated\\\_normal")(lower, upper\\\\\\\\\\\\\\\[, shape, ...\\\\\\\\\\\\\\\]) | Generate values from a truncated normal distribution. | | \\\[\\\`\\\`uniform\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.uniform.html#mlx.core.random.uniform "mlx.core.random.uniform")(\\\\\\\\\\\\\\\[low, high, shape, dtype, key, stream\\\\\\\\\\\\\\\]) | Generate uniformly distributed random numbers. | | \\\[\\\`\\\`laplace\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.laplace.html#mlx.core.random.laplace "mlx.core.random.laplace")(\\\\\\\\\\\\\\\[shape, dtype, loc, scale, key, stream\\\\\\\\\\\\\\\]) | Sample numbers from a Laplace distribution. | | \\\[\\\`\\\`permutation\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.permutation.html#mlx.core.random.permutation "mlx.core.random.permutation")(x\\\\\\\\\\\\\\\[, axis, key, stream\\\\\\\\\\\\\\\]) | Generate a random permutation or permute the entries of an array. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.zeros\\\_like.html "previous page") previous mlx.core.zeros\\\\\\\_like \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.random.bernoulli.html "next page") next mlx.core.random.bernoulli By MLX Contributors © Copyright 2023, MLX Contributors.
