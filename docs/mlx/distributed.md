Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - Initialize once per process: \\\\\\\`from mlx.core.distributed import init, is\\\\\\\_available; init() if is\\\\\\\_available() else ...\\\\\\\`. - Ensure both sender and receiver agree on \\\\\\\`shape\\\\\\\`/\\\\\\\`dtype\\\\\\\`; mismatches will fail at runtime. - Prefer collectives (\\\\\\\`all\\\\\\\_sum\\\\\\\`, \\\\\\\`all\\\\\\\_gather\\\\\\\`) for data/grad sync; use \\\\\\\`send/recv\\\\\\\` for pipeline or sparse paths. \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/distributed.rst "Download source file") - .pdf # Distributed Communication # Distributed Communication MLX provides a distributed communication package using MPI. The MPI library is loaded at runtime; if MPI is available then distributed communication is also made available. | | | |----|----| | \\\[\\\`\\\`Group\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.distributed.Group.html#mlx.core.distributed.Group "mlx.core.distributed.Group") | An \\\[\\\`\\\`mlx.core.distributed.Group\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.distributed.Group.html#mlx.core.distributed.Group "mlx.core.distributed.Group") represents a group of independent mlx processes that can communicate. | | \\\[\\\`\\\`is\\\_available\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.distributed.is\\\_available.html#mlx.core.distributed.is\\\_available "mlx.core.distributed.is\\\_available")() | Check if a communication backend is available. | | \\\[\\\`\\\`init\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.distributed.init.html#mlx.core.distributed.init "mlx.core.distributed.init")(\\\\\\\\\\\\\\\[strict, backend\\\\\\\\\\\\\\\]) | Initialize the communication backend and create the global communication group. | | \\\[\\\`\\\`all\\\_sum\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.distributed.all\\\_sum.html#mlx.core.distributed.all\\\_sum "mlx.core.distributed.all\\\_sum")(x, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, group, stream\\\\\\\\\\\\\\\]) | All reduce sum. | | \\\[\\\`\\\`all\\\_gather\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.distributed.all\\\_gather.html#mlx.core.distributed.all\\\_gather "mlx.core.distributed.all\\\_gather")(x, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, group, stream\\\\\\\\\\\\\\\]) | Gather arrays from all processes. | | \\\[\\\`\\\`send\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.distributed.send.html#mlx.core.distributed.send "mlx.core.distributed.send")(x, dst, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, group, stream\\\\\\\\\\\\\\\]) | Send an array from the current process to the process that has rank \\\`\\\\\\\`dst\\\\\\\`\\\` in the group. | | \\\[\\\`\\\`recv\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.distributed.recv.html#mlx.core.distributed.recv "mlx.core.distributed.recv")(shape, dtype, src, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, group, stream\\\\\\\\\\\\\\\]) | Recv an array with shape \\\`\\\\\\\`shape\\\\\\\`\\\` and dtype \\\`\\\\\\\`dtype\\\\\\\`\\\` from process with rank \\\`\\\\\\\`src\\\\\\\`\\\`. | | \\\[\\\`\\\`recv\\\_like\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.distributed.recv\\\_like.html#mlx.core.distributed.recv\\\_like "mlx.core.distributed.recv\\\_like")(x, src, \\\\\\\\\\\\\\\*\\\\\\\\\\\\\\\[, group, stream\\\\\\\\\\\\\\\]) | Recv an array with shape and type like \\\`\\\\\\\`x\\\\\\\`\\\` from process with rank \\\`\\\\\\\`src\\\\\\\`\\\`. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.optimizers.clip\\\_grad\\\_norm.html "previous page") previous mlx.optimizers.clip\\\\\\\_grad\\\\\\\_norm \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.distributed.Group.html "next page") next mlx.core.distributed.Group By MLX Contributors © Copyright 2023, MLX Contributors.
