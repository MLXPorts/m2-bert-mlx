Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - For classification: \\\\\\\`cross\\\\\\\_entropy(logits, targets)\\\\\\\` expects integer class indices by default; ensure \\\\\\\`axis\\\\\\\` matches your class dimension. - Many losses accept \\\\\\\`reduction\\\\\\\`: use \\\\\\\`'none'\\\\\\\` to inspect per‑example terms during debugging, then switch to \\\\\\\`'mean'\\\\\\\`/\\\\\\\`'sum'\\\\\\\` for training. - For NLL/CE, make sure logits are not passed through softmax unless the API specifically wants probabilities. - Precision tip: keep accumulations in \\\\\\\`float32\\\\\\\` for performance, but for very sensitive metrics you can compute reductions on CPU in \\\\\\\`float64\\\\\\\` and cast back for the rest of the pipeline. ### Examples \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx import mlx.nn as nn # Cross entropy with integer class targets logits = mx.random.normal((8, 10)) targets = mx.random.randint(0, 10, (8,)) ce = nn.losses.cross\\\\\\\_entropy(logits, targets) # Binary cross entropy (probabilities) probs = mx.sigmoid(mx.random.normal((8, 1))) labels = mx.array(\\\\\\\[\\\\\\\[0.0\\\\\\\], \\\\\\\[1.0\\\\\\\], \\\\\\\[0.0\\\\\\\], \\\\\\\[1.0\\\\\\\], \\\\\\\[1.0\\\\\\\], \\\\\\\[0.0\\\\\\\], \\\\\\\[0.0\\\\\\\], \\\\\\\[1.0\\\\\\\]\\\\\\\]) bce = nn.losses.binary\\\\\\\_cross\\\\\\\_entropy(probs, labels) # Inspect per-example terms per = nn.losses.mse\\\\\\\_loss(mx.zeros((4,)), mx.array(\\\\\\\[0.0, 1.0, 2.0, 3.0\\\\\\\]), reduction="none") \\\\\\\`\\\\\\\`\\\\\\\` \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/nn/losses.rst "Download source file") - .pdf # Loss Functions # Loss Functions | | | |----|----| | \\\[\\\`\\\`binary\\\_cross\\\_entropy\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.binary\\\_cross\\\_entropy.html#mlx.nn.losses.binary\\\_cross\\\_entropy "mlx.nn.losses.binary\\\_cross\\\_entropy")(inputs, targets\\\\\\\\\\\\\\\[, ...\\\\\\\\\\\\\\\]) | Computes the binary cross entropy loss. | | \\\[\\\`\\\`cosine\\\_similarity\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.cosine\\\_similarity\\\_loss.html#mlx.nn.losses.cosine\\\_similarity\\\_loss "mlx.nn.losses.cosine\\\_similarity\\\_loss")(x1, x2\\\\\\\\\\\\\\\[, axis, eps, ...\\\\\\\\\\\\\\\]) | Computes the cosine similarity between the two inputs. | | \\\[\\\`\\\`cross\\\_entropy\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.cross\\\_entropy.html#mlx.nn.losses.cross\\\_entropy "mlx.nn.losses.cross\\\_entropy")(logits, targets\\\\\\\\\\\\\\\[, weights, ...\\\\\\\\\\\\\\\]) | Computes the cross entropy loss. | | \\\[\\\`\\\`gaussian\\\_nll\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.gaussian\\\_nll\\\_loss.html#mlx.nn.losses.gaussian\\\_nll\\\_loss "mlx.nn.losses.gaussian\\\_nll\\\_loss")(inputs, targets, vars\\\\\\\\\\\\\\\[, ...\\\\\\\\\\\\\\\]) | Computes the negative log likelihood loss for a Gaussian distribution. | | \\\[\\\`\\\`hinge\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.hinge\\\_loss.html#mlx.nn.losses.hinge\\\_loss "mlx.nn.losses.hinge\\\_loss")(inputs, targets\\\\\\\\\\\\\\\[, reduction\\\\\\\\\\\\\\\]) | Computes the hinge loss between inputs and targets. | | \\\[\\\`\\\`huber\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.huber\\\_loss.html#mlx.nn.losses.huber\\\_loss "mlx.nn.losses.huber\\\_loss")(inputs, targets\\\\\\\\\\\\\\\[, delta, reduction\\\\\\\\\\\\\\\]) | Computes the Huber loss between inputs and targets. | | \\\[\\\`\\\`kl\\\_div\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.kl\\\_div\\\_loss.html#mlx.nn.losses.kl\\\_div\\\_loss "mlx.nn.losses.kl\\\_div\\\_loss")(inputs, targets\\\\\\\\\\\\\\\[, axis, reduction\\\\\\\\\\\\\\\]) | Computes the Kullback-Leibler divergence loss. | | \\\[\\\`\\\`l1\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.l1\\\_loss.html#mlx.nn.losses.l1\\\_loss "mlx.nn.losses.l1\\\_loss")(predictions, targets\\\\\\\\\\\\\\\[, reduction\\\\\\\\\\\\\\\]) | Computes the L1 loss. | | \\\[\\\`\\\`log\\\_cosh\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.log\\\_cosh\\\_loss.html#mlx.nn.losses.log\\\_cosh\\\_loss "mlx.nn.losses.log\\\_cosh\\\_loss")(inputs, targets\\\\\\\\\\\\\\\[, reduction\\\\\\\\\\\\\\\]) | Computes the log cosh loss between inputs and targets. | | \\\[\\\`\\\`margin\\\_ranking\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.margin\\\_ranking\\\_loss.html#mlx.nn.losses.margin\\\_ranking\\\_loss "mlx.nn.losses.margin\\\_ranking\\\_loss")(inputs1, inputs2, targets) | Calculate the margin ranking loss that loss given inputs \\\\\\\\\\\\\\\\x\\\\\\\_1\\\\\\\\\\\\\\\\, \\\\\\\\\\\\\\\\x\\\\\\\_2\\\\\\\\\\\\\\\\ and a label \\\\\\\\\\\\\\\\y\\\\\\\\\\\\\\\\ (containing 1 or -1). | | \\\[\\\`\\\`mse\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.mse\\\_loss.html#mlx.nn.losses.mse\\\_loss "mlx.nn.losses.mse\\\_loss")(predictions, targets\\\\\\\\\\\\\\\[, reduction\\\\\\\\\\\\\\\]) | Computes the mean squared error loss. | | \\\[\\\`\\\`nll\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.nll\\\_loss.html#mlx.nn.losses.nll\\\_loss "mlx.nn.losses.nll\\\_loss")(inputs, targets\\\\\\\\\\\\\\\[, axis, reduction\\\\\\\\\\\\\\\]) | Computes the negative log likelihood loss. | | \\\[\\\`\\\`smooth\\\_l1\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.smooth\\\_l1\\\_loss.html#mlx.nn.losses.smooth\\\_l1\\\_loss "mlx.nn.losses.smooth\\\_l1\\\_loss")(predictions, targets\\\\\\\\\\\\\\\[, beta, ...\\\\\\\\\\\\\\\]) | Computes the smooth L1 loss. | | \\\[\\\`\\\`triplet\\\_loss\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.triplet\\\_loss.html#mlx.nn.losses.triplet\\\_loss "mlx.nn.losses.triplet\\\_loss")(anchors, positives, negatives) | Computes the triplet loss for a set of anchor, positive, and negative samples. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.tanh.html "previous page") previous mlx.nn.tanh \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.losses.binary\\\_cross\\\_entropy.html "next page") next mlx.nn.losses.binary\\\\\\\_cross\\\\\\\_entropy By MLX Contributors © Copyright 2023, MLX Contributors.
