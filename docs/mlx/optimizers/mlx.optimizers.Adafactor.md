Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers/\\\_autosummary/mlx.optimizers.Adafactor.rst "Download source file") - .pdf # mlx.optimizers.Adafactor ## Contents \\\\- \\\[\\\`\\\`Adafactor\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.Adafactor) # mlx.optimizers.Adafactor \\\\\\\*\\\`class\\\` \\\\\\\*\\\`Adafactor\\\`(\\\\\\\*\\\`learning\\\\\\\_rate\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\\\\\|\\\` \\\[\\\`Callable\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\[\\\`\\\`\\\\\\\\\\\\\\\[\\\`\\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\`\\\\\\\\\\\\\\\]\\\`\\\`,\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\`\\\\\\\\\\\\\\\]\\\` \\\`\\\\\\\\|\\\` \\\[\\\`None\\\`\\\](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") \\\`\\\\=\\\` \\\`None\\\`\\\\\\\*, \\\\\\\*\\\`eps\\\`\\\`:\\\` \\\[\\\`Tuple\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\[\\\`\\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\`,\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\]\\\` \\\`\\\\=\\\` \\\`(1e-30,\\\` \\\`0.001)\\\`\\\\\\\*, \\\\\\\*\\\`clip\\\\\\\_threshold\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\=\\\` \\\`1.0\\\`\\\\\\\*, \\\\\\\*\\\`decay\\\\\\\_rate\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\=\\\` \\\`\\\\-0.8\\\`\\\\\\\*, \\\\\\\*\\\`beta\\\\\\\_1\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\\\\\|\\\` \\\[\\\`None\\\`\\\](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") \\\`\\\\=\\\` \\\`None\\\`\\\\\\\*, \\\\\\\*\\\`weight\\\\\\\_decay\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\=\\\` \\\`0.0\\\`\\\\\\\*, \\\\\\\*\\\`scale\\\\\\\_parameter\\\`\\\`:\\\` \\\[\\\`bool\\\`\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") \\\`\\\\=\\\` \\\`True\\\`\\\\\\\*, \\\\\\\*\\\`relative\\\\\\\_step\\\`\\\`:\\\` \\\[\\\`bool\\\`\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") \\\`\\\\=\\\` \\\`True\\\`\\\\\\\*, \\\\\\\*\\\`warmup\\\\\\\_init\\\`\\\`:\\\` \\\[\\\`bool\\\`\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") \\\`\\\\=\\\` \\\`False\\\`\\\\\\\*) The Adafactor optimizer. Our Adafactor implementation follows the original paper: \\\[Adafactor: Adaptive Learning Rates with Sublinear Memory Cost\\\](https://arxiv.org/abs/1804.04235) Parameters: - \\\\\\\*\\\\\\\*learning\\\\\\\_rate\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\\\\\*or\\\\\\\* \\\\\\\*callable,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The learning rate. Default: \\\`\\\\\\\`None\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*eps\\\\\\\*\\\\\\\* (\\\[\\\*tuple\\\*\\\](https://docs.python.org/3/library/stdtypes.html#tuple "(in Python v3.13)")\\\\\\\*(\\\\\\\*\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*),\\\\\\\* \\\\\\\*optional\\\\\\\*) – The first term \\\\\\\\\\\\\\\\\\\\\\\\epsilon\\\\\\\_1\\\\\\\\\\\\\\\\ added to the square of the gradients to improve numerical stability and the second term \\\\\\\\\\\\\\\\\\\\\\\\epsilon\\\\\\\_2\\\\\\\\\\\\\\\\ is used for parameter scaling if \\\`\\\\\\\`parameter\\\\\\\_scale\\\\\\\`\\\` is set to \\\`\\\\\\\`True\\\\\\\`\\\`. Default: \\\`\\\\\\\`(1e-30,\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`1e-3)\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*clip\\\\\\\_threshold\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – Clips the unscaled update at \\\`\\\\\\\`clip\\\\\\\_threshold\\\\\\\`\\\`. Default: \\\`\\\\\\\`1.0\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*decay\\\\\\\_rate\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – Coefficient for the running average of the squared gradient. Default: \\\`\\\\\\\`-0.8\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*beta\\\\\\\_1\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – If set to a value bigger than zero then first moment will be used. Default: \\\`\\\\\\\`None\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*weight\\\\\\\_decay\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The weight decay \\\\\\\\\\\\\\\\\\\\\\\\lambda\\\\\\\\\\\\\\\\. Default: \\\`\\\\\\\`0.0\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*scale\\\\\\\_parameter\\\\\\\*\\\\\\\* (\\\[\\\*bool\\\*\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – If set to \\\`\\\\\\\`True\\\\\\\`\\\` the learning rate will be scaled by \\\\\\\\\\\\\\\\\\\\\\\\max(\\\\\\\\epsilon\\\\\\\_1, \\\\\\\\text{RMS}(w\\\\\\\\\\\\\\\_{t-1}))\\\\\\\\\\\\\\\\. Default: \\\`\\\\\\\`True\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*relative\\\\\\\_step\\\\\\\*\\\\\\\* (\\\[\\\*bool\\\*\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – If set to \\\`\\\\\\\`True\\\\\\\`\\\` the \\\`\\\\\\\`learning\\\\\\\_rate\\\\\\\`\\\` will be ignored and relative step size will be computed. Default: \\\`\\\\\\\`True\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*warmup\\\\\\\_init\\\\\\\*\\\\\\\* (\\\[\\\*bool\\\*\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – If set to \\\`\\\\\\\`True\\\\\\\`\\\` then the relative step size will be calculated by the current step. Default: \\\`\\\\\\\`False\\\\\\\`\\\`. Methods | | | |----|----| | \\\`\\\\\\\`\\\\\\\_\\\\\\\_init\\\\\\\_\\\\\\\_\\\\\\\`\\\`(\\\\\\\\\\\\\\\[learning\\\\\\\_rate, eps, ...\\\\\\\\\\\\\\\]) | | | \\\`\\\\\\\`apply\\\\\\\_single\\\\\\\`\\\`(gradient, parameter, state) | Performs the Adafactor parameter and state update. | | \\\`\\\\\\\`init\\\\\\\_single\\\\\\\`\\\`(parameter, state) | Initialize optimizer state | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adagrad.html "previous page") previous mlx.optimizers.Adagrad \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.AdaDelta.html "next page") next mlx.optimizers.AdaDelta Contents \\\\- \\\[\\\`\\\`Adafactor\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.Adafactor) By MLX Contributors © Copyright 2023, MLX Contributors.
