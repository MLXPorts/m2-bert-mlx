Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers/\\\_autosummary/mlx.optimizers.AdamW.rst "Download source file") - .pdf # mlx.optimizers.AdamW ## Contents \\\\- \\\[\\\`\\\`AdamW\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.AdamW) # mlx.optimizers.AdamW \\\\\\\*\\\`class\\\` \\\\\\\*\\\`AdamW\\\`(\\\\\\\*\\\`learning\\\\\\\_rate\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\\\\\|\\\` \\\[\\\`Callable\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\[\\\`\\\`\\\\\\\\\\\\\\\[\\\`\\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\`\\\\\\\\\\\\\\\]\\\`\\\`,\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\`\\\\\\\\\\\\\\\]\\\`\\\\\\\*, \\\\\\\*\\\`betas\\\`\\\`:\\\` \\\[\\\`List\\\`\\\](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\[\\\`\\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\]\\\` \\\`\\\\=\\\` \\\`\\\\\\\\\\\\\\\[0.9,\\\` \\\`0.999\\\\\\\\\\\\\\\]\\\`\\\\\\\*, \\\\\\\*\\\`eps\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\=\\\` \\\`1e-08\\\`\\\\\\\*, \\\\\\\*\\\`weight\\\\\\\_decay\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\=\\\` \\\`0.01\\\`\\\\\\\*, \\\\\\\*\\\`bias\\\\\\\_correction\\\`\\\`:\\\` \\\[\\\`bool\\\`\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") \\\`\\\\=\\\` \\\`False\\\`\\\\\\\*) The AdamW optimizer \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\]. We update the weights with a weight\\\\\\\_decay (\\\\\\\\\\\\\\\\\\\\\\\\lambda\\\\\\\\\\\\\\\\) value: \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\]: Loshchilov, I. and Hutter, F., 2019. Decoupled weight decay regularization. ICLR 2019. \\\\\\\\\\\\\\\\\\\\\\\\begin{split}m\\\\\\\\\\\\\\\_{t+1} &= \\\\\\\\beta\\\\\\\_1 m\\\\\\\_t + (1 - \\\\\\\\beta\\\\\\\_1) g\\\\\\\_t \\\\\\\\\\\\\\\\ v\\\\\\\\\\\\\\\_{t+1} &= \\\\\\\\beta\\\\\\\_2 v\\\\\\\_t + (1 - \\\\\\\\beta\\\\\\\_2) g\\\\\\\_t^2 \\\\\\\\\\\\\\\\ w\\\\\\\\\\\\\\\_{t+1} &= w\\\\\\\_t - \\\\\\\\alpha (\\\\\\\\frac{m\\\\\\\\\\\\\\\_{t+1}}{\\\\\\\\sqrt{v\\\\\\\\\\\\\\\_{t+1} + \\\\\\\\epsilon}} + \\\\\\\\lambda w\\\\\\\_t)\\\\\\\\end{split}\\\\\\\\\\\\\\\\ Parameters: - \\\\\\\*\\\\\\\*learning\\\\\\\_rate\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\\\\\*or\\\\\\\* \\\\\\\*callable\\\\\\\*) – The learning rate \\\\\\\\\\\\\\\\\\\\\\\\alpha\\\\\\\\\\\\\\\\. - \\\\\\\*\\\\\\\*betas\\\\\\\*\\\\\\\* (\\\\\\\*Tuple\\\\\\\\\\\\\\\[\\\\\\\*\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*\\\\\\\\\\\\\\\],\\\\\\\* \\\\\\\*optional\\\\\\\*) – The coefficients \\\\\\\\\\\\\\\\(\\\\\\\\beta\\\\\\\_1, \\\\\\\\beta\\\\\\\_2)\\\\\\\\\\\\\\\\ used for computing running averages of the gradient and its square. Default: \\\`\\\\\\\`(0.9,\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`0.999)\\\\\\\`\\\` - \\\\\\\*\\\\\\\*eps\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The term \\\\\\\\\\\\\\\\\\\\\\\\epsilon\\\\\\\\\\\\\\\\ added to the denominator to improve numerical stability. Default: \\\`\\\\\\\`1e-8\\\\\\\`\\\` - \\\\\\\*\\\\\\\*weight\\\\\\\_decay\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The weight decay \\\\\\\\\\\\\\\\\\\\\\\\lambda\\\\\\\\\\\\\\\\. Default: \\\`\\\\\\\`0\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*bias\\\\\\\_correction\\\\\\\*\\\\\\\* (\\\[\\\*bool\\\*\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – If set to \\\`\\\\\\\`True\\\\\\\`\\\`, bias correction is applied. Default: \\\`\\\\\\\`False\\\\\\\`\\\` Methods | | | |----|----| | \\\`\\\\\\\`\\\\\\\_\\\\\\\_init\\\\\\\_\\\\\\\_\\\\\\\`\\\`(learning\\\\\\\_rate\\\\\\\\\\\\\\\[, betas, eps, ...\\\\\\\\\\\\\\\]) | | | \\\`\\\\\\\`apply\\\\\\\_single\\\\\\\`\\\`(gradient, parameter, state) | Performs the AdamW parameter update by modifying the parameters passed into Adam. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adam.html "previous page") previous mlx.optimizers.Adam \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adamax.html "next page") next mlx.optimizers.Adamax Contents \\\\- \\\[\\\`\\\`AdamW\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.AdamW) By MLX Contributors © Copyright 2023, MLX Contributors.
