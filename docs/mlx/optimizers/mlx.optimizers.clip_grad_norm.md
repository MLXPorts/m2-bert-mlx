Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/\\\_autosummary/mlx.optimizers.clip\\\_grad\\\_norm.rst "Download source file") - .pdf # mlx.optimizers.clip\\\\\\\_grad\\\\\\\_norm ## Contents \\\\- \\\[\\\`\\\`clip\\\_grad\\\_norm()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.clip\\\_grad\\\_norm) # mlx.optimizers.clip\\\\\\\_grad\\\\\\\_norm \\\`clip\\\\\\\_grad\\\\\\\_norm\\\`(\\\\\\\*\\\`grads\\\`\\\\\\\*, \\\\\\\*\\\`max\\\\\\\_norm\\\`\\\\\\\*) Clips the global norm of the gradients. This function ensures that the global norm of the gradients does not exceed \\\`\\\\\\\`max\\\\\\\_norm\\\\\\\`\\\`. It scales down the gradients proportionally if their norm is greater than \\\`\\\\\\\`max\\\\\\\_norm\\\\\\\`\\\`. Example \\\\>>> grads = {"w1": mx.array(\\\\\\\[2, 3\\\\\\\]), "w2": mx.array(\\\\\\\[1\\\\\\\])} >>> clipped\\\\\\\_grads, total\\\\\\\_norm = clip\\\\\\\_grad\\\\\\\_norm(grads, max\\\\\\\_norm=2.0) >>> print(clipped\\\\\\\_grads) {"w1": mx.array(\\\\\\\[...\\\\\\\]), "w2": mx.array(\\\\\\\[...\\\\\\\])} Parameters: - \\\\\\\*\\\\\\\*grads\\\\\\\*\\\\\\\* (\\\[\\\*dict\\\*\\\](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")) – A dictionary containing the gradient arrays. - \\\\\\\*\\\\\\\*max\\\\\\\_norm\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – The maximum allowed global norm of the gradients. Returns: The possibly rescaled gradients and the original gradient norm. Return type: (\\\[\\\*dict\\\*\\\](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)"), \\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.step\\\_decay.html "previous page") previous mlx.optimizers.step\\\\\\\_decay \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/distributed.html "next page") next Distributed Communication Contents \\\\- \\\[\\\`\\\`clip\\\_grad\\\_norm()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.clip\\\_grad\\\_norm) By MLX Contributors © Copyright 2023, MLX Contributors.
