Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers/\\\_autosummary/mlx.optimizers.SGD.rst "Download source file") - .pdf # mlx.optimizers.SGD ## Contents \\\\- \\\[\\\`\\\`SGD\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.SGD) # mlx.optimizers.SGD \\\\\\\*\\\`class\\\` \\\\\\\*\\\`SGD\\\`(\\\\\\\*\\\`learning\\\\\\\_rate\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\\\\\|\\\` \\\[\\\`Callable\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\[\\\`\\\`\\\\\\\\\\\\\\\[\\\`\\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\`\\\\\\\\\\\\\\\]\\\`\\\`,\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\`\\\\\\\\\\\\\\\]\\\`\\\\\\\*, \\\\\\\*\\\`momentum\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\=\\\` \\\`0.0\\\`\\\\\\\*, \\\\\\\*\\\`weight\\\\\\\_decay\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\=\\\` \\\`0.0\\\`\\\\\\\*, \\\\\\\*\\\`dampening\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\=\\\` \\\`0.0\\\`\\\\\\\*, \\\\\\\*\\\`nesterov\\\`\\\`:\\\` \\\[\\\`bool\\\`\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)") \\\`\\\\=\\\` \\\`False\\\`\\\\\\\*) The stochastic gradient descent optimizer. Updates a parameter \\\\\\\\\\\\\\\\w\\\\\\\\\\\\\\\\ with a gradient \\\\\\\\\\\\\\\\g\\\\\\\\\\\\\\\\ as follows \\\\\\\\\\\\\\\\\\\\\\\\begin{split}v\\\\\\\\\\\\\\\_{t+1} &= \\\\\\\\mu v\\\\\\\_t + (1 - \\\\\\\\tau) g\\\\\\\_t \\\\\\\\\\\\\\\\ w\\\\\\\\\\\\\\\_{t+1} &= w\\\\\\\_t - \\\\\\\\lambda v\\\\\\\\\\\\\\\_{t+1}\\\\\\\\end{split}\\\\\\\\\\\\\\\\ Parameters: - \\\\\\\*\\\\\\\*learning\\\\\\\_rate\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\\\\\*or\\\\\\\* \\\\\\\*callable\\\\\\\*) – The learning rate \\\\\\\\\\\\\\\\\\\\\\\\lambda\\\\\\\\\\\\\\\\. - \\\\\\\*\\\\\\\*momentum\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The momentum strength \\\\\\\\\\\\\\\\\\\\\\\\mu\\\\\\\\\\\\\\\\. Default: \\\`\\\\\\\`0\\\\\\\`\\\` - \\\\\\\*\\\\\\\*weight\\\\\\\_decay\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The weight decay (L2 penalty). Default: \\\`\\\\\\\`0\\\\\\\`\\\` - \\\\\\\*\\\\\\\*dampening\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – Dampening for momentum \\\\\\\\\\\\\\\\\\\\\\\\tau\\\\\\\\\\\\\\\\. Default: \\\`\\\\\\\`0\\\\\\\`\\\` - \\\\\\\*\\\\\\\*nesterov\\\\\\\*\\\\\\\* (\\\[\\\*bool\\\*\\\](https://docs.python.org/3/library/functions.html#bool "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – Enables Nesterov momentum. Default: \\\`\\\\\\\`False\\\\\\\`\\\` Methods | | | |----|----| | \\\`\\\\\\\`\\\\\\\_\\\\\\\_init\\\\\\\_\\\\\\\_\\\\\\\`\\\`(learning\\\\\\\_rate\\\\\\\\\\\\\\\[, momentum, ...\\\\\\\\\\\\\\\]) | | | \\\`\\\\\\\`apply\\\\\\\_single\\\\\\\`\\\`(gradient, parameter, state) | Performs the SGD parameter update and stores \\\\\\\\\\\\\\\\v\\\\\\\\\\\\\\\\ in the optimizer state. | | \\\`\\\\\\\`init\\\\\\\_single\\\\\\\`\\\`(parameter, state) | Initialize optimizer state | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/common\\\_optimizers.html "previous page") previous Common Optimizers \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.RMSprop.html "next page") next mlx.optimizers.RMSprop Contents \\\\- \\\[\\\`\\\`SGD\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.SGD) By MLX Contributors © Copyright 2023, MLX Contributors.
