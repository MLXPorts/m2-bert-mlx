Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers/\\\_autosummary/mlx.optimizers.AdaDelta.rst "Download source file") - .pdf # mlx.optimizers.AdaDelta ## Contents \\\\- \\\[\\\`\\\`AdaDelta\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.AdaDelta) # mlx.optimizers.AdaDelta \\\\\\\*\\\`class\\\` \\\\\\\*\\\`AdaDelta\\\`(\\\\\\\*\\\`learning\\\\\\\_rate\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\\\\\|\\\` \\\[\\\`Callable\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\[\\\`\\\`\\\\\\\\\\\\\\\[\\\`\\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\`\\\\\\\\\\\\\\\]\\\`\\\`,\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\`\\\\\\\\\\\\\\\]\\\`\\\\\\\*, \\\\\\\*\\\`rho\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\=\\\` \\\`0.9\\\`\\\\\\\*, \\\\\\\*\\\`eps\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\=\\\` \\\`1e-06\\\`\\\\\\\*) The AdaDelta optimizer with a learning rate \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\]. Our AdaDelta implementation follows the original paper. In detail, \\\\\\\\\\\\\\\[1\\\\\\\\\\\\\\\]: Zeiler, M.D., 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701. \\\\\\\\\\\\\\\\\\\\\\\\begin{split}v\\\\\\\\\\\\\\\_{t+1} &= \\\\\\\\rho v\\\\\\\_t + (1 - \\\\\\\\rho) g\\\\\\\_t^2 \\\\\\\\\\\\\\\\ \\\\\\\\Delta w\\\\\\\\\\\\\\\_{t+1} &= \\\\\\\\frac{\\\\\\\\sqrt{u\\\\\\\_t + \\\\\\\\epsilon}}{\\\\\\\\sqrt{v\\\\\\\\\\\\\\\_{t+1} + \\\\\\\\epsilon}} g\\\\\\\_t \\\\\\\\\\\\\\\\ u\\\\\\\\\\\\\\\_{t+1} &= \\\\\\\\rho u\\\\\\\_t + (1 - \\\\\\\\rho) \\\\\\\\Delta w\\\\\\\\\\\\\\\_{t+1}^2 \\\\\\\\\\\\\\\\ w\\\\\\\\\\\\\\\_{t+1} &= w\\\\\\\_t - \\\\\\\\lambda \\\\\\\\Delta w\\\\\\\\\\\\\\\_{t+1}\\\\\\\\end{split}\\\\\\\\\\\\\\\\ Parameters: - \\\\\\\*\\\\\\\*learning\\\\\\\_rate\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\\\\\*or\\\\\\\* \\\\\\\*callable\\\\\\\*) – The learning rate \\\\\\\\\\\\\\\\\\\\\\\\lambda\\\\\\\\\\\\\\\\. - \\\\\\\*\\\\\\\*rho\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The coefficient \\\\\\\\\\\\\\\\\\\\\\\\rho\\\\\\\\\\\\\\\\ used for computing a running average of squared gradients. Default: \\\`\\\\\\\`0.9\\\\\\\`\\\` - \\\\\\\*\\\\\\\*eps\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The term \\\\\\\\\\\\\\\\\\\\\\\\epsilon\\\\\\\\\\\\\\\\ added to the denominator to improve numerical stability. Default: 1e-8 Methods | | | |----|----| | \\\`\\\\\\\`\\\\\\\_\\\\\\\_init\\\\\\\_\\\\\\\_\\\\\\\`\\\`(learning\\\\\\\_rate\\\\\\\\\\\\\\\[, rho, eps\\\\\\\\\\\\\\\]) | | | \\\`\\\\\\\`apply\\\\\\\_single\\\\\\\`\\\`(gradient, parameter, state) | Performs the AdaDelta parameter update and stores \\\\\\\\\\\\\\\\v\\\\\\\\\\\\\\\\ and \\\\\\\\\\\\\\\\u\\\\\\\\\\\\\\\\ in the optimizer state. | | \\\`\\\\\\\`init\\\\\\\_single\\\\\\\`\\\`(parameter, state) | Initialize optimizer state | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adafactor.html "previous page") previous mlx.optimizers.Adafactor \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Adam.html "next page") next mlx.optimizers.Adam Contents \\\\- \\\[\\\`\\\`AdaDelta\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.AdaDelta) By MLX Contributors © Copyright 2023, MLX Contributors.
