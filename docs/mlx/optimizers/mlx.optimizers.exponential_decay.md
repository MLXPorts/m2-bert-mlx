Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers/\\\_autosummary/mlx.optimizers.exponential\\\_decay.rst "Download source file") - .pdf # mlx.optimizers.exponential\\\\\\\_decay ## Contents \\\\- \\\[\\\`\\\`exponential\\\_decay()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.exponential\\\_decay) # mlx.optimizers.exponential\\\\\\\_decay \\\`exponential\\\\\\\_decay\\\`(\\\\\\\*\\\`init\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*, \\\\\\\*\\\`decay\\\\\\\_rate\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*) → \\\[\\\`Callable\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)") Make an exponential decay scheduler. Parameters: - \\\\\\\*\\\\\\\*init\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – Initial value. - \\\\\\\*\\\\\\\*decay\\\\\\\_rate\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – Multiplicative factor to decay by. Example \\\\>>> lr\\\\\\\_schedule = optim.exponential\\\\\\\_decay(1e-1, 0.9) >>> optimizer = optim.SGD(learning\\\\\\\_rate=lr\\\\\\\_schedule) >>> optimizer.learning\\\\\\\_rate array(0.1, dtype=float32) >>> >>> for \\\\\\\_ in range(5): optimizer.update({}, {}) ... >>> optimizer.learning\\\\\\\_rate array(0.06561, dtype=float32) \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.cosine\\\_decay.html "previous page") previous mlx.optimizers.cosine\\\\\\\_decay \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.join\\\_schedules.html "next page") next mlx.optimizers.join\\\\\\\_schedules Contents \\\\- \\\[\\\`\\\`exponential\\\_decay()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.exponential\\\_decay) By MLX Contributors © Copyright 2023, MLX Contributors.
