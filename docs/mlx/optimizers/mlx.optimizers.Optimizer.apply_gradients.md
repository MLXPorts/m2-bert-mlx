Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.apply\\\_gradients.rst "Download source file") - .pdf # mlx.optimizers.Optimizer.apply\\\\\\\_gradients ## Contents \\\\- \\\[\\\`\\\`Optimizer.apply\\\_gradients()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.Optimizer.apply\\\_gradients) # mlx.optimizers.Optimizer.apply\\\\\\\_gradients \\\`Optimizer.\\\`\\\`apply\\\\\\\_gradients\\\`(\\\\\\\*\\\`gradients\\\`\\\`:\\\` \\\[\\\`dict\\\`\\\](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")\\\\\\\*, \\\\\\\*\\\`parameters\\\`\\\`:\\\` \\\[\\\`dict\\\`\\\](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")\\\\\\\*) Apply the gradients to the parameters and return the updated parameters. Can be used to update a model via \\\`\\\\\\\`model.update(opt.apply\\\\\\\_gradients(grads,\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`model))\\\\\\\`\\\` which is precisely how \\\[\\\`\\\`Optimizer.update()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.update.html#mlx.optimizers.Optimizer.update "mlx.optimizers.Optimizer.update") is implemented. Parameters: - \\\\\\\*\\\\\\\*gradients\\\\\\\*\\\\\\\* (\\\[\\\*dict\\\*\\\](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")) – A Python tree of gradients. - \\\\\\\*\\\\\\\*parameters\\\\\\\*\\\\\\\* (\\\[\\\*dict\\\*\\\](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")) – A Python tree of parameters. It can be a superset of the gradients. In that case the returned python tree will be of the same structure as the gradients. \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.state.html "previous page") previous mlx.optimizers.Optimizer.state \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.init.html "next page") next mlx.optimizers.Optimizer.init Contents \\\\- \\\[\\\`\\\`Optimizer.apply\\\_gradients()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.Optimizer.apply\\\_gradients) By MLX Contributors © Copyright 2023, MLX Contributors.
