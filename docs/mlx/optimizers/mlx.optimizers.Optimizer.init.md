Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.init.rst "Download source file") - .pdf # mlx.optimizers.Optimizer.init ## Contents \\\\- \\\[\\\`\\\`Optimizer.init()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.Optimizer.init) # mlx.optimizers.Optimizer.init \\\`Optimizer.\\\`\\\`init\\\`(\\\\\\\*\\\`parameters\\\`\\\`:\\\` \\\[\\\`dict\\\`\\\](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")\\\\\\\*) Initialize the optimizer’s state This function can be used to initialize optimizers which have state (like momentum in \\\[\\\`\\\`SGD\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.SGD.html#mlx.optimizers.SGD "mlx.optimizers.SGD")). Using this method is optional as the optimizer will initialize itself if the state is not yet set. However, there are some cases where explicit initialization is useful in order to have access to the \\\[\\\`\\\`Optimizer.state\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.state.html#mlx.optimizers.Optimizer.state "mlx.optimizers.Optimizer.state") before the first call to \\\[\\\`\\\`Optimizer.update()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.update.html#mlx.optimizers.Optimizer.update "mlx.optimizers.Optimizer.update"). Parameters: \\\\\\\*\\\\\\\*model\\\\\\\*\\\\\\\* (\\\[\\\*dict\\\*\\\](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.13)")) – A Python tree of parameters. Example \\\\>>> optimizer = optim.SGD(learning\\\\\\\_rate=1e-1, momentum=0.9) >>> model = nn.Linear(2, 2) >>> optimizer.init(model.trainable\\\\\\\_parameters()) >>> optimizer.state.keys() dict\\\\\\\_keys(\\\\\\\['step', 'learning\\\\\\\_rate', 'weight', 'bias'\\\\\\\]) \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.apply\\\_gradients.html "previous page") previous mlx.optimizers.Optimizer.apply\\\\\\\_gradients \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.Optimizer.update.html "next page") next mlx.optimizers.Optimizer.update Contents \\\\- \\\[\\\`\\\`Optimizer.init()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.Optimizer.init) By MLX Contributors © Copyright 2023, MLX Contributors.
