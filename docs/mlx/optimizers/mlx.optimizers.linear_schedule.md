Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers/\\\_autosummary/mlx.optimizers.linear\\\_schedule.rst "Download source file") - .pdf # mlx.optimizers.linear\\\\\\\_schedule ## Contents \\\\- \\\[\\\`\\\`linear\\\_schedule()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.linear\\\_schedule) # mlx.optimizers.linear\\\\\\\_schedule \\\`linear\\\\\\\_schedule\\\`(\\\\\\\*\\\`init\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*, \\\\\\\*\\\`end\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*, \\\\\\\*\\\`steps\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*) → \\\[\\\`Callable\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)") Make a linear scheduler. Parameters: - \\\\\\\*\\\\\\\*init\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – Initial value. - \\\\\\\*\\\\\\\*end\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – Final value. - \\\\\\\*\\\\\\\*steps\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – Number of steps to apply the schedule over. The value is \\\`\\\\\\\`end\\\\\\\`\\\` for any steps beyond \\\`\\\\\\\`steps\\\\\\\`\\\`. Example \\\\>>> lr\\\\\\\_schedule = optim.linear\\\\\\\_schedule(0, 1e-1, 100) >>> optimizer = optim.Adam(learning\\\\\\\_rate=lr\\\\\\\_schedule) >>> optimizer.learning\\\\\\\_rate array(0.0, dtype=float32) >>> for \\\\\\\_ in range(101): optimizer.update({}, {}) ... >>> optimizer.learning\\\\\\\_rate array(0.1, dtype=float32) \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.join\\\_schedules.html "previous page") previous mlx.optimizers.join\\\\\\\_schedules \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.step\\\_decay.html "next page") next mlx.optimizers.step\\\\\\\_decay Contents \\\\- \\\[\\\`\\\`linear\\\_schedule()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.linear\\\_schedule) By MLX Contributors © Copyright 2023, MLX Contributors.
