Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers/\\\_autosummary/mlx.optimizers.step\\\_decay.rst "Download source file") - .pdf # mlx.optimizers.step\\\\\\\_decay ## Contents \\\\- \\\[\\\`\\\`step\\\_decay()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.step\\\_decay) # mlx.optimizers.step\\\\\\\_decay \\\`step\\\\\\\_decay\\\`(\\\\\\\*\\\`init\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*, \\\\\\\*\\\`decay\\\\\\\_rate\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*, \\\\\\\*\\\`step\\\\\\\_size\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*) → \\\[\\\`Callable\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)") Make a step decay scheduler. Parameters: - \\\\\\\*\\\\\\\*init\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – Initial value. - \\\\\\\*\\\\\\\*decay\\\\\\\_rate\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – Multiplicative factor to decay by. - \\\\\\\*\\\\\\\*step\\\\\\\_size\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – Decay every \\\`\\\\\\\`step\\\\\\\_size\\\\\\\`\\\` steps. Example \\\\>>> lr\\\\\\\_schedule = optim.step\\\\\\\_decay(1e-1, 0.9, 10) >>> optimizer = optim.SGD(learning\\\\\\\_rate=lr\\\\\\\_schedule) >>> optimizer.learning\\\\\\\_rate array(0.1, dtype=float32) >>> >>> for \\\\\\\_ in range(21): optimizer.update({}, {}) ... >>> optimizer.learning\\\\\\\_rate array(0.081, dtype=float32) \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.linear\\\_schedule.html "previous page") previous mlx.optimizers.linear\\\\\\\_schedule \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.optimizers.clip\\\_grad\\\_norm.html "next page") next mlx.optimizers.clip\\\\\\\_grad\\\\\\\_norm Contents \\\\- \\\[\\\`\\\`step\\\_decay()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.step\\\_decay) By MLX Contributors © Copyright 2023, MLX Contributors.
