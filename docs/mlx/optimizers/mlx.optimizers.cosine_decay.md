Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/optimizers/\\\_autosummary/mlx.optimizers.cosine\\\_decay.rst "Download source file") - .pdf # mlx.optimizers.cosine\\\\\\\_decay ## Contents \\\\- \\\[\\\`\\\`cosine\\\_decay()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.cosine\\\_decay) # mlx.optimizers.cosine\\\\\\\_decay \\\`cosine\\\\\\\_decay\\\`(\\\\\\\*\\\`init\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*, \\\\\\\*\\\`decay\\\\\\\_steps\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*, \\\\\\\*\\\`end\\\`\\\`:\\\` \\\[\\\`float\\\`\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)") \\\`\\\\=\\\` \\\`0.0\\\`\\\\\\\*) → \\\[\\\`Callable\\\`\\\](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.13)") Make a cosine decay scheduler. Parameters: - \\\\\\\*\\\\\\\*init\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")) – Initial value. - \\\\\\\*\\\\\\\*decay\\\\\\\_steps\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – Number of steps to decay over. The decayed value is constant for steps beyond \\\`\\\\\\\`decay\\\\\\\_steps\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*end\\\\\\\*\\\\\\\* (\\\[\\\*float\\\*\\\](https://docs.python.org/3/library/functions.html#float "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – Final value to decay to. Default: \\\`\\\\\\\`0\\\\\\\`\\\`. Example \\\\>>> lr\\\\\\\_schedule = optim.cosine\\\\\\\_decay(1e-1, 1000) >>> optimizer = optim.SGD(learning\\\\\\\_rate=lr\\\\\\\_schedule) >>> optimizer.learning\\\\\\\_rate array(0.1, dtype=float32) >>> >>> for \\\\\\\_ in range(5): optimizer.update({}, {}) ... >>> optimizer.learning\\\\\\\_rate array(0.0999961, dtype=float32) \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/schedulers.html "previous page") previous Schedulers \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/\\\_autosummary/mlx.optimizers.exponential\\\_decay.html "next page") next mlx.optimizers.exponential\\\\\\\_decay Contents \\\\- \\\[\\\`\\\`cosine\\\_decay()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.optimizers.cosine\\\_decay) By MLX Contributors © Copyright 2023, MLX Contributors.
