Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Variant (value\\\\\\\_and\\\\\\\_grad) \\\\\\\`\\\\\\\`\\\\\\\`python opt\\\\\\\_w = mx.random.normal((num\\\\\\\_features,)) def loss\\\\\\\_w(w): return 0.5 \\\\\\\* mx.mean((X @ w - y) \\\\\\\*\\\\\\\* 2) for step in range(2000): loss, grad = mx.value\\\\\\\_and\\\\\\\_grad(loss\\\\\\\_w)(opt\\\\\\\_w) opt\\\\\\\_w = opt\\\\\\\_w - lr \\\\\\\* grad if step % 200 == 0: print(step, loss.item()) \\\\\\\`\\\\\\\`\\\\\\\` Tip: monitor a validation split or a held‑out metric to avoid overfitting when you extend this to NN models. \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/examples/linear\\\_regression.rst "Download source file") - .pdf # Linear Regression # Linear Regression Let’s implement a basic linear regression model as a starting point to learn MLX. First import the core package and setup some problem metadata: \\\`\\\`\\\`python import mlx.core as mx num\\\_features = 100 num\\\_examples = 1\\\_000 num\\\_iters = 10\\\_000 # iterations of SGD lr = 0.01 # learning rate for SGD \\\`\\\`\\\` We’ll generate a synthetic dataset by: 1. Sampling the design matrix \\\`\\\\\\\`X\\\\\\\`\\\`. 2. Sampling a ground truth parameter vector \\\`\\\\\\\`w\\\\\\\_star\\\\\\\`\\\`. 3. Compute the dependent values \\\`\\\\\\\`y\\\\\\\`\\\` by adding Gaussian noise to \\\`\\\\\\\`X\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`@\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`w\\\\\\\_star\\\\\\\`\\\`. \\\`\\\`\\\`python # True parameters w\\\_star = mx.random.normal((num\\\_features,)) # Input examples (design matrix) X = mx.random.normal((num\\\_examples, num\\\_features)) # Noisy labels eps = 1e-2 \\\* mx.random.normal((num\\\_examples,)) y = X @ w\\\_star + eps \\\`\\\`\\\` We will use SGD to find the optimal weights. To start, define the squared loss and get the gradient function of the loss with respect to the parameters. \\\`\\\`\\\`python def loss\\\_fn(w): return 0.5 \\\* mx.mean(mx.square(X @ w - y)) grad\\\_fn = mx.grad(loss\\\_fn) \\\`\\\`\\\` Start the optimization by initializing the parameters \\\`\\\\\\\`w\\\\\\\`\\\` randomly. Then repeatedly update the parameters for \\\`\\\\\\\`num\\\\\\\_iters\\\\\\\`\\\` iterations. \\\`\\\`\\\`python w = 1e-2 \\\* mx.random.normal((num\\\_features,)) for \\\_ in range(num\\\_iters): grad = grad\\\_fn(w) w = w - lr \\\* grad mx.eval(w) \\\`\\\`\\\` Finally, compute the loss of the learned parameters and verify that they are close to the ground truth parameters. \\\`\\\`\\\`python loss = loss\\\_fn(w) error\\\_norm = mx.sum(mx.square(w - w\\\_star)).item() \\\*\\\* 0.5 print( f"Loss {loss.item():.5f}, |w-w\\\*| = {error\\\_norm:.5f}, " ) # Should print something close to: Loss 0.00005, |w-w\\\*| = 0.00364 \\\`\\\`\\\` Complete \\\[linear regression\\\](https://github.com/ml-explore/mlx/tree/main/examples/python/linear\\\_regression.py) and \\\[logistic regression\\\](https://github.com/ml-explore/mlx/tree/main/examples/python/logistic\\\_regression.py) examples are available in the MLX GitHub repo. \\\[\\\](https://ml-explore.github.io/mlx/build/html/usage/export.html "previous page") previous Exporting Functions \\\[\\\](https://ml-explore.github.io/mlx/build/html/examples/mlp.html "next page") next Multi-Layer Perceptron By MLX Contributors © Copyright 2023, MLX Contributors.
