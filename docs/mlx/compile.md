Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - Keep compiled functions pure: no hidden mutation of globals or captured state. - Stabilize shapes for best results; frequent shape changes reduce compile benefits. - Validate numerics after enabling compile; small differences can appear due to fused kernels. - Toggle quickly during bring-up: \\\\\\\`mx.enable\\\\\\\_compile()\\\\\\\` / \\\\\\\`mx.disable\\\\\\\_compile()\\\\\\\`. ## How It Works - Function transform: \\\\\\\`mx.compile(fn)\\\\\\\` returns a compiled version of \\\\\\\`fn\\\\\\\` (you can also use the \\\\\\\`@mx.compile\\\\\\\` decorator). - First-call compile: the first call traces and optimizes the graph, so it is slower; results are cached. - Caching: subsequent calls are fast as long as input count, dtypes, ranks, and shapes match the cached signature. - Recompilation triggers: any change to - number of inputs - dtype of an input - rank (ndim) of an input - shape of an input will cause a new compile. ## Basic Example (timed) \\\\\\\`\\\\\\\`\\\\\\\`python import time import mlx.core as mx def run(): x = mx.random.normal((2048, 2048)) y = mx.random.normal((2048, 2048)) return mx.sum(x @ y) mx.disable\\\\\\\_compile() t0 = time.time(); \\\\\\\_ = run(); mx.eval(\\\\\\\_); t1 = time.time() mx.enable\\\\\\\_compile() t2 = time.time(); \\\\\\\_ = run(); mx.eval(\\\\\\\_); t3 = time.time() print("no-compile:", t1 - t0) print("compile: ", t3 - t2) \\\\\\\`\\\\\\\`\\\\\\\` ## Basic Example (math function) \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx import time def fun(x, y): return mx.exp(-x) + y x = mx.array(1.0) y = mx.array(2.0) t0 = time.time(); r0 = fun(x, y); mx.eval(r0); t1 = time.time() compiled\\\\\\\_fun = mx.compile(fun) t2 = time.time(); r1 = compiled\\\\\\\_fun(x, y); mx.eval(r1); t3 = time.time() t4 = time.time(); r2 = compiled\\\\\\\_fun(x, y); mx.eval(r2); t5 = time.time() print(f"Regular: {t1 - t0:.6f}s | First compiled: {t3 - t2:.6f}s | Cached: {t5 - t4:.6f}s") print(r0, r2) \\\\\\\`\\\\\\\`\\\\\\\` ## Advanced Example (training step) \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx import mlx.nn as nn from mlx.optimizers import SGD class MLP(nn.Module): def \\\\\\\_\\\\\\\_init\\\\\\\_\\\\\\\_(self, d\\\\\\\_in, d\\\\\\\_out): super().\\\\\\\_\\\\\\\_init\\\\\\\_\\\\\\\_() self.l1 = nn.Linear(d\\\\\\\_in, 256) self.l2 = nn.Linear(256, d\\\\\\\_out) def \\\\\\\_\\\\\\\_call\\\\\\\_\\\\\\\_(self, x): x = nn.relu(self.l1(x)) return self.l2(x) model = MLP(10, 5) params = model.parameters() opt = SGD(1e-2) def loss\\\\\\\_fn(p, x, y): logits = model.apply(p, x) return mx.mean(nn.losses.cross\\\\\\\_entropy(logits, y)) # Compile value\\\\\\\_and\\\\\\\_grad for the loss function loss\\\\\\\_and\\\\\\\_grad = mx.compile(mx.value\\\\\\\_and\\\\\\\_grad(loss\\\\\\\_fn)) x = mx.random.uniform(shape=(32, 10)) y = mx.random.randint(0, 5, (32,)) for step in range(5): loss, grads = loss\\\\\\\_and\\\\\\\_grad(params, x, y) params = opt.update(params, grads) mx.eval(loss) # ensure compute if step == 0: print("Initial loss:", float(loss)) print("Final loss:", float(loss)) \\\\\\\`\\\\\\\`\\\\\\\` ## Caveats and Best Practices - Pure functions: compile only side‑effect‑free functions; avoid I/O (e.g., \\\\\\\`print\\\\\\\`) in compiled regions. - Debugging: first call compiles; disable with \\\\\\\`mx.disable\\\\\\\_compile()\\\\\\\` or \\\\\\\`MLX\\\\\\\_DISABLE\\\\\\\_COMPILE=1\\\\\\\` when needed. - Control flow: input‑dependent branching can force recompiles; prefer shape‑stable, data‑independent control flow in compiled regions. \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/usage/compile.rst "Download source file") - .pdf # Compilation ## Contents \\\\- \\\[Basics of Compile\\\](https://ml-explore.github.io/mlx/build/html/#basics-of-compile) - \\\[Example Speedup\\\](https://ml-explore.github.io/mlx/build/html/#example-speedup) - \\\[Debugging\\\](https://ml-explore.github.io/mlx/build/html/#debugging) - \\\[Pure Functions\\\](https://ml-explore.github.io/mlx/build/html/#pure-functions) - \\\[Compiling Training Graphs\\\](https://ml-explore.github.io/mlx/build/html/#compiling-training-graphs) - \\\[Transformations with Compile\\\](https://ml-explore.github.io/mlx/build/html/#transformations-with-compile) - \\\[Shapeless Compilation\\\](https://ml-explore.github.io/mlx/build/html/#shapeless-compilation) # Compilation MLX has a \\\[\\\`\\\`compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html#mlx.core.compile "mlx.core.compile") function transformation which compiles computation graphs. Function compilation results in smaller graphs by merging common work and fusing certain operations. In many cases this can lead to big improvements in run-time and memory use. Getting started with \\\[\\\`\\\`compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html#mlx.core.compile "mlx.core.compile") is simple, but there are some edge cases that are good to be aware of for more complex graphs and advanced usage. ## Basics of Compile Let’s start with a simple example: \\\`\\\`\\\`python def fun(x, y): return mx.exp(-x) + y x = mx.array(1.0) y = mx.array(2.0) # Regular call, no compilation # Prints: array(2.36788, dtype=float32) print(fun(x, y)) # Compile the function compiled\\\_fun = mx.compile(fun) # Prints: array(2.36788, dtype=float32) print(compiled\\\_fun(x, y)) \\\`\\\`\\\` The output of both the regular function and the compiled function is the same up to numerical precision. The first time you call a compiled function, MLX will build the compute graph, optimize it, and generate and compile code. This can be relatively slow. However, MLX will cache compiled functions, so calling a compiled function multiple times will not initiate a new compilation. This means you should typically compile functions that you plan to use more than once. \\\`\\\`\\\`python def fun(x, y): return mx.exp(-x) + y x = mx.array(1.0) y = mx.array(2.0) compiled\\\_fun = mx.compile(fun) # Compiled here compiled\\\_fun(x, y) # Not compiled again compiled\\\_fun(x, y) # Not compiled again mx.compile(fun)(x, y) \\\`\\\`\\\` There are some important cases to be aware of that can cause a function to be recompiled: - Changing the shape or number of dimensions - Changing the type of any of the inputs - Changing the number of inputs to the function In certain cases only some of the compilation stack will be rerun (for example when changing the shapes) and in other cases the full compilation stack will be rerun (for example when changing the types). In general you should avoid compiling functions too frequently. Another idiom to watch out for is compiling functions which get created and destroyed frequently. This can happen, for example, when compiling an anonymous function in a loop: \\\`\\\`\\\`python a = mx.array(1.0) # Don't do this, compiles lambda at each iteration for \\\_ in range(5): mx.compile(lambda x: mx.exp(mx.abs(x)))(a) \\\`\\\`\\\` ## Example Speedup The \\\[\\\`\\\`mlx.nn.gelu()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary\\\_functions/mlx.nn.gelu.html#mlx.nn.gelu "mlx.nn.gelu") is a nonlinear activation function commonly used with Transformer-based models. The implementation involves several unary and binary element-wise operations: \\\`\\\`\\\`python def gelu(x): return x \\\* (1 + mx.erf(x / math.sqrt(2))) / 2 \\\`\\\`\\\` If you use this function with small arrays, it will be overhead bound. If you use it with large arrays it will be memory bandwidth bound. However, all of the operations in the \\\`\\\\\\\`gelu\\\\\\\`\\\` are fusible into a single kernel with \\\[\\\`\\\`compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html#mlx.core.compile "mlx.core.compile"). This can speedup both cases considerably. Let’s compare the runtime of the regular function versus the compiled function. We’ll use the following timing helper which does a warm up and handles synchronization: \\\`\\\`\\\`python import time def timeit(fun, x): # warm up for \\\_ in range(10): mx.eval(fun(x)) tic = time.perf\\\_counter() for \\\_ in range(100): mx.eval(fun(x)) toc = time.perf\\\_counter() tpi = 1e3 \\\* (toc - tic) / 100 print(f"Time per iteration {tpi:.3f} (ms)") \\\`\\\`\\\` Now make an array, and benchmark both functions: \\\`\\\`\\\`python x = mx.random.uniform(shape=(32, 1000, 4096)) timeit(nn.gelu, x) timeit(mx.compile(nn.gelu), x) \\\`\\\`\\\` On an M1 Max the times are 15.5 and 3.1 milliseconds. The compiled \\\`\\\\\\\`gelu\\\\\\\`\\\` is five times faster. ## Debugging When a compiled function is first called, it is traced with placeholder inputs. This means you can’t evaluate arrays (for example to print their contents) inside compiled functions. \\\`\\\`\\\`python @mx.compile def fun(x): z = -x print(z) # Crash return mx.exp(z) fun(mx.array(5.0)) \\\`\\\`\\\` For debugging, inspecting arrays can be helpful. One way to do that is to globally disable compilation using the \\\[\\\`\\\`disable\\\_compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.disable\\\_compile.html#mlx.core.disable\\\_compile "mlx.core.disable\\\_compile") function or \\\`\\\\\\\`MLX\\\\\\\_DISABLE\\\\\\\_COMPILE\\\\\\\`\\\` flag. For example the following is okay even though \\\`\\\\\\\`fun\\\\\\\`\\\` is compiled: \\\`\\\`\\\`python @mx.compile def fun(x): z = -x print(z) # Okay return mx.exp(z) mx.disable\\\_compile() fun(mx.array(5.0)) \\\`\\\`\\\` ## Pure Functions Compiled functions are intended to be \\\\\\\*pure\\\\\\\*; that is they should not have side effects. For example: \\\`\\\`\\\`python state = \\\[\\\] @mx.compile def fun(x, y): z = x + y state.append(z) return mx.exp(z) fun(mx.array(1.0), mx.array(2.0)) # Crash! print(state) \\\`\\\`\\\` After the first call of \\\`\\\\\\\`fun\\\\\\\`\\\`, the \\\`\\\\\\\`state\\\\\\\`\\\` list will hold a placeholder array. The placeholder does not have any data; it is only used to build the computation graph. Printing such an array results in a crash. You have two options to deal with this. The first option is to simply return \\\`\\\\\\\`state\\\\\\\`\\\` as an output: \\\`\\\`\\\`python state = \\\[\\\] @mx.compile def fun(x, y): z = x + y state.append(z) return mx.exp(z), state \\\_, state = fun(mx.array(1.0), mx.array(2.0)) # Prints \\\[array(3, dtype=float32)\\\] print(state) \\\`\\\`\\\` In some cases returning updated state can be pretty inconvenient. Hence, \\\[\\\`\\\`compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html#mlx.core.compile "mlx.core.compile") has a parameter to capture implicit outputs: \\\`\\\`\\\`python from functools import partial state = \\\[\\\] # Tell compile to capture state as an output @partial(mx.compile, outputs=state) def fun(x, y): z = x + y state.append(z) return mx.exp(z), state fun(mx.array(1.0), mx.array(2.0)) # Prints \\\[array(3, dtype=float32)\\\] print(state) \\\`\\\`\\\` This is particularly useful for compiling a function which includes an update to a container of arrays, as is commonly done when training the parameters of a \\\[\\\`\\\`mlx.nn.Module\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.Module"). Compiled functions will also treat any inputs not in the parameter list as constants. For example: \\\`\\\`\\\`python state = \\\[mx.array(1.0)\\\] @mx.compile def fun(x): return x + state\\\[0\\\] # Prints array(2, dtype=float32) print(fun(mx.array(1.0))) # Update state state\\\[0\\\] = mx.array(5.0) # Still prints array(2, dtype=float32) print(fun(mx.array(1.0))) \\\`\\\`\\\` In order to have the change of state reflected in the outputs of \\\`\\\\\\\`fun\\\\\\\`\\\` you again have two options. The first option is to simply pass \\\`\\\\\\\`state\\\\\\\`\\\` as input to the function. In some cases this can be pretty inconvenient. Hence, \\\[\\\`\\\`compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html#mlx.core.compile "mlx.core.compile") also has a parameter to capture implicit inputs: \\\`\\\`\\\`python from functools import partial state = \\\[mx.array(1.0)\\\] # Tell compile to capture state as an input @partial(mx.compile, inputs=state) def fun(x): return x + state\\\[0\\\] # Prints array(2, dtype=float32) print(fun(mx.array(1.0))) # Update state state\\\[0\\\] = mx.array(5.0) # Prints array(6, dtype=float32) print(fun(mx.array(1.0))) \\\`\\\`\\\` ## Compiling Training Graphs This section will step through how to use \\\[\\\`\\\`compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html#mlx.core.compile "mlx.core.compile") with a simple example of a common setup: training a model with \\\[\\\`\\\`mlx.nn.Module\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.Module") using an \\\[\\\`\\\`mlx.optimizers.Optimizer\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/optimizers/optimizer.html#mlx.optimizers.Optimizer "mlx.optimizers.Optimizer") with state. We will show how to compile the full forward, backward, and update with \\\[\\\`\\\`compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html#mlx.core.compile "mlx.core.compile"). To start, here is the simple example without any compilation: \\\`\\\`\\\`python import mlx.core as mx import mlx.nn as nn import mlx.optimizers as optim # 4 examples with 10 features each x = mx.random.uniform(shape=(4, 10)) # 0, 1 targets y = mx.array(\\\[0, 1, 0, 1\\\]) # Simple linear model model = nn.Linear(10, 1) # SGD with momentum optimizer = optim.SGD(learning\\\_rate=0.1, momentum=0.8) def loss\\\_fn(model, x, y): logits = model(x).squeeze() return nn.losses.binary\\\_cross\\\_entropy(logits, y) loss\\\_and\\\_grad\\\_fn = nn.value\\\_and\\\_grad(model, loss\\\_fn) # Perform 10 steps of gradient descent for it in range(10): loss, grads = loss\\\_and\\\_grad\\\_fn(model, x, y) optimizer.update(model, grads) mx.eval(model.parameters(), optimizer.state) \\\`\\\`\\\` To compile the update we can put it all in a function and compile it with the appropriate input and output captures. Here’s the same example but compiled: \\\`\\\`\\\`python import mlx.core as mx import mlx.nn as nn import mlx.optimizers as optim from functools import partial # 4 examples with 10 features each x = mx.random.uniform(shape=(4, 10)) # 0, 1 targets y = mx.array(\\\[0, 1, 0, 1\\\]) # Simple linear model model = nn.Linear(10, 1) # SGD with momentum optimizer = optim.SGD(learning\\\_rate=0.1, momentum=0.8) def loss\\\_fn(model, x, y): logits = model(x).squeeze() return nn.losses.binary\\\_cross\\\_entropy(logits, y) # The state that will be captured as input and output state = \\\[model.state, optimizer.state\\\] @partial(mx.compile, inputs=state, outputs=state) def step(x, y): loss\\\_and\\\_grad\\\_fn = nn.value\\\_and\\\_grad(model, loss\\\_fn) loss, grads = loss\\\_and\\\_grad\\\_fn(model, x, y) optimizer.update(model, grads) return loss # Perform 10 steps of gradient descent for it in range(10): loss = step(x, y) # Evaluate the model and optimizer state mx.eval(state) print(loss) \\\`\\\`\\\` Note If you are using a module which performs random sampling such as \\\[\\\`\\\`mlx.nn.Dropout()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/\\\_autosummary/mlx.nn.Dropout.html#mlx.nn.Dropout "mlx.nn.Dropout"), make sure you also include \\\`\\\\\\\`mx.random.state\\\\\\\`\\\` in the \\\`\\\\\\\`state\\\\\\\`\\\` captured by \\\[\\\`\\\`compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html#mlx.core.compile "mlx.core.compile"), i.e. \\\`\\\\\\\`state\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`=\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`\\\\\\\[model.state,\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`optimizer.state,\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`mx.random.state\\\\\\\]\\\\\\\`\\\`. Note For more examples of compiling full training graphs checkout the \\\[MLX Examples\\\](https://github.com/ml-explore/mlx-examples) GitHub repo. ## Transformations with Compile In MLX function transformations are composable. You can apply any function transformation to the output of any other function transformation. For more on this, see the documentation on \\\[function transforms\\\](https://ml-explore.github.io/mlx/build/html/usage/function\\\_transforms.html#function-transforms). Compiling transformed functions works just as expected: \\\`\\\`\\\`python grad\\\_fn = mx.grad(mx.exp) compiled\\\_grad\\\_fn = mx.compile(grad\\\_fn) # Prints: array(2.71828, dtype=float32) print(grad\\\_fn(mx.array(1.0))) # Also prints: array(2.71828, dtype=float32) print(compiled\\\_grad\\\_fn(mx.array(1.0))) \\\`\\\`\\\` Note In order to compile as much as possible, a transformation of a compiled function will not by default be compiled. To compile the transformed function simply pass it through \\\[\\\`\\\`compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html#mlx.core.compile "mlx.core.compile"). You can also compile functions which themselves call compiled functions. A good practice is to compile the outer most function to give \\\[\\\`\\\`compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html#mlx.core.compile "mlx.core.compile") the most opportunity to optimize the computation graph: \\\`\\\`\\\`python @mx.compile def inner(x): return mx.exp(-mx.abs(x)) def outer(x): inner(inner(x)) # Compiling the outer function is good to do as it will likely # be faster even though the inner functions are compiled fun = mx.compile(outer) \\\`\\\`\\\` ## Shapeless Compilation When the shape of an input to a compiled function changes, the function is recompiled. You can compile a function once and run it on inputs with variable shapes by specifying \\\`\\\\\\\`shapeless=True\\\\\\\`\\\` to \\\[\\\`\\\`compile()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html#mlx.core.compile "mlx.core.compile"). In this case changes to the shapes of the inputs do not cause the function to be recompiled. \\\`\\\`\\\`python def fun(x, y): return mx.abs(x + y) compiled\\\_fun = mx.compile(fun, shapeless=True) x = mx.array(1.0) y = mx.array(-2.0) # Firt call compiles the function print(compiled\\\_fun(x, y)) # Second call with different shapes # does not recompile the function x = mx.array(\\\[1.0, -6.0\\\]) y = mx.array(\\\[-2.0, 3.0\\\]) print(compiled\\\_fun(x, y)) \\\`\\\`\\\` Use shapeless compilations carefully. Since compilation is not triggered when shapes change, any graphs which are conditional on the input shapes will not work as expected. Shape-dependent computations are common and sometimes subtle to detect. For example: \\\`\\\`\\\`python def fun(x): return x.reshape(x.shape\\\[0\\\] \\\* x.shape\\\[1\\\], -1) compiled\\\_fun = mx.compile(fun, shapeless=True) x = mx.random.uniform(shape=(2, 3, 4)) out = compiled\\\_fun(x) x = mx.random.uniform(shape=(5, 5, 3)) # Error, can't reshape (5, 5, 3) to (6, -1) out = compiled\\\_fun(x) \\\`\\\`\\\` The second call to the \\\`\\\\\\\`compiled\\\\\\\_fun\\\\\\\`\\\` fails because of the call to \\\[\\\`\\\`reshape()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.reshape.html#mlx.core.reshape "mlx.core.reshape") which uses the static shape of \\\`\\\\\\\`x\\\\\\\`\\\` in the first call. We can fix this by using \\\[\\\`\\\`flatten()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.flatten.html#mlx.core.flatten "mlx.core.flatten") to avoid hardcoding the shape of \\\`\\\\\\\`x\\\\\\\`\\\`: \\\`\\\`\\\`python def fun(x): return x.flatten(0, 1) compiled\\\_fun = mx.compile(fun, shapeless=True) x = mx.random.uniform(shape=(2, 3, 4)) out = compiled\\\_fun(x) x = mx.random.uniform(shape=(5, 5, 3)) # Ok out = compiled\\\_fun(x) \\\`\\\`\\\` \\\[\\\](https://ml-explore.github.io/mlx/build/html/usage/function\\\_transforms.html "previous page") previous Function Transforms \\\[\\\](https://ml-explore.github.io/mlx/build/html/usage/numpy.html "next page") next Conversion to NumPy and Other Frameworks Contents \\\\- \\\[Basics of Compile\\\](https://ml-explore.github.io/mlx/build/html/#basics-of-compile) - \\\[Example Speedup\\\](https://ml-explore.github.io/mlx/build/html/#example-speedup) - \\\[Debugging\\\](https://ml-explore.github.io/mlx/build/html/#debugging) - \\\[Pure Functions\\\](https://ml-explore.github.io/mlx/build/html/#pure-functions) - \\\[Compiling Training Graphs\\\](https://ml-explore.github.io/mlx/build/html/#compiling-training-graphs) - \\\[Transformations with Compile\\\](https://ml-explore.github.io/mlx/build/html/#transformations-with-compile) - \\\[Shapeless Compilation\\\](https://ml-explore.github.io/mlx/build/html/#shapeless-compilation) By MLX Contributors © Copyright 2023, MLX Contributors.
