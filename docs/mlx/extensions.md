Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - Start with a Python shim that checks dtypes/shapes, then dispatches to your C++ op; fail early with clear errors. - Provide both CPU and GPU paths; feature‑gate GPU behind \\\\\\\`mx.metal.is\\\\\\\_available()\\\\\\\`. - Add a tiny correctness test: compare to a NumPy/MLX reference on small inputs, including non‑contiguous views. \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/dev/extensions.rst "Download source file") - .pdf # Custom Extensions in MLX ## Contents \\\\- \\\[Introducing the Example\\\](https://ml-explore.github.io/mlx/build/html/#introducing-the-example) - \\\[Operations and Primitives\\\](https://ml-explore.github.io/mlx/build/html/#operations-and-primitives) - \\\[Operations\\\](https://ml-explore.github.io/mlx/build/html/#operations) - \\\[Primitives\\\](https://ml-explore.github.io/mlx/build/html/#primitives) - \\\[Using the Primitive\\\](https://ml-explore.github.io/mlx/build/html/#using-the-primitive) - \\\[Implementing the Primitive\\\](https://ml-explore.github.io/mlx/build/html/#implementing-the-primitive) - \\\[Implementing the CPU Back-end\\\](https://ml-explore.github.io/mlx/build/html/#implementing-the-cpu-back-end) - \\\[Implementing the GPU Back-end\\\](https://ml-explore.github.io/mlx/build/html/#implementing-the-gpu-back-end) - \\\[Primitive Transforms\\\](https://ml-explore.github.io/mlx/build/html/#primitive-transforms) - \\\[Building and Binding\\\](https://ml-explore.github.io/mlx/build/html/#building-and-binding) - \\\[Binding to Python\\\](https://ml-explore.github.io/mlx/build/html/#binding-to-python) - \\\[Building with CMake\\\](https://ml-explore.github.io/mlx/build/html/#building-with-cmake) - \\\[Building with \\\`\\\`setuptools\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#building-with-setuptools) - \\\[Usage\\\](https://ml-explore.github.io/mlx/build/html/#usage) - \\\[Results\\\](https://ml-explore.github.io/mlx/build/html/#results) - \\\[Scripts\\\](https://ml-explore.github.io/mlx/build/html/#scripts) # Custom Extensions in MLX You can extend MLX with custom operations on the CPU or GPU. This guide explains how to do that with a simple example. ## Introducing the Example Let’s say you would like an operation that takes in two arrays, \\\`\\\\\\\`x\\\\\\\`\\\` and \\\`\\\\\\\`y\\\\\\\`\\\`, scales them both by coefficients \\\`\\\\\\\`alpha\\\\\\\`\\\` and \\\`\\\\\\\`beta\\\\\\\`\\\` respectively, and then adds them together to get the result \\\`\\\\\\\`z\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`=\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`alpha\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`\\\\\\\*\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`x\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`+\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`beta\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`\\\\\\\*\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`y\\\\\\\`\\\`. You can do that in MLX directly: \\\`\\\`\\\`python import mlx.core as mx def simple\\\_axpby(x: mx.array, y: mx.array, alpha: float, beta: float) -> mx.array: return alpha \\\* x + beta \\\* y \\\`\\\`\\\` This function performs that operation while leaving the implementation and function transformations to MLX. However, you may want to customize the underlying implementation, perhaps to make it faster. In this tutorial we will go through adding custom extensions. It will cover: - The structure of the MLX library. - Implementing a CPU operation. - Implementing a GPU operation using metal. - Adding the \\\`\\\\\\\`vjp\\\\\\\`\\\` and \\\`\\\\\\\`jvp\\\\\\\`\\\` function transformation. - Building a custom extension and binding it to python. ## Operations and Primitives Operations in MLX build the computation graph. Primitives provide the rules for evaluating and transforming the graph. Let’s start by discussing operations in more detail. ### Operations Operations are the front-end functions that operate on arrays. They are defined in the C++ API (\\\[Operations\\\](https://ml-explore.github.io/mlx/build/html/cpp/ops.html#cpp-ops)), and the Python API (\\\[Operations\\\](https://ml-explore.github.io/mlx/build/html/python/ops.html#ops)) binds them. We would like an operation \\\`\\\\\\\`axpby()\\\\\\\`\\\` that takes in two arrays, \\\`\\\\\\\`x\\\\\\\`\\\` and \\\`\\\\\\\`y\\\\\\\`\\\`, and two scalars, \\\`\\\\\\\`alpha\\\\\\\`\\\` and \\\`\\\\\\\`beta\\\\\\\`\\\`. This is how to define it in C++: \\\`\\\`\\\`c++ /\\\*\\\* \\\* Scale and sum two vectors element-wise \\\* z = alpha \\\* x + beta \\\* y \\\* \\\* Use NumPy-style broadcasting between x and y \\\* Inputs are upcasted to floats if needed \\\*\\\*/ array axpby( const array& x, // Input array x const array& y, // Input array y const float alpha, // Scaling factor for x const float beta, // Scaling factor for y StreamOrDevice s = {} // Stream on which to schedule the operation ); \\\`\\\`\\\` The simplest way to implement this is with existing operations: \\\`\\\`\\\`c++ array axpby( const array& x, // Input array x const array& y, // Input array y const float alpha, // Scaling factor for x const float beta, // Scaling factor for y StreamOrDevice s /\\\* = {} \\\*/ // Stream on which to schedule the operation ) { // Scale x and y on the provided stream auto ax = multiply(array(alpha), x, s); auto by = multiply(array(beta), y, s); // Add and return return add(ax, by, s); } \\\`\\\`\\\` The operations themselves do not contain the implementations that act on the data, nor do they contain the rules of transformations. Rather, they are an easy to use interface that use \\\`\\\\\\\`Primitive\\\\\\\`\\\` building blocks. ### Primitives A \\\`\\\\\\\`Primitive\\\\\\\`\\\` is part of the computation graph of an \\\`\\\\\\\`array\\\\\\\`\\\`. It defines how to create output arrays given input arrays. Further, a \\\`\\\\\\\`Primitive\\\\\\\`\\\` has methods to run on the CPU or GPU and for function transformations such as \\\`\\\\\\\`vjp\\\\\\\`\\\` and \\\`\\\\\\\`jvp\\\\\\\`\\\`. Let’s go back to our example to be more concrete: \\\`\\\`\\\`c++ class Axpby : public Primitive { public: explicit Axpby(Stream stream, float alpha, float beta) : Primitive(stream), alpha\\\_(alpha), beta\\\_(beta){}; /\\\*\\\* \\\* A primitive must know how to evaluate itself on the CPU/GPU \\\* for the given inputs and populate the output array. \\\* \\\* To avoid unnecessary allocations, the evaluation function \\\* is responsible for allocating space for the array. \\\*/ void eval\\\_cpu( const std::vector& inputs, std::vector& outputs) override; void eval\\\_gpu( const std::vector& inputs, std::vector& outputs) override; /\\\*\\\* The Jacobian-vector product. \\\*/ std::vector jvp( const std::vector& primals, const std::vector& tangents, const std::vector& argnums) override; /\\\*\\\* The vector-Jacobian product. \\\*/ std::vector vjp( const std::vector& primals, const std::vector& cotangents, const std::vector& argnums, const std::vector& outputs) override; /\\\*\\\* \\\* The primitive must know how to vectorize itself across \\\* the given axes. The output is a pair containing the array \\\* representing the vectorized computation and the axis which \\\* corresponds to the output vectorized dimension. \\\*/ virtual std::pair, std::vector> vmap( const std::vector& inputs, const std::vector& axes) override; /\\\*\\\* Print the primitive. \\\*/ void print(std::ostream& os) override { os << "Axpby"; } /\\\*\\\* Equivalence check \\\*\\\*/ bool is\\\_equivalent(const Primitive& other) const override; private: float alpha\\\_; float beta\\\_; }; \\\`\\\`\\\` The \\\`\\\\\\\`Axpby\\\\\\\`\\\` class derives from the base \\\`\\\\\\\`Primitive\\\\\\\`\\\` class. The \\\`\\\\\\\`Axpby\\\\\\\`\\\` treats \\\`\\\\\\\`alpha\\\\\\\`\\\` and \\\`\\\\\\\`beta\\\\\\\`\\\` as parameters. It then provides implementations of how the output array is produced given the inputs through \\\`\\\\\\\`Axpby::eval\\\\\\\_cpu()\\\\\\\`\\\` and \\\`\\\\\\\`Axpby::eval\\\\\\\_gpu()\\\\\\\`\\\`. It also provides rules of transformations in \\\`\\\\\\\`Axpby::jvp()\\\\\\\`\\\`, \\\`\\\\\\\`Axpby::vjp()\\\\\\\`\\\`, and \\\`\\\\\\\`Axpby::vmap()\\\\\\\`\\\`. ### Using the Primitive Operations can use this \\\`\\\\\\\`Primitive\\\\\\\`\\\` to add a new \\\`\\\\\\\`array\\\\\\\`\\\` to the computation graph. An \\\`\\\\\\\`array\\\\\\\`\\\` can be constructed by providing its data type, shape, the \\\`\\\\\\\`Primitive\\\\\\\`\\\` that computes it, and the \\\`\\\\\\\`array\\\\\\\`\\\` inputs that are passed to the primitive. Let’s reimplement our operation now in terms of our \\\`\\\\\\\`Axpby\\\\\\\`\\\` primitive. \\\`\\\`\\\`c++ array axpby( const array& x, // Input array x const array& y, // Input array y const float alpha, // Scaling factor for x const float beta, // Scaling factor for y StreamOrDevice s /\\\* = {} \\\*/ // Stream on which to schedule the operation ) { // Promote dtypes between x and y as needed auto promoted\\\_dtype = promote\\\_types(x.dtype(), y.dtype()); // Upcast to float32 for non-floating point inputs x and y auto out\\\_dtype = issubdtype(promoted\\\_dtype, float32) ? promoted\\\_dtype : promote\\\_types(promoted\\\_dtype, float32); // Cast x and y up to the determined dtype (on the same stream s) auto x\\\_casted = astype(x, out\\\_dtype, s); auto y\\\_casted = astype(y, out\\\_dtype, s); // Broadcast the shapes of x and y (on the same stream s) auto broadcasted\\\_inputs = broadcast\\\_arrays({x\\\_casted, y\\\_casted}, s); auto out\\\_shape = broadcasted\\\_inputs\\\[0\\\].shape(); // Construct the array as the output of the Axpby primitive // with the broadcasted and upcasted arrays as inputs return array( /\\\* const std::vector& shape = \\\*/ out\\\_shape, /\\\* Dtype dtype = \\\*/ out\\\_dtype, /\\\* std::unique\\\_ptr primitive = \\\*/ std::make\\\_shared(to\\\_stream(s), alpha, beta), /\\\* const std::vector& inputs = \\\*/ broadcasted\\\_inputs); } \\\`\\\`\\\` This operation now handles the following: 1. Upcast inputs and resolve the output data type. 2. Broadcast the inputs and resolve the output shape. 3. Construct the primitive \\\`\\\\\\\`Axpby\\\\\\\`\\\` using the given stream, \\\`\\\\\\\`alpha\\\\\\\`\\\`, and \\\`\\\\\\\`beta\\\\\\\`\\\`. 4. Construct the output \\\`\\\\\\\`array\\\\\\\`\\\` using the primitive and the inputs. ## Implementing the Primitive No computation happens when we call the operation alone. The operation only builds the computation graph. When we evaluate the output array, MLX schedules the execution of the computation graph, and calls \\\`\\\\\\\`Axpby::eval\\\\\\\_cpu()\\\\\\\`\\\` or \\\`\\\\\\\`Axpby::eval\\\\\\\_gpu()\\\\\\\`\\\` depending on the stream/device specified by the user. Warning When \\\`\\\\\\\`Primitive::eval\\\\\\\_cpu()\\\\\\\`\\\` or \\\`\\\\\\\`Primitive::eval\\\\\\\_gpu()\\\\\\\`\\\` are called, no memory has been allocated for the output array. It falls on the implementation of these functions to allocate memory as needed. ### Implementing the CPU Back-end Let’s start by implementing \\\`\\\\\\\`Axpby::eval\\\\\\\_cpu()\\\\\\\`\\\`. The method will go over each element of the output array, find the corresponding input elements of \\\`\\\\\\\`x\\\\\\\`\\\` and \\\`\\\\\\\`y\\\\\\\`\\\` and perform the operation point-wise. This is captured in the templated function \\\`\\\\\\\`axpby\\\\\\\_impl()\\\\\\\`\\\`. \\\`\\\`\\\`c++ template void axpby\\\_impl( const mx::array& x, const mx::array& y, mx::array& out, float alpha\\\_, float beta\\\_, mx::Stream stream) { out.set\\\_data(mx::allocator::malloc(out.nbytes())); // Get the CPU command encoder and register input and output arrays auto& encoder = mx::cpu::get\\\_command\\\_encoder(stream); encoder.set\\\_input\\\_array(x); encoder.set\\\_input\\\_array(y); encoder.set\\\_output\\\_array(out); // Launch the CPU kernel encoder.dispatch(\\\[x\\\_ptr = x.data(), y\\\_ptr = y.data(), out\\\_ptr = out.data(), size = out.size(), shape = out.shape(), x\\\_strides = x.strides(), y\\\_strides = y.strides(), alpha\\\_, beta\\\_\\\]() { // Cast alpha and beta to the relevant types T alpha = static\\\_cast(alpha\\\_); T beta = static\\\_cast(beta\\\_); // Do the element-wise operation for each output for (size\\\_t out\\\_idx = 0; out\\\_idx < size; out\\\_idx++) { // Map linear indices to offsets in x and y auto x\\\_offset = mx::elem\\\_to\\\_loc(out\\\_idx, shape, x\\\_strides); auto y\\\_offset = mx::elem\\\_to\\\_loc(out\\\_idx, shape, y\\\_strides); // We allocate the output to be contiguous and regularly strided // (defaults to row major) and hence it doesn't need additional mapping out\\\_ptr\\\[out\\\_idx\\\] = alpha \\\* x\\\_ptr\\\[x\\\_offset\\\] + beta \\\* y\\\_ptr\\\[y\\\_offset\\\]; } }); } \\\`\\\`\\\` Our implementation should work for all incoming floating point arrays. Accordingly, we add dispatches for \\\`\\\\\\\`float32\\\\\\\`\\\`, \\\`\\\\\\\`float16\\\\\\\`\\\`, \\\`\\\\\\\`bfloat16\\\\\\\`\\\` and \\\`\\\\\\\`complex64\\\\\\\`\\\`. We throw an error if we encounter an unexpected type. \\\`\\\`\\\`c++ void Axpby::eval\\\_cpu( const std::vector& inputs, std::vector& outputs) { auto& x = inputs\\\[0\\\]; auto& y = inputs\\\[1\\\]; auto& out = outputs\\\[0\\\]; // Dispatch to the correct dtype if (out.dtype() == mx::float32) { return axpby\\\_impl(x, y, out, alpha\\\_, beta\\\_, stream()); } else if (out.dtype() == mx::float16) { return axpby\\\_impl(x, y, out, alpha\\\_, beta\\\_, stream()); } else if (out.dtype() == mx::bfloat16) { return axpby\\\_impl(x, y, out, alpha\\\_, beta\\\_, stream()); } else if (out.dtype() == mx::complex64) { return axpby\\\_impl(x, y, out, alpha\\\_, beta\\\_, stream()); } else { throw std::runtime\\\_error( "Axpby is only supported for floating point types."); } } \\\`\\\`\\\` Just this much is enough to run the operation \\\`\\\\\\\`axpby()\\\\\\\`\\\` on a CPU stream! If you do not plan on running the operation on the GPU or using transforms on computation graphs that contain \\\`\\\\\\\`Axpby\\\\\\\`\\\`, you can stop implementing the primitive here. ### Implementing the GPU Back-end Apple silicon devices address their GPUs using the \\\[Metal\\\](https://developer.apple.com/documentation/metal?language=objc) shading language, and GPU kernels in MLX are written using Metal. Note Here are some helpful resources if you are new to Metal: - A walkthrough of the metal compute pipeline: \\\[Metal Example\\\](https://developer.apple.com/documentation/metal/performing\\\_calculations\\\_on\\\_a\\\_gpu?language=objc) - Documentation for metal shading language: \\\[Metal Specification\\\](https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf) - Using metal from C++: \\\[Metal-cpp\\\](https://developer.apple.com/metal/cpp/) Let’s keep the GPU kernel simple. We will launch exactly as many threads as there are elements in the output. Each thread will pick the element it needs from \\\`\\\\\\\`x\\\\\\\`\\\` and \\\`\\\\\\\`y\\\\\\\`\\\`, do the point-wise operation, and update its assigned element in the output. \\\`\\\`\\\`c++ template \\\[\\\[kernel\\\]\\\] void axpby\\\_general( device const T\\\* x \\\[\\\[buffer(0)\\\]\\\], device const T\\\* y \\\[\\\[buffer(1)\\\]\\\], device T\\\* out \\\[\\\[buffer(2)\\\]\\\], constant const float& alpha \\\[\\\[buffer(3)\\\]\\\], constant const float& beta \\\[\\\[buffer(4)\\\]\\\], constant const int\\\* shape \\\[\\\[buffer(5)\\\]\\\], constant const int64\\\_t\\\* x\\\_strides \\\[\\\[buffer(6)\\\]\\\], constant const int64\\\_t\\\* y\\\_strides \\\[\\\[buffer(7)\\\]\\\], constant const int& ndim \\\[\\\[buffer(8)\\\]\\\], uint index \\\[\\\[thread\\\_position\\\_in\\\_grid\\\]\\\]) { // Convert linear indices to offsets in array auto x\\\_offset = elem\\\_to\\\_loc(index, shape, x\\\_strides, ndim); auto y\\\_offset = elem\\\_to\\\_loc(index, shape, y\\\_strides, ndim); // Do the operation and update the output out\\\[index\\\] = static\\\_cast(alpha) \\\* x\\\[x\\\_offset\\\] + static\\\_cast(beta) \\\* y\\\[y\\\_offset\\\]; } \\\`\\\`\\\` We then need to instantiate this template for all floating point types and give each instantiation a unique host name so we can identify it. \\\`\\\`\\\`c++ instantiate\\\_kernel("axpby\\\_general\\\_float32", axpby\\\_general, float) instantiate\\\_kernel("axpby\\\_general\\\_float16", axpby\\\_general, float16\\\_t) instantiate\\\_kernel("axpby\\\_general\\\_bfloat16", axpby\\\_general, bfloat16\\\_t) instantiate\\\_kernel("axpby\\\_general\\\_complex64", axpby\\\_general, complex64\\\_t) \\\`\\\`\\\` The logic to determine the kernel, set the inputs, resolve the grid dimensions, and dispatch to the GPU are contained in \\\`\\\\\\\`Axpby::eval\\\\\\\_gpu()\\\\\\\`\\\` as shown below. \\\`\\\`\\\`c++ /\\\*\\\* Evaluate primitive on GPU \\\*/ void Axpby::eval\\\_gpu( const std::vector& inputs, std::vector& outputs) { // Prepare inputs assert(inputs.size() == 2); auto& x = inputs\\\[0\\\]; auto& y = inputs\\\[1\\\]; auto& out = outputs\\\[0\\\]; // Each primitive carries the stream it should execute on // and each stream carries its device identifiers auto& s = stream(); // We get the needed metal device using the stream auto& d = metal::device(s.device); // Allocate output memory out.set\\\_data(allocator::malloc(out.nbytes())); // Resolve name of kernel std::ostringstream kname; kname maxTotalThreadsPerThreadgroup()); // Fix the 3D size of each threadgroup (in terms of threads) MTL::Size group\\\_dims = MTL::Size(tgp\\\_size, 1, 1); // Fix the 3D size of the launch grid (in terms of threads) MTL::Size grid\\\_dims = MTL::Size(nelem, 1, 1); // Launch the grid with the given number of threads divided among // the given threadgroups compute\\\_encoder.dispatch\\\_threads(grid\\\_dims, group\\\_dims); } \\\`\\\`\\\` We can now call the \\\`\\\\\\\`axpby()\\\\\\\`\\\` operation on both the CPU and the GPU! A few things to note about MLX and Metal before moving on. MLX keeps track of the active \\\`\\\\\\\`command\\\\\\\_buffer\\\\\\\`\\\` and the \\\`\\\\\\\`MTLCommandBuffer\\\\\\\`\\\` to which it is associated. We rely on \\\`\\\\\\\`d.get\\\\\\\_command\\\\\\\_encoder()\\\\\\\`\\\` to give us the active metal compute command encoder instead of building a new one and calling \\\`\\\\\\\`compute\\\\\\\_encoder->end\\\\\\\_encoding()\\\\\\\`\\\` at the end. MLX adds kernels (compute pipelines) to the active command buffer until some specified limit is hit or the command buffer needs to be flushed for synchronization. ### Primitive Transforms Next, let’s add implementations for transformations in a \\\`\\\\\\\`Primitive\\\\\\\`\\\`. These transformations can be built on top of other operations, including the one we just defined: \\\`\\\`\\\`c++ /\\\*\\\* The Jacobian-vector product. \\\*/ std::vector Axpby::jvp( const std::vector& primals, const std::vector& tangents, const std::vector& argnums) { // Forward mode diff that pushes along the tangents // The jvp transform on the primitive can be built with ops // that are scheduled on the same stream as the primitive // If argnums = {0}, we only push along x in which case the // jvp is just the tangent scaled by alpha // Similarly, if argnums = {1}, the jvp is just the tangent // scaled by beta if (argnums.size() > 1) { auto scale = argnums\\\[0\\\] == 0 ? alpha\\\_ : beta\\\_; auto scale\\\_arr = array(scale, tangents\\\[0\\\].dtype()); return {multiply(scale\\\_arr, tangents\\\[0\\\], stream())}; } // If argnums = {0, 1}, we take contributions from both // which gives us jvp = tangent\\\_x \\\* alpha + tangent\\\_y \\\* beta else { return {axpby(tangents\\\[0\\\], tangents\\\[1\\\], alpha\\\_, beta\\\_, stream())}; } } \\\`\\\`\\\` \\\`\\\`\\\`c++ /\\\*\\\* The vector-Jacobian product. \\\*/ std::vector Axpby::vjp( const std::vector& primals, const std::vector& cotangents, const std::vector& argnums, const std::vector& /\\\* unused \\\*/) { // Reverse mode diff std::vector vjps; for (auto arg : argnums) { auto scale = arg == 0 ? alpha\\\_ : beta\\\_; auto scale\\\_arr = array(scale, cotangents\\\[0\\\].dtype()); vjps.push\\\_back(multiply(scale\\\_arr, cotangents\\\[0\\\], stream())); } return vjps; } \\\`\\\`\\\` Note, a transformation does not need to be fully defined to start using the \\\`\\\\\\\`Primitive\\\\\\\`\\\`. \\\`\\\`\\\`c++ /\\\*\\\* Vectorize primitive along given axis \\\*/ std::pair, std::vector> Axpby::vmap( const std::vector& inputs, const std::vector& axes) { throw std::runtime\\\_error("\\\[Axpby\\\] vmap not implemented."); } \\\`\\\`\\\` ## Building and Binding Let’s look at the overall directory structure first. extensions ├── axpby │ ├── axpby.cpp │ ├── axpby.h │ └── axpby.metal ├── mlx\\\\\\\_sample\\\\\\\_extensions │ └── \\\\\\\\\\\\\\\_\\\\\\\\\\\\\\\_init\\\\\\\\\\\\\\\_\\\\\\\\\\\\\\\_.py ├── bindings.cpp ├── CMakeLists.txt └── setup.py \\\\- \\\`\\\\\\\`extensions/axpby/\\\\\\\`\\\` defines the C++ extension library - \\\`\\\\\\\`extensions/mlx\\\\\\\_sample\\\\\\\_extensions\\\\\\\`\\\` sets out the structure for the associated Python package - \\\`\\\\\\\`extensions/bindings.cpp\\\\\\\`\\\` provides Python bindings for our operation - \\\`\\\\\\\`extensions/CMakeLists.txt\\\\\\\`\\\` holds CMake rules to build the library and Python bindings - \\\`\\\\\\\`extensions/setup.py\\\\\\\`\\\` holds the \\\`\\\\\\\`setuptools\\\\\\\`\\\` rules to build and install the Python package ### Binding to Python We use \\\[nanobind\\\](https://nanobind.readthedocs.io/en/latest/) to build a Python API for the C++ library. Since bindings for components such as \\\[\\\`\\\`mlx.core.array\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array"), \\\[\\\`\\\`mlx.core.stream\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.stream.html#mlx.core.stream "mlx.core.stream"), etc. are already provided, adding our \\\`\\\\\\\`axpby()\\\\\\\`\\\` is simple. \\\`\\\`\\\`c++ NB\\\_MODULE(\\\_ext, m) { m.doc() = "Sample extension for MLX"; m.def( "axpby", &axpby, "x"\\\_a, "y"\\\_a, "alpha"\\\_a, "beta"\\\_a, nb::kw\\\_only(), "stream"\\\_a = nb::none(), R"( Scale and sum two vectors element-wise \\\`\\\`z = alpha \\\* x + beta \\\* y\\\`\\\` Follows numpy style broadcasting between \\\`\\\`x\\\`\\\` and \\\`\\\`y\\\`\\\` Inputs are upcasted to floats if needed Args: x (array): Input array. y (array): Input array. alpha (float): Scaling factor for \\\`\\\`x\\\`\\\`. beta (float): Scaling factor for \\\`\\\`y\\\`\\\`. Returns: array: \\\`\\\`alpha \\\* x + beta \\\* y\\\`\\\` )"); } \\\`\\\`\\\` Most of the complexity in the above example comes from additional bells and whistles such as the literal names and doc-strings. Warning \\\`\\\\\\\`mlx.core\\\\\\\`\\\` must be imported before importing \\\`\\\\\\\`mlx\\\\\\\_sample\\\\\\\_extensions\\\\\\\`\\\` as defined by the nanobind module above to ensure that the casters for \\\`\\\\\\\`mlx.core\\\\\\\`\\\` components like \\\[\\\`\\\`mlx.core.array\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array") are available. ### Building with CMake Building the C++ extension library only requires that you \\\`\\\\\\\`find\\\\\\\_package(MLX\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`CONFIG)\\\\\\\`\\\` and then link it to your library. \\\`\\\`\\\`cmake # Add library add\\\_library(mlx\\\_ext) # Add sources target\\\_sources( mlx\\\_ext PUBLIC ${CMAKE\\\_CURRENT\\\_LIST\\\_DIR}/axpby/axpby.cpp ) # Add include headers target\\\_include\\\_directories( mlx\\\_ext PUBLIC ${CMAKE\\\_CURRENT\\\_LIST\\\_DIR} ) # Link to mlx target\\\_link\\\_libraries(mlx\\\_ext PUBLIC mlx) \\\`\\\`\\\` We also need to build the attached Metal library. For convenience, we provide a \\\`\\\\\\\`mlx\\\\\\\_build\\\\\\\_metallib()\\\\\\\`\\\` function that builds a \\\`\\\\\\\`.metallib\\\\\\\`\\\` target given sources, headers, destinations, etc. (defined in \\\`\\\\\\\`cmake/extension.cmake\\\\\\\`\\\` and automatically imported with MLX package). Here is what that looks like in practice: \\\`\\\`\\\`cmake # Build metallib if(MLX\\\_BUILD\\\_METAL) mlx\\\_build\\\_metallib( TARGET mlx\\\_ext\\\_metallib TITLE mlx\\\_ext SOURCES ${CMAKE\\\_CURRENT\\\_LIST\\\_DIR}/axpby/axpby.metal INCLUDE\\\_DIRS ${PROJECT\\\_SOURCE\\\_DIR} ${MLX\\\_INCLUDE\\\_DIRS} OUTPUT\\\_DIRECTORY ${CMAKE\\\_LIBRARY\\\_OUTPUT\\\_DIRECTORY} ) add\\\_dependencies( mlx\\\_ext mlx\\\_ext\\\_metallib ) endif() \\\`\\\`\\\` Finally, we build the \\\[nanobind\\\](https://nanobind.readthedocs.io/en/latest/) bindings \\\`\\\`\\\`cmake nanobind\\\_add\\\_module( \\\_ext NB\\\_STATIC STABLE\\\_ABI LTO NOMINSIZE NB\\\_DOMAIN mlx ${CMAKE\\\_CURRENT\\\_LIST\\\_DIR}/bindings.cpp ) target\\\_link\\\_libraries(\\\_ext PRIVATE mlx\\\_ext) if(BUILD\\\_SHARED\\\_LIBS) target\\\_link\\\_options(\\\_ext PRIVATE -Wl,-rpath,@loader\\\_path) endif() \\\`\\\`\\\` ### Building with \\\`\\\\\\\`setuptools\\\\\\\`\\\` Once we have set out the CMake build rules as described above, we can use the build utilities defined in \\\`\\\\\\\`mlx.extension\\\\\\\`\\\`: \\\`\\\`\\\`python from mlx import extension from setuptools import setup if \\\_\\\_name\\\_\\\_ == "\\\_\\\_main\\\_\\\_": setup( name="mlx\\\_sample\\\_extensions", version="0.0.0", description="Sample C++ and Metal extensions for MLX primitives.", ext\\\_modules=\\\[extension.CMakeExtension("mlx\\\_sample\\\_extensions.\\\_ext")\\\], cmdclass={"build\\\_ext": extension.CMakeBuild}, packages=\\\["mlx\\\_sample\\\_extensions"\\\], package\\\_data={"mlx\\\_sample\\\_extensions": \\\["\\\*.so", "\\\*.dylib", "\\\*.metallib"\\\]}, extras\\\_require={"dev":\\\[\\\]}, zip\\\_safe=False, python\\\_requires=">=3.8", ) \\\`\\\`\\\` Note We treat \\\`\\\\\\\`extensions/mlx\\\\\\\_sample\\\\\\\_extensions\\\\\\\`\\\` as the package directory even though it only contains a \\\`\\\\\\\`\\\\\\\_\\\\\\\_init\\\\\\\_\\\\\\\_.py\\\\\\\`\\\` to ensure the following: - \\\`\\\\\\\`mlx.core\\\\\\\`\\\` must be imported before importing \\\`\\\\\\\`\\\\\\\_ext\\\\\\\`\\\` - The C++ extension library and the metal library are co-located with the python bindings and copied together if the package is installed To build the package, first install the build dependencies with \\\`\\\\\\\`pip\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`install\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`-r\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`requirements.txt\\\\\\\`\\\`. You can then build inplace for development using \\\`\\\\\\\`python\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`setup.py\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`build\\\\\\\_ext\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`-j8\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`--inplace\\\\\\\`\\\` (in \\\`\\\\\\\`extensions/\\\\\\\`\\\`) This results in the directory structure: extensions ├── mlx\\\\\\\_sample\\\\\\\_extensions │ ├── \\\\\\\\\\\\\\\_\\\\\\\\\\\\\\\_init\\\\\\\\\\\\\\\_\\\\\\\\\\\\\\\_.py │ ├── libmlx\\\\\\\_ext.dylib \\\\\\\\# C++ extension library │ ├── mlx\\\\\\\_ext.metallib \\\\\\\\# Metal library │ └── \\\\\\\\\\\\\\\_ext.cpython-3x-darwin.so \\\\\\\\# Python Binding … When you try to install using the command \\\`\\\\\\\`python\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`-m\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`pip\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`install\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`.\\\\\\\`\\\` (in \\\`\\\\\\\`extensions/\\\\\\\`\\\`), the package will be installed with the same structure as \\\`\\\\\\\`extensions/mlx\\\\\\\_sample\\\\\\\_extensions\\\\\\\`\\\` and the C++ and Metal library will be copied along with the Python binding since they are specified as \\\`\\\\\\\`package\\\\\\\_data\\\\\\\`\\\`. ## Usage After installing the extension as described above, you should be able to simply import the Python package and play with it as you would any other MLX operation. Let’s look at a simple script and its results: \\\`\\\`\\\`python import mlx.core as mx from mlx\\\_sample\\\_extensions import axpby a = mx.ones((3, 4)) b = mx.ones((3, 4)) c = axpby(a, b, 4.0, 2.0, stream=mx.cpu) print(f"c shape: {c.shape}") print(f"c dtype: {c.dtype}") print(f"c is correct: {mx.all(c == 6.0).item()}") \\\`\\\`\\\` Output: \\\`\\\`\\\`python c shape: \\\[3, 4\\\] c dtype: float32 c is correct: True \\\`\\\`\\\` ### Results Let’s run a quick benchmark and see how our new \\\`\\\\\\\`axpby\\\\\\\`\\\` operation compares with the naive \\\`\\\\\\\`simple\\\\\\\_axpby()\\\\\\\`\\\` we first defined. \\\`\\\`\\\`python import mlx.core as mx from mlx\\\_sample\\\_extensions import axpby import time def simple\\\_axpby(x: mx.array, y: mx.array, alpha: float, beta: float) -> mx.array: return alpha \\\* x + beta \\\* y M = 4096 N = 4096 x = mx.random.normal((M, N)) y = mx.random.normal((M, N)) alpha = 4.0 beta = 2.0 mx.eval(x, y) def bench(f): # Warm up for i in range(5): z = f(x, y, alpha, beta) mx.eval(z) # Timed run s = time.time() for i in range(100): z = f(x, y, alpha, beta) mx.eval(z) e = time.time() return 1000 \\\* (e - s) / 100 simple\\\_time = bench(simple\\\_axpby) custom\\\_time = bench(axpby) print(f"Simple axpby: {simple\\\_time:.3f} ms | Custom axpby: {custom\\\_time:.3f} ms") \\\`\\\`\\\` The results are \\\`\\\\\\\`Simple\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`axpby:\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`1.559\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`ms\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`|\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`Custom\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`axpby:\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`0.774\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`ms\\\\\\\`\\\`. We see modest improvements right away! This operation is now good to be used to build other operations, in \\\[\\\`\\\`mlx.nn.Module\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.Module") calls, and also as a part of graph transformations like \\\`\\\\\\\`grad()\\\\\\\`\\\`. ## Scripts Download the code The full example code is available in \\\[mlx\\\](https://github.com/ml-explore/mlx/tree/main/examples/extensions/). \\\[\\\](https://ml-explore.github.io/mlx/build/html/cpp/ops.html "previous page") previous Operations \\\[\\\](https://ml-explore.github.io/mlx/build/html/dev/metal\\\_debugger.html "next page") next Metal Debugger Contents \\\\- \\\[Introducing the Example\\\](https://ml-explore.github.io/mlx/build/html/#introducing-the-example) - \\\[Operations and Primitives\\\](https://ml-explore.github.io/mlx/build/html/#operations-and-primitives) - \\\[Operations\\\](https://ml-explore.github.io/mlx/build/html/#operations) - \\\[Primitives\\\](https://ml-explore.github.io/mlx/build/html/#primitives) - \\\[Using the Primitive\\\](https://ml-explore.github.io/mlx/build/html/#using-the-primitive) - \\\[Implementing the Primitive\\\](https://ml-explore.github.io/mlx/build/html/#implementing-the-primitive) - \\\[Implementing the CPU Back-end\\\](https://ml-explore.github.io/mlx/build/html/#implementing-the-cpu-back-end) - \\\[Implementing the GPU Back-end\\\](https://ml-explore.github.io/mlx/build/html/#implementing-the-gpu-back-end) - \\\[Primitive Transforms\\\](https://ml-explore.github.io/mlx/build/html/#primitive-transforms) - \\\[Building and Binding\\\](https://ml-explore.github.io/mlx/build/html/#building-and-binding) - \\\[Binding to Python\\\](https://ml-explore.github.io/mlx/build/html/#binding-to-python) - \\\[Building with CMake\\\](https://ml-explore.github.io/mlx/build/html/#building-with-cmake) - \\\[Building with \\\`\\\`setuptools\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#building-with-setuptools) - \\\[Usage\\\](https://ml-explore.github.io/mlx/build/html/#usage) - \\\[Results\\\](https://ml-explore.github.io/mlx/build/html/#results) - \\\[Scripts\\\](https://ml-explore.github.io/mlx/build/html/#scripts) By MLX Contributors © Copyright 2023, MLX Contributors.
