Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - Force materialization when needed: \\\\\\\`mx.eval(x)\\\\\\\` or by calling \\\\\\\`.item()\\\\\\\` on scalars. - When profiling, insert evals at boundaries to isolate segments; remove them for final performance. - Be cautious with side‑effects depending on timing; rely on pure functions to avoid surprises. - Framework boundaries: converting to NumPy and mutating via a view is out‑of‑graph and breaks lazy optimizations/gradients. Prefer to keep compute in MLX; if interop is required, copy at the boundary (\\\\\\\`np.array(x, copy=True)\\\\\\\` and \\\\\\\`mx.array(np\\\\\\\_arr.copy())\\\\\\\`). \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/usage/lazy\\\_evaluation.rst "Download source file") - .pdf # Lazy Evaluation ## Contents \\\\- \\\[Why Lazy Evaluation\\\](https://ml-explore.github.io/mlx/build/html/#why-lazy-evaluation) - \\\[Transforming Compute Graphs\\\](https://ml-explore.github.io/mlx/build/html/#transforming-compute-graphs) - \\\[Only Compute What You Use\\\](https://ml-explore.github.io/mlx/build/html/#only-compute-what-you-use) - \\\[When to Evaluate\\\](https://ml-explore.github.io/mlx/build/html/#when-to-evaluate) # Lazy Evaluation ## Why Lazy Evaluation When you perform operations in MLX, no computation actually happens. Instead a compute graph is recorded. The actual computation only happens if an \\\[\\\`\\\`eval()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.eval.html#mlx.core.eval "mlx.core.eval") is performed. MLX uses lazy evaluation because it has some nice features, some of which we describe below. ### Transforming Compute Graphs Lazy evaluation lets us record a compute graph without actually doing any computations. This is useful for function transformations like \\\[\\\`\\\`grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.grad.html#mlx.core.grad "mlx.core.grad") and \\\[\\\`\\\`vmap()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.vmap.html#mlx.core.vmap "mlx.core.vmap") and graph optimizations. Currently, MLX does not compile and rerun compute graphs. They are all generated dynamically. However, lazy evaluation makes it much easier to integrate compilation for future performance enhancements. ### Only Compute What You Use In MLX you do not need to worry as much about computing outputs that are never used. For example: \\\`\\\`\\\`python def fun(x): a = fun1(x) b = expensive\\\_fun(a) return a, b y, \\\_ = fun(x) \\\`\\\`\\\` Here, we never actually compute the output of \\\`\\\\\\\`expensive\\\\\\\_fun\\\\\\\`\\\`. Use this pattern with care though, as the graph of \\\`\\\\\\\`expensive\\\\\\\_fun\\\\\\\`\\\` is still built, and that has some cost associated to it. Similarly, lazy evaluation can be beneficial for saving memory while keeping code simple. Say you have a very large model \\\`\\\\\\\`Model\\\\\\\`\\\` derived from \\\[\\\`\\\`mlx.nn.Module\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/nn/module.html#mlx.nn.Module "mlx.nn.Module"). You can instantiate this model with \\\`\\\\\\\`model\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`=\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`Model()\\\\\\\`\\\`. Typically, this will initialize all of the weights as \\\`\\\\\\\`float32\\\\\\\`\\\`, but the initialization does not actually compute anything until you perform an \\\[\\\`\\\`eval()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.eval.html#mlx.core.eval "mlx.core.eval"). If you update the model with \\\`\\\\\\\`float16\\\\\\\`\\\` weights, your maximum consumed memory will be half that required if eager computation was used instead. This pattern is simple to do in MLX thanks to lazy computation: \\\`\\\`\\\`python model = Model() # no memory used yet model.load\\\_weights("weights\\\_fp16.safetensors") \\\`\\\`\\\` ## When to Evaluate A common question is when to use \\\[\\\`\\\`eval()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.eval.html#mlx.core.eval "mlx.core.eval"). The trade-off is between letting graphs get too large and not batching enough useful work. For example: \\\`\\\`\\\`python for \\\_ in range(100): a = a + b mx.eval(a) b = b \\\* 2 mx.eval(b) \\\`\\\`\\\` This is a bad idea because there is some fixed overhead with each graph evaluation. On the other hand, there is some slight overhead which grows with the compute graph size, so extremely large graphs (while computationally correct) can be costly. Luckily, a wide range of compute graph sizes work pretty well with MLX: anything from a few tens of operations to many thousands of operations per evaluation should be okay. Most numerical computations have an iterative outer loop (e.g. the iteration in stochastic gradient descent). A natural and usually efficient place to use \\\[\\\`\\\`eval()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.eval.html#mlx.core.eval "mlx.core.eval") is at each iteration of this outer loop. Here is a concrete example: \\\`\\\`\\\`python for batch in dataset: # Nothing has been evaluated yet loss, grad = value\\\_and\\\_grad\\\_fn(model, batch) # Still nothing has been evaluated optimizer.update(model, grad) # Evaluate the loss and the new parameters which will # run the full gradient computation and optimizer update mx.eval(loss, model.parameters()) \\\`\\\`\\\` An important behavior to be aware of is when the graph will be implicitly evaluated. Anytime you \\\`\\\\\\\`print\\\\\\\`\\\` an array, convert it to an \\\[\\\`\\\`numpy.ndarray\\\`\\\`\\\](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray "(in NumPy v2.2)"), or otherwise access its memory via \\\[\\\`\\\`memoryview\\\`\\\`\\\](https://docs.python.org/3/library/stdtypes.html#memoryview "(in Python v3.13)"), the graph will be evaluated. Saving arrays via \\\[\\\`\\\`save()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.save.html#mlx.core.save "mlx.core.save") (or any other MLX saving functions) will also evaluate the array. Calling \\\[\\\`\\\`array.item()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.item.html#mlx.core.array.item "mlx.core.array.item") on a scalar array will also evaluate it. In the example above, printing the loss (\\\`\\\\\\\`print(loss)\\\\\\\`\\\`) or adding the loss scalar to a list (\\\`\\\\\\\`losses.append(loss.item())\\\\\\\`\\\`) would cause a graph evaluation. If these lines are before \\\`\\\\\\\`mx.eval(loss,\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`model.parameters())\\\\\\\`\\\` then this will be a partial evaluation, computing only the forward pass. Also, calling \\\[\\\`\\\`eval()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.eval.html#mlx.core.eval "mlx.core.eval") on an array or set of arrays multiple times is perfectly fine. This is effectively a no-op. Warning Using scalar arrays for control-flow will cause an evaluation. Here is an example: \\\`\\\`\\\`python def fun(x): h, y = first\\\_layer(x) if y > 0: # An evaluation is done here! z = second\\\_layer\\\_a(h) else: z = second\\\_layer\\\_b(h) return z \\\`\\\`\\\` Using arrays for control flow should be done with care. The above example works and can even be used with gradient transformations. However, this can be very inefficient if evaluations are done too frequently. \\\[\\\](https://ml-explore.github.io/mlx/build/html/usage/quick\\\_start.html "previous page") previous Quick Start Guide \\\[\\\](https://ml-explore.github.io/mlx/build/html/usage/unified\\\_memory.html "next page") next Unified Memory Contents \\\\- \\\[Why Lazy Evaluation\\\](https://ml-explore.github.io/mlx/build/html/#why-lazy-evaluation) - \\\[Transforming Compute Graphs\\\](https://ml-explore.github.io/mlx/build/html/#transforming-compute-graphs) - \\\[Only Compute What You Use\\\](https://ml-explore.github.io/mlx/build/html/#only-compute-what-you-use) - \\\[When to Evaluate\\\](https://ml-explore.github.io/mlx/build/html/#when-to-evaluate) By MLX Contributors © Copyright 2023, MLX Contributors.
