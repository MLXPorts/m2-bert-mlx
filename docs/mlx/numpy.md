Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/usage/numpy.rst "Download source file") - .pdf # Conversion to NumPy and Other Frameworks ## Contents \\\\- \\\[PyTorch\\\](https://ml-explore.github.io/mlx/build/html/#pytorch) - \\\[JAX\\\](https://ml-explore.github.io/mlx/build/html/#jax) - \\\[TensorFlow\\\](https://ml-explore.github.io/mlx/build/html/#tensorflow) # Conversion to NumPy and Other Frameworks MLX array supports conversion between other frameworks with either: - The \\\[Python Buffer Protocol\\\](https://docs.python.org/3/c-api/buffer.html). - \\\[DLPack\\\](https://dmlc.github.io/dlpack/latest/). Let’s convert an array to NumPy and back. \\\`\\\`\\\`python import mlx.core as mx import numpy as np a = mx.arange(3) b = np.array(a) # copy of a c = mx.array(b) # copy of b \\\`\\\`\\\` Note Since NumPy does not support \\\`\\\\\\\`bfloat16\\\\\\\`\\\` arrays, you will need to convert to \\\`\\\\\\\`float16\\\\\\\`\\\` or \\\`\\\\\\\`float32\\\\\\\`\\\` first: \\\`\\\\\\\`np.array(a.astype(mx.float32))\\\\\\\`\\\`. Otherwise, you will receive an error like: \\\`\\\\\\\`Item\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`size\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`2\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`for\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`PEP\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`3118\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`buffer\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`format\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`string\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`does\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`not\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`match\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`the\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`dtype\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`V\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`item\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`size\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`0.\\\\\\\`\\\` By default, NumPy copies data to a new array. This can be prevented by creating an array view: \\\`\\\`\\\`python a = mx.arange(3) a\\\_view = np.array(a, copy=False) print(a\\\_view.flags.owndata) # False a\\\_view\\\[0\\\] = 1 print(a\\\[0\\\].item()) # 1 \\\`\\\`\\\` Note NumPy arrays with type \\\`\\\\\\\`float64\\\\\\\`\\\` will be default converted to MLX arrays with type \\\`\\\\\\\`float32\\\\\\\`\\\`. A NumPy array view is a normal NumPy array, except that it does not own its memory. This means writing to the view is reflected in the original array. While this is quite powerful to prevent copying arrays, it should be noted that external changes to the memory of arrays cannot be reflected in gradients. Let’s demonstrate this in an example: \\\`\\\`\\\`python def f(x): x\\\_view = np.array(x, copy=False) x\\\_view\\\[:\\\] \\\*= x\\\_view # modify memory without telling mx return x.sum() x = mx.array(\\\[3.0\\\]) y, df = mx.value\\\_and\\\_grad(f)(x) print("f(x) = x² =", y.item()) # 9.0 print("f'(x) = 2x !=", df.item()) # 1.0 \\\`\\\`\\\` The function \\\`\\\\\\\`f\\\\\\\`\\\` indirectly modifies the array \\\`\\\\\\\`x\\\\\\\`\\\` through a memory view. However, this modification is not reflected in the gradient, as seen in the last line outputting \\\`\\\\\\\`1.0\\\\\\\`\\\`, representing the gradient of the sum operation alone. The squaring of \\\`\\\\\\\`x\\\\\\\`\\\` occurs externally to MLX, meaning that no gradient is incorporated. It’s important to note that a similar issue arises during array conversion and copying. For instance, a function defined as \\\`\\\\\\\`mx.array(np.array(x)\\\\\\\*\\\\\\\*2).sum()\\\\\\\`\\\` would also result in an incorrect gradient, even though no in-place operations on MLX memory are executed. ## PyTorch Warning PyTorch Support for \\\[\\\`\\\`memoryview\\\`\\\`\\\](https://docs.python.org/3/library/stdtypes.html#memoryview "(in Python v3.13)") is experimental and can break for multi-dimensional arrays. Casting to NumPy first is advised for now. PyTorch supports the buffer protocol, but it requires an explicit \\\[\\\`\\\`memoryview\\\`\\\`\\\](https://docs.python.org/3/library/stdtypes.html#memoryview "(in Python v3.13)"). \\\`\\\`\\\`python import mlx.core as mx import torch a = mx.arange(3) b = torch.tensor(memoryview(a)) c = mx.array(b.numpy()) \\\`\\\`\\\` Conversion from PyTorch tensors back to arrays must be done via intermediate NumPy arrays with \\\`\\\\\\\`numpy()\\\\\\\`\\\`. ## JAX JAX fully supports the buffer protocol. \\\`\\\`\\\`python import mlx.core as mx import jax.numpy as jnp a = mx.arange(3) b = jnp.array(a) c = mx.array(b) \\\`\\\`\\\` ## TensorFlow TensorFlow supports the buffer protocol, but it requires an explicit \\\[\\\`\\\`memoryview\\\`\\\`\\\](https://docs.python.org/3/library/stdtypes.html#memoryview "(in Python v3.13)"). \\\`\\\`\\\`python import mlx.core as mx import tensorflow as tf a = mx.arange(3) b = tf.constant(memoryview(a)) c = mx.array(b) \\\`\\\`\\\` \\\[\\\](https://ml-explore.github.io/mlx/build/html/usage/compile.html "previous page") previous Compilation \\\[\\\](https://ml-explore.github.io/mlx/build/html/usage/distributed.html "next page") next Distributed Communication Contents \\\\- \\\[PyTorch\\\](https://ml-explore.github.io/mlx/build/html/#pytorch) - \\\[JAX\\\](https://ml-explore.github.io/mlx/build/html/#jax) - \\\[TensorFlow\\\](https://ml-explore.github.io/mlx/build/html/#tensorflow) By MLX Contributors © Copyright 2023, MLX Contributors.
