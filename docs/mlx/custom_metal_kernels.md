Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\\## Curated Notes - Start with a CPU reference and assert equality before moving work to a Metal kernel. - Use \\\\\\\`mx.metal.start\\\\\\\_capture()\\\\\\\` / \\\\\\\`mx.metal.stop\\\\\\\_capture()\\\\\\\` to collect command buffers for inspection in Xcode. - Respect array strides: many views are non‑contiguous; either operate with strides or call \\\\\\\`mx.contiguous\\\\\\\` upfront. - Document kernel preconditions (dtype, layout) and validate them in a Python wrapper before dispatch. ## Patterns and Examples ### Simple Elementwise Kernel (row‑contiguous) \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx def exp\\\\\\\_elementwise(a: mx.array): src = """ uint elem = thread\\\\\\\_position\\\\\\\_in\\\\\\\_grid.x; T tmp = inp\\\\\\\[elem\\\\\\\]; out\\\\\\\[elem\\\\\\\] = metal::exp(tmp); """ kernel = mx.fast.metal\\\\\\\_kernel( name="myexp", input\\\\\\\_names=\\\\\\\["inp"\\\\\\\], output\\\\\\\_names=\\\\\\\["out"\\\\\\\], source=src, ) out = kernel( inputs=\\\\\\\[a\\\\\\\], template=\\\\\\\[("T", a.dtype)\\\\\\\], # dtype template grid=(a.size, 1, 1), threadgroup=(256, 1, 1), output\\\\\\\_shapes=\\\\\\\[a.shape\\\\\\\], output\\\\\\\_dtypes=\\\\\\\[a.dtype\\\\\\\], )\\\\\\\[0\\\\\\\] return out a = mx.random.normal((4, 16)).astype(mx.float16) b = exp\\\\\\\_elementwise(a) assert mx.allclose(b, mx.exp(a)) \\\\\\\`\\\\\\\`\\\\\\\` Notes: - Pass only the kernel body in \\\\\\\`source\\\\\\\`; MLX generates the full signature from inputs/outputs and template params. - Use \\\\\\\`template=\\\\\\\[("T", ...)\\\\\\\]\\\\\\\` to parameterize dtype/ints/bools. - Tune \\\\\\\`grid\\\\\\\` and \\\\\\\`threadgroup\\\\\\\` (threads launched = prod(grid)). - Add \\\\\\\`verbose=True\\\\\\\` to the kernel call to print the generated code. ### Strided Inputs (no row‑contiguous copy) \\\\\\\`\\\\\\\`\\\\\\\`python def exp\\\\\\\_elementwise\\\\\\\_strided(a: mx.array): src = """ uint elem = thread\\\\\\\_position\\\\\\\_in\\\\\\\_grid.x; // utils.h helpers are available automatically uint loc = elem\\\\\\\_to\\\\\\\_loc(elem, inp\\\\\\\_shape, inp\\\\\\\_strides, inp\\\\\\\_ndim); T tmp = inp\\\\\\\[loc\\\\\\\]; // outputs are row contiguous out\\\\\\\[elem\\\\\\\] = metal::exp(tmp); """ k = mx.fast.metal\\\\\\\_kernel( name="myexp\\\\\\\_strided", input\\\\\\\_names=\\\\\\\["inp"\\\\\\\], output\\\\\\\_names=\\\\\\\["out"\\\\\\\], source=src ) return k( inputs=\\\\\\\[a\\\\\\\], template=\\\\\\\[("T", a.dtype)\\\\\\\], grid=(a.size, 1, 1), threadgroup=(256, 1, 1), output\\\\\\\_shapes=\\\\\\\[a.shape\\\\\\\], output\\\\\\\_dtypes=\\\\\\\[a.dtype\\\\\\\], ensure\\\\\\\_row\\\\\\\_contiguous=False, # opt‑out of input copy )\\\\\\\[0\\\\\\\] x = mx.random.normal((4, 16)).astype(mx.float16)\\\\\\\[::2\\\\\\\] # make non‑contiguous y = exp\\\\\\\_elementwise\\\\\\\_strided(x) assert mx.allclose(y, mx.exp(x)) \\\\\\\`\\\\\\\`\\\\\\\` ### Custom Function + VJP (backprop) Sketch \\\\\\\`\\\\\\\`\\\\\\\`python @mx.custom\\\\\\\_function def myop(x): # forward via metal kernel (like above) ... return y @myop.vjp def myop\\\\\\\_vjp(primals, cotangent, aux): x, = primals # fused backward via metal kernel k = mx.fast.metal\\\\\\\_kernel( name="myop\\\\\\\_grad", input\\\\\\\_names=\\\\\\\["x", "cotangent"\\\\\\\], output\\\\\\\_names=\\\\\\\["x\\\\\\\_grad"\\\\\\\], source="""/\\\\\\\* atomic updates to x\\\\\\\_grad \\\\\\\*/""", atomic\\\\\\\_outputs=True, ) xg = k( inputs=\\\\\\\[x, cotangent\\\\\\\], template=\\\\\\\[("T", x.dtype)\\\\\\\], output\\\\\\\_shapes=\\\\\\\[x.shape\\\\\\\], output\\\\\\\_dtypes=\\\\\\\[x.dtype\\\\\\\], grid=(x.size, 1, 1), threadgroup=(256, 1, 1), init\\\\\\\_value=0, )\\\\\\\[0\\\\\\\] return (xg,) \\\\\\\`\\\\\\\`\\\\\\\` Tips: - \\\\\\\`atomic\\\\\\\_outputs=True\\\\\\\` and \\\\\\\`init\\\\\\\_value=0\\\\\\\` enable safe accumulation across threads. - Consider padding channels to simd‑group multiples when doing simd reductions. ### Tiled/Block Cholesky (pattern) Block‑based factorizations are a good fit for GPU when you keep the numerics stable. A practical approach is: 1) Provide a numerically safe single‑thread kernel for the diagonal block (or run diagonal updates on CPU), then 2) Use multiple threads to update trailing blocks, with threadgroup barriers to synchronize phases. \\\\\\\`\\\\\\\`\\\\\\\`python # Header (math helpers) hdr = """ #include #include using namespace metal; """ src = """ // thread and problem sizing uint tid = thread\\\\\\\_position\\\\\\\_in\\\\\\\_grid.x; uint n = A\\\\\\\_shape\\\\\\\[0\\\\\\\]; uint blk = block\\\\\\\_param\\\\\\\[0\\\\\\\]; // block size uint nblk = (n + blk - 1) / blk; uint nthreads = thread\\\\\\\_count\\\\\\\[0\\\\\\\]; for (uint k = 0; k < nblk; ++k) { uint b0 = k \\\\\\\* blk, b1 = min(b0 + blk, n); // 1) diagonal block with single thread for stability if (tid == 0) { for (uint j = b0; j < b1; ++j) { float s = 0.0f; for (uint p = 0; p < j; ++p) s += out\\\\\\\[j\\\\\\\*n + p\\\\\\\] \\\\\\\* out\\\\\\\[j\\\\\\\*n + p\\\\\\\]; float d = A\\\\\\\[j\\\\\\\*n + j\\\\\\\] - s; d = d <= 1e-10f ? 1e-10f : d; out\\\\\\\[j\\\\\\\*n + j\\\\\\\] = sqrt(d); for (uint i = j+1; i < b1; ++i) { float acc = 0.0f; for (uint p = 0; p < j; ++p) acc += out\\\\\\\[i\\\\\\\*n + p\\\\\\\] \\\\\\\* out\\\\\\\[j\\\\\\\*n + p\\\\\\\]; float denom = out\\\\\\\[j\\\\\\\*n + j\\\\\\\]; out\\\\\\\[i\\\\\\\*n + j\\\\\\\] = denom > 1e-10f ? (A\\\\\\\[i\\\\\\\*n + j\\\\\\\] - acc) / denom : 0.0f; } } } threadgroup\\\\\\\_barrier(mem\\\\\\\_flags::mem\\\\\\\_device); // 2) trailing updates: distribute rows across threads for (uint row = tid; row < n; row += nthreads) { if (row >= b1) { for (uint j = b0; j < b1; ++j) { float acc = 0.0f; for (uint p = 0; p < j; ++p) acc += out\\\\\\\[row\\\\\\\*n + p\\\\\\\] \\\\\\\* out\\\\\\\[j\\\\\\\*n + p\\\\\\\]; float denom = out\\\\\\\[j\\\\\\\*n + j\\\\\\\]; out\\\\\\\[row\\\\\\\*n + j\\\\\\\] = denom > 1e-10f ? (A\\\\\\\[row\\\\\\\*n + j\\\\\\\] - acc) / denom : 0.0f; } } } threadgroup\\\\\\\_barrier(mem\\\\\\\_flags::mem\\\\\\\_device); } """ kernel = mx.fast.metal\\\\\\\_kernel( name="block\\\\\\\_cholesky\\\\\\\_kernel", input\\\\\\\_names=\\\\\\\["A", "block\\\\\\\_param", "thread\\\\\\\_count"\\\\\\\], output\\\\\\\_names=\\\\\\\["out"\\\\\\\], source=src, header=hdr, ensure\\\\\\\_row\\\\\\\_contiguous=True, ) def block\\\\\\\_cholesky(A, block\\\\\\\_size=16): nthreads = min(32, A.shape\\\\\\\[0\\\\\\\]) return kernel( inputs=\\\\\\\[A, mx.array(\\\\\\\[block\\\\\\\_size\\\\\\\], dtype=mx.uint32), mx.array(\\\\\\\[nthreads\\\\\\\], dtype=mx.uint32)\\\\\\\], output\\\\\\\_shapes=\\\\\\\[A.shape\\\\\\\], output\\\\\\\_dtypes=\\\\\\\[A.dtype\\\\\\\], grid=(nthreads,1,1), threadgroup=(nthreads,1,1), )\\\\\\\[0\\\\\\\] \\\\\\\`\\\\\\\`\\\\\\\` Notes: - Use a small block size and limited threads for stability; benchmark for your matrices. - Verify correctness via \\\\\\\`mx.allclose(A, L @ L.T)\\\\\\\` and fall back to CPU for edge cases. ### Panel/Block QR (sketch) - Pattern: build Householder reflectors for a panel (single thread or warp‑sized group for stability), then tile the trailing matrix update across threads; sync with \\\\\\\`threadgroup\\\\\\\_barrier\\\\\\\`. - Guards: cap panel width, reject tiny norms (add jitter), check for NaN/Inf, optionally re‑orthogonalize. \\\\\\\`\\\\\\\`\\\\\\\`python hdr = """ #include using namespace metal; """ src = R"METAL( uint tid = thread\\\\\\\_position\\\\\\\_in\\\\\\\_grid.x; uint m = A\\\\\\\_shape\\\\\\\[0\\\\\\\]; uint n = A\\\\\\\_shape\\\\\\\[1\\\\\\\]; uint bw = panel\\\\\\\_shape\\\\\\\[0\\\\\\\]; // panel width for (uint k = 0; k < min(m, n); k += bw) { uint k1 = min(k + bw, n); // 1) form Householder vectors for columns k..k1-1 (single thread for stability) if (tid == 0) { // compute v, tau for each column; apply to panel // add small jitter if column norm is tiny to avoid breakdown } threadgroup\\\\\\\_barrier(mem\\\\\\\_flags::mem\\\\\\\_device); // 2) apply block of reflectors to trailing matrix in parallel for (uint col = k1 + tid; col < n; col += grid\\\\\\\_size.x) { // compute W = V^T \\\\\\\* A\\\\\\\[:, col\\\\\\\]; A\\\\\\\[:, col\\\\\\\] -= V \\\\\\\* (tau .\\\\\\\* W) } threadgroup\\\\\\\_barrier(mem\\\\\\\_flags::mem\\\\\\\_device); } )METAL"; kernel = mx.fast.metal\\\\\\\_kernel( name="panel\\\\\\\_qr\\\\\\\_kernel", input\\\\\\\_names=\\\\\\\["A", "panel"\\\\\\\], output\\\\\\\_names=\\\\\\\["Q", "R"\\\\\\\], source=src, header=hdr, ensure\\\\\\\_row\\\\\\\_contiguous=True, ) \\\\\\\`\\\\\\\`\\\\\\\` Tips: - Keep numerical work in stable scalar loops; parallelize level‑3 style updates. - Validate with \\\\\\\`||I - Q^T Q||\\\\\\\` and lower‑triangular residual on R; fall back to CPU on failure. - Use HPC16x8 limb accumulation for dot products in challenging cases. ### Debug and Safety Flags (pattern) - Maintain a small \\\\\\\`dbg\\\\\\\` output buffer to signal health and capture metrics without crashing the kernel. - Example mapping (adapt to your kernel): - \\\\\\\`dbg\\\\\\\[0\\\\\\\]\\\\\\\`: start flag (1.0 if preflight checks passed) - \\\\\\\`dbg\\\\\\\[1..3\\\\\\\]\\\\\\\`: matrix dims \\\\\\\`m, n, min(m,n)\\\\\\\` - \\\\\\\`dbg\\\\\\\[5..7\\\\\\\]\\\\\\\`: threads per group, total threads, work per thread - \\\\\\\`dbg\\\\\\\[8..11\\\\\\\]\\\\\\\`: intermediate values (e.g., sigma, norms, vTv) - \\\\\\\`dbg\\\\\\\[12\\\\\\\]\\\\\\\`: iteration count - \\\\\\\`dbg\\\\\\\[13\\\\\\\]\\\\\\\`: error code (1=matrix too large, 2=work too large, 3=too many iterations, 4/5=numerical instability) - \\\\\\\`dbg\\\\\\\[14\\\\\\\]\\\\\\\`: error value (context) - \\\\\\\`dbg\\\\\\\[15\\\\\\\]\\\\\\\`: success flag set at the end if all checks passed Pattern: - Perform preflight checks (dims, workload), set \\\\\\\`dbg\\\\\\\[0\\\\\\\]\\\\\\\`, and early‑return on failure. - Guard inner computations; if NaN/Inf, set \\\\\\\`dbg\\\\\\\[13\\\\\\\]\\\\\\\`/\\\\\\\`dbg\\\\\\\[14\\\\\\\]\\\\\\\` and exit cleanly. - Prefer to return a flagged \\\\\\\`dbg\\\\\\\` rather than crashing; host code can fallback to CPU. ### Apple GPU sizing (cheat sheet) - Execution width: 32; choose threadgroup sizes as multiples of 32. - Max threads per threadgroup: 1024 (cap here or lower). - Tiny matrices (≤8): use one 32‑thread group. - Small (≤32): up to one thread per element, cap around 256. - Larger: cap total launched threads ≤ 1024; round to execution width. ### Wrapper with timeout (reusable) \\\\\\\`\\\\\\\`\\\\\\\`python import signal def launch\\\\\\\_with\\\\\\\_timeout(callable\\\\\\\_kernel, args, kwargs=None, timeout\\\\\\\_seconds=1.0): kwargs = {} if kwargs is None else kwargs def handler(signum, frame): raise TimeoutError(f"Kernel timed out after {timeout\\\\\\\_seconds}s") orig = signal.getsignal(signal.SIGALRM) signal.signal(signal.SIGALRM, handler) signal.alarm(int(timeout\\\\\\\_seconds)) try: return callable\\\\\\\_kernel(\\\\\\\*args, \\\\\\\*\\\\\\\*kwargs) finally: signal.alarm(0) signal.signal(signal.SIGALRM, orig) \\\\\\\`\\\\\\\`\\\\\\\` \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/dev/custom\\\_metal\\\_kernels.rst "Download source file") - .pdf # Custom Metal Kernels ## Contents \\\\- \\\[Simple Example\\\](https://ml-explore.github.io/mlx/build/html/#simple-example) - \\\[Using Shape/Strides\\\](https://ml-explore.github.io/mlx/build/html/#using-shape-strides) - \\\[Complex Example\\\](https://ml-explore.github.io/mlx/build/html/#complex-example) - \\\[Grid Sample VJP\\\](https://ml-explore.github.io/mlx/build/html/#grid-sample-vjp) # Custom Metal Kernels MLX supports writing custom Metal kernels through the Python and C++ APIs. ## Simple Example Let’s write a custom kernel that computes \\\`\\\\\\\`exp\\\\\\\`\\\` elementwise: \\\`\\\`\\\`python def exp\\\_elementwise(a: mx.array): source = """ uint elem = thread\\\_position\\\_in\\\_grid.x; T tmp = inp\\\[elem\\\]; out\\\[elem\\\] = metal::exp(tmp); """ kernel = mx.fast.metal\\\_kernel( name="myexp", input\\\_names=\\\["inp"\\\], output\\\_names=\\\["out"\\\], source=source, ) outputs = kernel( inputs=\\\[a\\\], template=\\\[("T", mx.float32)\\\], grid=(a.size, 1, 1), threadgroup=(256, 1, 1), output\\\_shapes=\\\[a.shape\\\], output\\\_dtypes=\\\[a.dtype\\\], ) return outputs\\\[0\\\] a = mx.random.normal(shape=(4, 16)).astype(mx.float16) b = exp\\\_elementwise(a) assert mx.allclose(b, mx.exp(a)) \\\`\\\`\\\` Note We are only required to pass the body of the Metal kernel in \\\`\\\\\\\`source\\\\\\\`\\\`. The full function signature will be generated using: - The shapes/dtypes of \\\`\\\\\\\`inputs\\\\\\\`\\\` In the above, \\\`\\\\\\\`a\\\\\\\`\\\` is an \\\`\\\\\\\`mx.array\\\\\\\`\\\` of type \\\`\\\\\\\`mx.float16\\\\\\\`\\\` and we pass it with the key \\\`\\\\\\\`inp\\\\\\\`\\\` so we will add \\\`\\\\\\\`const\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`device\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`float16\\\\\\\_t\\\\\\\*\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`inp\\\\\\\`\\\` to the signature. \\\`\\\\\\\`inp\\\\\\\_shape\\\\\\\`\\\`, \\\`\\\\\\\`inp\\\\\\\_strides\\\\\\\`\\\` and \\\`\\\\\\\`inp\\\\\\\_ndim\\\\\\\`\\\` are also added for convenience if they are present in \\\`\\\\\\\`source\\\\\\\`\\\`. - The list of \\\`\\\\\\\`output\\\\\\\_dtypes\\\\\\\`\\\` In the above, \\\`\\\\\\\`out\\\\\\\`\\\` is an \\\`\\\\\\\`mx.array\\\\\\\`\\\` of type \\\`\\\\\\\`mx.float16\\\\\\\`\\\` so we add \\\`\\\\\\\`device\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`float16\\\\\\\_t\\\\\\\*\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`out\\\\\\\`\\\`. - Template parameters passed using \\\`\\\\\\\`template\\\\\\\`\\\` In the above, \\\`\\\\\\\`template=\\\\\\\[("T",\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`mx.float32)\\\\\\\]\\\\\\\`\\\` adds a template of \\\`\\\\\\\`template\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`T>\\\\\\\`\\\` to the function and instantiates the template with \\\`\\\\\\\`custom\\\\\\\_kernel\\\\\\\_myexp\\\\\\\_float\\\\\\\`\\\`. Template parameters can be \\\`\\\\\\\`mx.core.Dtype\\\\\\\`\\\`, \\\`\\\\\\\`int\\\\\\\`\\\` or \\\`\\\\\\\`bool\\\\\\\`\\\`. - Metal attributes used in \\\`\\\\\\\`source\\\\\\\`\\\` such as \\\`\\\\\\\`\\\\\\\[\\\\\\\[thread\\\\\\\_position\\\\\\\_in\\\\\\\_grid\\\\\\\]\\\\\\\]\\\\\\\`\\\` These will be added as function arguments. All the attributes defined in Table 5.8 of the \\\[Metal Shading Language Specification\\\](https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf) are supported. Putting this all together, the generated function signature for \\\`\\\\\\\`myexp\\\\\\\`\\\` is as follows: \\\`\\\`\\\`cpp template \\\[\\\[kernel\\\]\\\] void custom\\\_kernel\\\_myexp\\\_float( const device float16\\\_t\\\* inp \\\[\\\[buffer(0)\\\]\\\], device float16\\\_t\\\* out \\\[\\\[buffer(1)\\\]\\\], uint3 thread\\\_position\\\_in\\\_grid \\\[\\\[thread\\\_position\\\_in\\\_grid\\\]\\\]) { uint elem = thread\\\_position\\\_in\\\_grid.x; T tmp = inp\\\[elem\\\]; out\\\[elem\\\] = metal::exp(tmp); } template \\\[\\\[host\\\_name("custom\\\_kernel\\\_myexp\\\_float")\\\]\\\] \\\[\\\[kernel\\\]\\\] decltype(custom\\\_kernel\\\_myexp\\\_float) custom\\\_kernel\\\_myexp\\\_float; \\\`\\\`\\\` Note: \\\`\\\\\\\`grid\\\\\\\`\\\` and \\\`\\\\\\\`threadgroup\\\\\\\`\\\` are parameters to the Metal \\\[dispatchThreads\\\](https://developer.apple.com/documentation/metal/mtlcomputecommandencoder/2866532-dispatchthreads) function. This means we will launch \\\`\\\\\\\`mx.prod(grid)\\\\\\\`\\\` threads, subdivided into \\\`\\\\\\\`threadgroup\\\\\\\`\\\` size threadgroups. For optimal performance, each thread group dimension should be less than or equal to the corresponding grid dimension. Passing \\\`\\\\\\\`verbose=True\\\\\\\`\\\` to \\\`\\\\\\\`mx.fast.metal\\\\\\\_kernel.\\\\\\\_\\\\\\\_call\\\\\\\_\\\\\\\_\\\\\\\`\\\` will print the generated code for debugging purposes.\\\` ## Using Shape/Strides \\\`\\\\\\\`mx.fast.metal\\\\\\\_kernel\\\\\\\`\\\` supports an argument \\\`\\\\\\\`ensure\\\\\\\_row\\\\\\\_contiguous\\\\\\\`\\\` which is \\\`\\\\\\\`True\\\\\\\`\\\` by default. This will copy the \\\`\\\\\\\`mx.array\\\\\\\`\\\` inputs if needed before the kernel is launched to ensure that the memory layout is row contiguous. Generally this makes writing the kernel easier, since we don’t have to worry about gaps or the ordering of the dims when indexing. If we want to avoid this copy, \\\`\\\\\\\`metal\\\\\\\_kernel\\\\\\\`\\\` automatically passes \\\`\\\\\\\`a\\\\\\\_shape\\\\\\\`\\\`, \\\`\\\\\\\`a\\\\\\\_strides\\\\\\\`\\\` and \\\`\\\\\\\`a\\\\\\\_ndim\\\\\\\`\\\` for each input array \\\`\\\\\\\`a\\\\\\\`\\\` if any are present in \\\`\\\\\\\`source\\\\\\\`\\\`. We can then use MLX’s built in indexing utils to fetch the right elements for each thread. Let’s convert \\\`\\\\\\\`myexp\\\\\\\`\\\` above to support arbitrarily strided arrays without relying on a copy from \\\`\\\\\\\`ensure\\\\\\\_row\\\\\\\_contiguous\\\\\\\`\\\`: \\\`\\\`\\\`python def exp\\\_elementwise(a: mx.array): source = """ uint elem = thread\\\_position\\\_in\\\_grid.x; // Utils from \\\`mlx/backend/metal/kernels/utils.h\\\` are automatically included uint loc = elem\\\_to\\\_loc(elem, inp\\\_shape, inp\\\_strides, inp\\\_ndim); T tmp = inp\\\[loc\\\]; // Output arrays are always row contiguous out\\\[elem\\\] = metal::exp(tmp); """ kernel = mx.fast.metal\\\_kernel( name="myexp\\\_strided", input\\\_names=\\\["inp"\\\], output\\\_names=\\\["out"\\\], source=source ) outputs = kernel( inputs=\\\[a\\\], template=\\\[("T", mx.float32)\\\], grid=(a.size, 1, 1), threadgroup=(256, 1, 1), output\\\_shapes=\\\[a.shape\\\], output\\\_dtypes=\\\[a.dtype\\\], ensure\\\_row\\\_contiguous=False, ) return outputs\\\[0\\\] a = mx.random.normal(shape=(4, 16)).astype(mx.float16) # make non-contiguous a = a\\\[::2\\\] b = exp\\\_elementwise(a) assert mx.allclose(b, mx.exp(a)) \\\`\\\`\\\` ## Complex Example Let’s implement a more complex example: \\\`\\\\\\\`grid\\\\\\\_sample\\\\\\\`\\\` in \\\`\\\\\\\`"bilinear"\\\\\\\`\\\` mode. We’ll start with the following MLX implementation using standard ops: \\\`\\\`\\\`python def grid\\\_sample\\\_ref(x, grid): N, H\\\_in, W\\\_in, \\\_ = x.shape ix = ((grid\\\[..., 0\\\] + 1) \\\* W\\\_in - 1) / 2 iy = ((grid\\\[..., 1\\\] + 1) \\\* H\\\_in - 1) / 2 ix\\\_nw = mx.floor(ix).astype(mx.int32) iy\\\_nw = mx.floor(iy).astype(mx.int32) ix\\\_ne = ix\\\_nw + 1 iy\\\_ne = iy\\\_nw ix\\\_sw = ix\\\_nw iy\\\_sw = iy\\\_nw + 1 ix\\\_se = ix\\\_nw + 1 iy\\\_se = iy\\\_nw + 1 nw = (ix\\\_se - ix) \\\* (iy\\\_se - iy) ne = (ix - ix\\\_sw) \\\* (iy\\\_sw - iy) sw = (ix\\\_ne - ix) \\\* (iy - iy\\\_ne) se = (ix - ix\\\_nw) \\\* (iy - iy\\\_nw) I\\\_nw = x\\\[mx.arange(N)\\\[:, None, None\\\], iy\\\_nw, ix\\\_nw, :\\\] I\\\_ne = x\\\[mx.arange(N)\\\[:, None, None\\\], iy\\\_ne, ix\\\_ne, :\\\] I\\\_sw = x\\\[mx.arange(N)\\\[:, None, None\\\], iy\\\_sw, ix\\\_sw, :\\\] I\\\_se = x\\\[mx.arange(N)\\\[:, None, None\\\], iy\\\_se, ix\\\_se, :\\\] mask\\\_nw = (iy\\\_nw >= 0) & (iy\\\_nw = 0) & (ix\\\_nw = 0) & (iy\\\_ne = 0) & (ix\\\_ne = 0) & (iy\\\_sw = 0) & (ix\\\_sw = 0) & (iy\\\_se = 0) & (ix\\\_se <= W\\\_in - 1) I\\\_nw \\\*= mask\\\_nw\\\[..., None\\\] I\\\_ne \\\*= mask\\\_ne\\\[..., None\\\] I\\\_sw \\\*= mask\\\_sw\\\[..., None\\\] I\\\_se \\\*= mask\\\_se\\\[..., None\\\] output = nw\\\[..., None\\\] \\\* I\\\_nw + ne\\\[..., None\\\] \\\* I\\\_ne + sw\\\[..., None\\\] \\\* I\\\_sw + se\\\[..., None\\\] \\\* I\\\_se return output \\\`\\\`\\\` Now let’s use \\\`\\\\\\\`mx.custom\\\\\\\_function\\\\\\\`\\\` together with \\\`\\\\\\\`mx.fast.metal\\\\\\\_kernel\\\\\\\`\\\` to write a fast GPU kernel for both the forward and backward passes. First we’ll implement the forward pass as a fused kernel: \\\`\\\`\\\`python @mx.custom\\\_function def grid\\\_sample(x, grid): assert x.ndim == 4, "\\\`x\\\` must be 4D." assert grid.ndim == 4, "\\\`grid\\\` must be 4D." B, \\\_, \\\_, C = x.shape \\\_, gN, gM, D = grid.shape out\\\_shape = (B, gN, gM, C) assert D == 2, "Last dim of \\\`grid\\\` must be size 2." source = """ uint elem = thread\\\_position\\\_in\\\_grid.x; int H = x\\\_shape\\\[1\\\]; int W = x\\\_shape\\\[2\\\]; int C = x\\\_shape\\\[3\\\]; int gH = grid\\\_shape\\\[1\\\]; int gW = grid\\\_shape\\\[2\\\]; int w\\\_stride = C; int h\\\_stride = W \\\* w\\\_stride; int b\\\_stride = H \\\* h\\\_stride; uint grid\\\_idx = elem / C \\\* 2; float ix = ((grid\\\[grid\\\_idx\\\] + 1) \\\* W - 1) / 2; float iy = ((grid\\\[grid\\\_idx + 1\\\] + 1) \\\* H - 1) / 2; int ix\\\_nw = floor(ix); int iy\\\_nw = floor(iy); int ix\\\_ne = ix\\\_nw + 1; int iy\\\_ne = iy\\\_nw; int ix\\\_sw = ix\\\_nw; int iy\\\_sw = iy\\\_nw + 1; int ix\\\_se = ix\\\_nw + 1; int iy\\\_se = iy\\\_nw + 1; T nw = (ix\\\_se - ix) \\\* (iy\\\_se - iy); T ne = (ix - ix\\\_sw) \\\* (iy\\\_sw - iy); T sw = (ix\\\_ne - ix) \\\* (iy - iy\\\_ne); T se = (ix - ix\\\_nw) \\\* (iy - iy\\\_nw); int batch\\\_idx = elem / C / gH / gW \\\* b\\\_stride; int channel\\\_idx = elem % C; int base\\\_idx = batch\\\_idx + channel\\\_idx; T I\\\_nw = x\\\[base\\\_idx + iy\\\_nw \\\* h\\\_stride + ix\\\_nw \\\* w\\\_stride\\\]; T I\\\_ne = x\\\[base\\\_idx + iy\\\_ne \\\* h\\\_stride + ix\\\_ne \\\* w\\\_stride\\\]; T I\\\_sw = x\\\[base\\\_idx + iy\\\_sw \\\* h\\\_stride + ix\\\_sw \\\* w\\\_stride\\\]; T I\\\_se = x\\\[base\\\_idx + iy\\\_se \\\* h\\\_stride + ix\\\_se \\\* w\\\_stride\\\]; I\\\_nw = iy\\\_nw >= 0 && iy\\\_nw = 0 && ix\\\_nw = 0 && iy\\\_ne = 0 && ix\\\_ne = 0 && iy\\\_sw = 0 && ix\\\_sw = 0 && iy\\\_se = 0 && ix\\\_se <= W - 1 ? I\\\_se : 0; out\\\[elem\\\] = nw \\\* I\\\_nw + ne \\\* I\\\_ne + sw \\\* I\\\_sw + se \\\* I\\\_se; """ kernel = mx.fast.metal\\\_kernel( name="grid\\\_sample", input\\\_names=\\\["x", "grid"\\\], output\\\_names=\\\["out"\\\], source=source, ) outputs = kernel( inputs=\\\[x, grid\\\], template=\\\[("T", x.dtype)\\\], output\\\_shapes=\\\[out\\\_shape\\\], output\\\_dtypes=\\\[x.dtype\\\], grid=(np.prod(out\\\_shape), 1, 1), threadgroup=(256, 1, 1), ) return outputs\\\[0\\\] \\\`\\\`\\\` For a reasonably sized input such as: \\\`\\\`\\\`python x.shape = (8, 1024, 1024, 64) grid.shape = (8, 256, 256, 2) \\\`\\\`\\\` On an M1 Max, we see a big performance improvement: \\\`\\\\\\\`55.7ms\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`->\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`6.7ms\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`=>\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`8x\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`speed\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`up\\\\\\\`\\\` ## Grid Sample VJP Since we decorated \\\`\\\\\\\`grid\\\\\\\_sample\\\\\\\`\\\` with \\\`\\\\\\\`mx.custom\\\\\\\_function\\\\\\\`\\\`, we can now define its custom vjp transform so MLX can differentiate it. The backwards pass requires atomically updating \\\`\\\\\\\`x\\\\\\\_grad\\\\\\\`\\\`/\\\`\\\\\\\`grid\\\\\\\_grad\\\\\\\`\\\` and so requires a few extra \\\`\\\\\\\`mx.fast.metal\\\\\\\_kernel\\\\\\\`\\\` features: - \\\`\\\\\\\`init\\\\\\\_value=0\\\\\\\`\\\` Initialize all of the kernel’s outputs to this value before it runs. This allows us to update only part of the output arrays with the kernel. - \\\`\\\\\\\`atomic\\\\\\\_outputs=True\\\\\\\`\\\` Designate all of the kernel outputs as \\\`\\\\\\\`atomic\\\\\\\`\\\` in the function signature. This means we can use Metal’s \\\`\\\\\\\`atomic\\\\\\\`\\\` features to simultaneously update the \\\`\\\\\\\`x\\\\\\\_grad\\\\\\\`\\\` and \\\`\\\\\\\`grid\\\\\\\_grad\\\\\\\`\\\` arrays from multiple threadgroups. See section 6.15 of the \\\[Metal Shading Language Specification\\\](https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf) for more details. We can then implement the backwards pass as follows: \\\`\\\`\\\`python @grid\\\_sample.vjp def grid\\\_sample\\\_vjp(primals, cotangent, \\\_): x, grid = primals B, \\\_, \\\_, C = x.shape \\\_, gN, gM, D = grid.shape assert D == 2, "Last dim of \\\`grid\\\` must be size 2." source = """ uint elem = thread\\\_position\\\_in\\\_grid.x; int H = x\\\_shape\\\[1\\\]; int W = x\\\_shape\\\[2\\\]; int C = x\\\_shape\\\[3\\\]; // Pad C to the nearest larger simdgroup size multiple int C\\\_padded = ceildiv(C, threads\\\_per\\\_simdgroup) \\\* threads\\\_per\\\_simdgroup; int gH = grid\\\_shape\\\[1\\\]; int gW = grid\\\_shape\\\[2\\\]; int w\\\_stride = C; int h\\\_stride = W \\\* w\\\_stride; int b\\\_stride = H \\\* h\\\_stride; uint grid\\\_idx = elem / C\\\_padded \\\* 2; float ix = ((grid\\\[grid\\\_idx\\\] + 1) \\\* W - 1) / 2; float iy = ((grid\\\[grid\\\_idx + 1\\\] + 1) \\\* H - 1) / 2; int ix\\\_nw = floor(ix); int iy\\\_nw = floor(iy); int ix\\\_ne = ix\\\_nw + 1; int iy\\\_ne = iy\\\_nw; int ix\\\_sw = ix\\\_nw; int iy\\\_sw = iy\\\_nw + 1; int ix\\\_se = ix\\\_nw + 1; int iy\\\_se = iy\\\_nw + 1; T nw = (ix\\\_se - ix) \\\* (iy\\\_se - iy); T ne = (ix - ix\\\_sw) \\\* (iy\\\_sw - iy); T sw = (ix\\\_ne - ix) \\\* (iy - iy\\\_ne); T se = (ix - ix\\\_nw) \\\* (iy - iy\\\_nw); int batch\\\_idx = elem / C\\\_padded / gH / gW \\\* b\\\_stride; int channel\\\_idx = elem % C\\\_padded; int base\\\_idx = batch\\\_idx + channel\\\_idx; T gix = T(0); T giy = T(0); if (channel\\\_idx = 0 && iy\\\_nw = 0 && ix\\\_nw = 0 && iy\\\_ne = 0 && ix\\\_ne = 0 && iy\\\_sw = 0 && ix\\\_sw = 0 && iy\\\_se = 0 && ix\\\_se <= W - 1) { int offset = base\\\_idx + iy\\\_se \\\* h\\\_stride + ix\\\_se \\\* w\\\_stride; atomic\\\_fetch\\\_add\\\_explicit(&x\\\_grad\\\[offset\\\], se \\\* cot, memory\\\_order\\\_relaxed); T I\\\_se = x\\\[offset\\\]; gix += I\\\_se \\\* (iy - iy\\\_nw) \\\* cot; giy += I\\\_se \\\* (ix - ix\\\_nw) \\\* cot; } } T gix\\\_mult = W / 2; T giy\\\_mult = H / 2; // Reduce across each simdgroup first. // This is much faster than relying purely on atomics. gix = simd\\\_sum(gix); giy = simd\\\_sum(giy); if (thread\\\_index\\\_in\\\_simdgroup == 0) { atomic\\\_fetch\\\_add\\\_explicit(&grid\\\_grad\\\[grid\\\_idx\\\], gix \\\* gix\\\_mult, memory\\\_order\\\_relaxed); atomic\\\_fetch\\\_add\\\_explicit(&grid\\\_grad\\\[grid\\\_idx + 1\\\], giy \\\* giy\\\_mult, memory\\\_order\\\_relaxed); } """ kernel = mx.fast.metal\\\_kernel( name="grid\\\_sample\\\_grad", input\\\_names=\\\["x", "grid", "cotangent"\\\], output\\\_names=\\\["x\\\_grad", "grid\\\_grad"\\\], source=source, atomic\\\_outputs=True, ) # pad the output channels to simd group size # so that our \\\`simd\\\_sum\\\`s don't overlap. simdgroup\\\_size = 32 C\\\_padded = (C + simdgroup\\\_size - 1) // simdgroup\\\_size \\\* simdgroup\\\_size grid\\\_size = B \\\* gN \\\* gM \\\* C\\\_padded outputs = kernel( inputs=\\\[x, grid, cotangent\\\], template=\\\[("T", x.dtype)\\\], output\\\_shapes=\\\[x.shape, grid.shape\\\], output\\\_dtypes=\\\[x.dtype, x.dtype\\\], grid=(grid\\\_size, 1, 1), threadgroup=(256, 1, 1), init\\\_value=0, ) return outputs\\\[0\\\], outputs\\\[1\\\] \\\`\\\`\\\` There’s an even larger speed up for the vjp: \\\`\\\\\\\`676.4ms\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`->\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`16.7ms\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`=>\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`40x\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`speed\\\\\\\`\\\`\\\\\\\` \\\\\\\`\\\`\\\\\\\`up\\\\\\\`\\\` \\\[\\\](https://ml-explore.github.io/mlx/build/html/dev/metal\\\_debugger.html "previous page") previous Metal Debugger \\\[\\\](https://ml-explore.github.io/mlx/build/html/dev/mlx\\\_in\\\_cpp.html "next page") next Using MLX in C++ Contents \\\\- \\\[Simple Example\\\](https://ml-explore.github.io/mlx/build/html/#simple-example) - \\\[Using Shape/Strides\\\](https://ml-explore.github.io/mlx/build/html/#using-shape-strides) - \\\[Complex Example\\\](https://ml-explore.github.io/mlx/build/html/#complex-example) - \\\[Grid Sample VJP\\\](https://ml-explore.github.io/mlx/build/html/#grid-sample-vjp) By MLX Contributors © Copyright 2023, MLX Contributors.
