Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/\\\_autosummary/mlx.core.value\\\_and\\\_grad.rst "Download source file") - .pdf # mlx.core.value\\\\\\\_and\\\\\\\_grad ## Contents \\\\- \\\[\\\`\\\`value\\\_and\\\_grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.core.value\\\_and\\\_grad) # mlx.core.value\\\\\\\_and\\\\\\\_grad ## Curated Notes - Functional training: wrap a pure loss function; gradients are returned, not attached to arrays. - Scalar loss: the first element of the return must be a scalar for gradients to be well‑defined. - No in‑place updates: optimizers return new parameter trees; rebind your \\\\\\\`params\\\\\\\` variable. ### PyTorch mental shift - Instead of \\\\\\\`loss.backward(); optimizer.step()\\\\\\\`, use: \\\\\\\`\\\\\\\`\\\\\\\`python value, grads = mx.value\\\\\\\_and\\\\\\\_grad(loss\\\\\\\_fn)(params, batch) params = opt.update(params, grads) \\\\\\\`\\\\\\\`\\\\\\\` ### Example \\\\\\\`\\\\\\\`\\\\\\\`python import mlx.core as mx import mlx.nn as nn from mlx.optimizers import AdamW net = nn.Sequential(nn.Linear(32, 64), nn.ReLU(), nn.Linear(64, 10)) params = net.parameters(); opt = AdamW(1e-3) def loss\\\\\\\_fn(p, x, y): logits = net.apply(p, x) return mx.mean((logits - y) \\\\\\\*\\\\\\\* 2) for step in range(1000): x = mx.random.normal((128, 32)); y = mx.random.normal((128, 10)) val, grads = mx.value\\\\\\\_and\\\\\\\_grad(loss\\\\\\\_fn)(params, x, y) params = opt.update(params, grads) \\\\\\\`\\\\\\\`\\\\\\\` \\\`value\\\\\\\_and\\\\\\\_grad\\\`(\\\\\\\*\\\`fun\\\`\\\`:\\\` \\\`Callable\\\`\\\\\\\*, \\\\\\\*\\\`argnums\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") \\\`\\\\\\\\|\\\` \\\`Sequence\\\`\\\`\\\\\\\\\\\\\\\[\\\`\\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\]\\\` \\\`\\\\\\\\|\\\` \\\[\\\`None\\\`\\\](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") \\\`\\\\=\\\` \\\`None\\\`\\\\\\\*, \\\\\\\*\\\`argnames\\\`\\\`:\\\` \\\[\\\`str\\\`\\\](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") \\\`\\\\\\\\|\\\` \\\`Sequence\\\`\\\`\\\\\\\\\\\\\\\[\\\`\\\[\\\`str\\\`\\\](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\]\\\` \\\`\\\\=\\\` \\\`\\\\\\\\\\\\\\\[\\\\\\\\\\\\\\\]\\\`\\\\\\\*) → \\\`Callable\\\` Returns a function which computes the value and gradient of \\\`\\\\\\\`fun\\\\\\\`\\\`. The function passed to \\\[\\\`\\\`value\\\_and\\\_grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.core.value\\\_and\\\_grad "mlx.core.value\\\_and\\\_grad") should return either a scalar loss or a tuple in which the first element is a scalar loss and the remaining elements can be anything. \\\`\\\`\\\`python import mlx.core as mx def mse(params, inputs, targets): outputs = forward(params, inputs) lvalue = (outputs - targets).square().mean() return lvalue # Returns lvalue, dlvalue/dparams lvalue, grads = mx.value\\\_and\\\_grad(mse)(params, inputs, targets) def lasso(params, inputs, targets, a=1.0, b=1.0): outputs = forward(params, inputs) mse = (outputs - targets).square().mean() l1 = mx.abs(outputs - targets).mean() loss = a\\\*mse + b\\\*l1 return loss, mse, l1 (loss, mse, l1), grads = mx.value\\\_and\\\_grad(lasso)(params, inputs, targets) \\\`\\\`\\\` Parameters: - \\\\\\\*\\\\\\\*fun\\\\\\\*\\\\\\\* (\\\\\\\*Callable\\\\\\\*) – A function which takes a variable number of \\\[\\\`\\\`array\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array") or trees of \\\[\\\`\\\`array\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array") and returns a scalar output \\\[\\\`\\\`array\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array") or a tuple the first element of which should be a scalar \\\[\\\`\\\`array\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array"). - \\\\\\\*\\\\\\\*argnums\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") \\\\\\\*or\\\\\\\* \\\[\\\*list\\\*\\\](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)")\\\\\\\*(\\\\\\\*\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*),\\\\\\\* \\\\\\\*optional\\\\\\\*) – Specify the index (or indices) of the positional arguments of \\\`\\\\\\\`fun\\\\\\\`\\\` to compute the gradient with respect to. If neither \\\`\\\\\\\`argnums\\\\\\\`\\\` nor \\\`\\\\\\\`argnames\\\\\\\`\\\` are provided \\\`\\\\\\\`argnums\\\\\\\`\\\` defaults to \\\`\\\\\\\`0\\\\\\\`\\\` indicating \\\`\\\\\\\`fun\\\\\\\`\\\`’s first argument. - \\\\\\\*\\\\\\\*argnames\\\\\\\*\\\\\\\* (\\\[\\\*str\\\*\\\](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)") \\\\\\\*or\\\\\\\* \\\[\\\*list\\\*\\\](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.13)")\\\\\\\*(\\\\\\\*\\\[\\\*str\\\*\\\](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.13)")\\\\\\\*),\\\\\\\* \\\\\\\*optional\\\\\\\*) – Specify keyword arguments of \\\`\\\\\\\`fun\\\\\\\`\\\` to compute gradients with respect to. It defaults to \\\\\\\\\\\\\\\[\\\\\\\\\\\\\\\] so no gradients for keyword arguments by default. Returns: A function which returns a tuple where the first element is the output of fun and the second element is the gradients w.r.t. the loss. Return type: \\\\\\\*Callable\\\\\\\* \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.grad.html "previous page") previous mlx.core.grad \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.jvp.html "next page") next mlx.core.jvp Contents \\\\- \\\[\\\`\\\`value\\\_and\\\_grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.core.value\\\_and\\\_grad) By MLX Contributors © Copyright 2023, MLX Contributors.
