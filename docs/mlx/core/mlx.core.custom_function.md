Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/\\\_autosummary/mlx.core.custom\\\_function.rst "Download source file") - .pdf # mlx.core.custom\\\\\\\_function ## Contents \\\\- \\\[\\\`\\\`custom\\\_function\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.core.custom\\\_function) - \\\[\\\`\\\`custom\\\_function.\\\_\\\_init\\\_\\\_()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.core.custom\\\_function.\\\_\\\_init\\\_\\\_) # mlx.core.custom\\\\\\\_function \\\\\\\*\\\`class\\\` \\\\\\\*\\\`custom\\\\\\\_function\\\` Set up a function for custom gradient and vmap definitions. This class is meant to be used as a function decorator. Instances are callables that behave identically to the wrapped function. However, when a function transformation is used (e.g. computing gradients using \\\[\\\`\\\`value\\\_and\\\_grad()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.value\\\_and\\\_grad.html#mlx.core.value\\\_and\\\_grad "mlx.core.value\\\_and\\\_grad")) then the functions defined via \\\`\\\\\\\`custom\\\\\\\_function.vjp()\\\\\\\`\\\`, \\\`\\\\\\\`custom\\\\\\\_function.jvp()\\\\\\\`\\\` and \\\`\\\\\\\`custom\\\\\\\_function.vmap()\\\\\\\`\\\` are used instead of the default transformation. Note, all custom transformations are optional. Undefined transformations fall back to the default behaviour. Example \\\`\\\`\\\`python import mlx.core as mx @mx.custom\\\_function def f(x, y): return mx.sin(x) \\\* y @f.vjp def f\\\_vjp(primals, cotangent, output): x, y = primals return cotan \\\* mx.cos(x) \\\* y, cotan \\\* mx.sin(x) @f.jvp def f\\\_jvp(primals, tangents): x, y = primals dx, dy = tangents return dx \\\* mx.cos(x) \\\* y + dy \\\* mx.sin(x) @f.vmap def f\\\_vmap(inputs, axes): x, y = inputs ax, ay = axes if ay != ax and ax is not None: y = y.swapaxes(ay, ax) return mx.sin(x) \\\* y, (ax or ay) \\\`\\\`\\\` All \\\`\\\\\\\`custom\\\\\\\_function\\\\\\\`\\\` instances behave as pure functions. Namely, any variables captured will be treated as constants and no gradients will be computed with respect to the captured arrays. For instance: > \\\\> > \\\`\\\`\\\`python > > import mlx.core as mx > > def g(x, y): > @mx.custom\\\_function > def f(x): > return x \\\* y > > @f.vjp > def f\\\_vjp(x, dx, fx): > # Note that we have only x, dx and fx and nothing with respect to y > raise ValueError("Abort!") > > return f(x) > > x = mx.array(2.0) > y = mx.array(3.0) > print(g(x, y)) # prints 6.0 > print(mx.grad(g)(x, y)) # Raises exception > print(mx.grad(g, argnums=1)(x, y)) # prints 0.0 > > \\\`\\\`\\\` > > \\\`\\\\\\\\\\\\\\\_\\\\\\\\\\\\\\\_init\\\\\\\\\\\\\\\_\\\\\\\\\\\\\\\_\\\`(\\\\\\\*\\\`self\\\`\\\\\\\*, \\\\\\\*\\\`f\\\`\\\`:\\\` \\\`Callable\\\`\\\\\\\*) Methods | | | |----|----| | \\\[\\\`\\\`\\\_\\\_init\\\_\\\_\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.core.custom\\\_function.\\\_\\\_init\\\_\\\_ "mlx.core.custom\\\_function.\\\_\\\_init\\\_\\\_")(self, f) | | | \\\`\\\\\\\`jvp\\\\\\\`\\\`(self, f) | Define a custom jvp for the wrapped function. | | \\\`\\\\\\\`vjp\\\\\\\`\\\`(self, f) | Define a custom vjp for the wrapped function. | | \\\`\\\\\\\`vmap\\\\\\\`\\\`(self, f) | Define a custom vectorization transformation for the wrapped function. | \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.compile.html "previous page") previous mlx.core.compile \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.disable\\\_compile.html "next page") next mlx.core.disable\\\\\\\_compile Contents \\\\- \\\[\\\`\\\`custom\\\_function\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.core.custom\\\_function) - \\\[\\\`\\\`custom\\\_function.\\\_\\\_init\\\_\\\_()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.core.custom\\\_function.\\\_\\\_init\\\_\\\_) By MLX Contributors © Copyright 2023, MLX Contributors.
