Curator's note: Prefer the human-authored MLX guides for clarity. - ../docs\\\\\\\_curated/README.md - ../docs\\\\\\\_curated/PYTORCH\\\\\\\_DISSONANCE.md - ../docs\\\\\\\_curated/NUMPY\\\\\\\_USERS.md - ../docs\\\\\\\_curated/COMMON\\\\\\\_PITFALLS.md \\\[\\\](https://github.com/ml-explore/mlx "Source repository")\\\\- \\\[.rst\\\](https://ml-explore.github.io/mlx/build/html/\\\_sources/python/\\\_autosummary/mlx.core.quantize.rst "Download source file") - .pdf # mlx.core.quantize ## Contents \\\\- \\\[\\\`\\\`quantize()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.core.quantize) # mlx.core.quantize \\\`quantize\\\`(\\\\\\\*\\\`w\\\`\\\`:\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\\\\\*, \\\\\\\*\\\`/\\\`\\\\\\\*, \\\\\\\*\\\`group\\\\\\\_size\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") \\\`\\\\=\\\` \\\`64\\\`\\\\\\\*, \\\\\\\*\\\`bits\\\`\\\`:\\\` \\\[\\\`int\\\`\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)") \\\`\\\\=\\\` \\\`4\\\`\\\\\\\*, \\\\\\\*\\\`\\\\\\\\\\\\\\\*\\\`\\\\\\\*, \\\\\\\*\\\`stream\\\`\\\`:\\\` \\\[\\\`None\\\`\\\](https://docs.python.org/3/library/constants.html#None "(in Python v3.13)") \\\`\\\\\\\\|\\\` \\\[\\\`Stream\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/stream\\\_class.html#mlx.core.Stream "mlx.core.Stream") \\\`\\\\\\\\|\\\` \\\[\\\`Device\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.Device.html#mlx.core.Device "mlx.core.Device") \\\`\\\\=\\\` \\\`None\\\`\\\\\\\*) → \\\[\\\`tuple\\\`\\\](https://docs.python.org/3/library/stdtypes.html#tuple "(in Python v3.13)")\\\`\\\\\\\\\\\\\\\[\\\`\\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\`,\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\`,\\\` \\\[\\\`array\\\`\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")\\\`\\\\\\\\\\\\\\\]\\\` Quantize the matrix \\\`\\\\\\\`w\\\\\\\`\\\` using \\\`\\\\\\\`bits\\\\\\\`\\\` bits per element. Note, every \\\`\\\\\\\`group\\\\\\\_size\\\\\\\`\\\` elements in a row of \\\`\\\\\\\`w\\\\\\\`\\\` are quantized together. Hence, number of columns of \\\`\\\\\\\`w\\\\\\\`\\\` should be divisible by \\\`\\\\\\\`group\\\\\\\_size\\\\\\\`\\\`. In particular, the rows of \\\`\\\\\\\`w\\\\\\\`\\\` are divided into groups of size \\\`\\\\\\\`group\\\\\\\_size\\\\\\\`\\\` which are quantized together. Warning \\\`\\\\\\\`quantize\\\\\\\`\\\` currently only supports 2D inputs with dimensions which are multiples of 32 Formally, for a group of \\\\\\\\\\\\\\\\g\\\\\\\\\\\\\\\\ consecutive elements \\\\\\\\\\\\\\\\w\\\\\\\_1\\\\\\\\\\\\\\\\ to \\\\\\\\\\\\\\\\w\\\\\\\_g\\\\\\\\\\\\\\\\ in a row of \\\`\\\\\\\`w\\\\\\\`\\\` we compute the quantized representation of each element \\\\\\\\\\\\\\\\\\\\\\\\hat{w\\\\\\\_i}\\\\\\\\\\\\\\\\ as follows \\\\\\\\\\\\\\\\\\\\\\\\begin{split}\\\\\\\\begin{aligned} \\\\\\\\alpha &= \\\\\\\\max\\\\\\\_i w\\\\\\\_i \\\\\\\\\\\\\\\\ \\\\\\\\beta &= \\\\\\\\min\\\\\\\_i w\\\\\\\_i \\\\\\\\\\\\\\\\ s &= \\\\\\\\frac{\\\\\\\\alpha - \\\\\\\\beta}{2^b - 1} \\\\\\\\\\\\\\\\ \\\\\\\\hat{w\\\\\\\_i} &= \\\\\\\\textrm{round}\\\\\\\\left( \\\\\\\\frac{w\\\\\\\_i - \\\\\\\\beta}{s}\\\\\\\\right). \\\\\\\\end{aligned}\\\\\\\\end{split}\\\\\\\\\\\\\\\\ After the above computation, \\\\\\\\\\\\\\\\\\\\\\\\hat{w\\\\\\\_i}\\\\\\\\\\\\\\\\ fits in \\\\\\\\\\\\\\\\b\\\\\\\\\\\\\\\\ bits and is packed in an unsigned 32-bit integer from the lower to upper bits. For instance, for 4-bit quantization we fit 8 elements in an unsigned 32 bit integer where the 1st element occupies the 4 least significant bits, the 2nd bits 4-7 etc. In order to be able to dequantize the elements of \\\`\\\\\\\`w\\\\\\\`\\\` we also need to save \\\\\\\\\\\\\\\\s\\\\\\\\\\\\\\\\ and \\\\\\\\\\\\\\\\\\\\\\\\beta\\\\\\\\\\\\\\\\ which are the returned \\\`\\\\\\\`scales\\\\\\\`\\\` and \\\`\\\\\\\`biases\\\\\\\`\\\` respectively. Parameters: - \\\\\\\*\\\\\\\*w\\\\\\\*\\\\\\\* (\\\[\\\*array\\\*\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.array.html#mlx.core.array "mlx.core.array")) – Matrix to be quantized - \\\\\\\*\\\\\\\*group\\\\\\\_size\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The size of the group in \\\`\\\\\\\`w\\\\\\\`\\\` that shares a scale and bias. Default: \\\`\\\\\\\`64\\\\\\\`\\\`. - \\\\\\\*\\\\\\\*bits\\\\\\\*\\\\\\\* (\\\[\\\*int\\\*\\\](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")\\\\\\\*,\\\\\\\* \\\\\\\*optional\\\\\\\*) – The number of bits occupied by each element of \\\`\\\\\\\`w\\\\\\\`\\\` in the returned quantized matrix. Default: \\\`\\\\\\\`4\\\\\\\`\\\`. Returns: A tuple containing - w\\\\\\\_q (array): The quantized version of \\\`\\\\\\\`w\\\\\\\`\\\` - scales (array): The scale to multiply each element with, namely \\\\\\\\\\\\\\\\s\\\\\\\\\\\\\\\\ - biases (array): The biases to add to each element, namely \\\\\\\\\\\\\\\\\\\\\\\\beta\\\\\\\\\\\\\\\\ Return type: \\\[\\\*tuple\\\*\\\](https://docs.python.org/3/library/stdtypes.html#tuple "(in Python v3.13)") \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.put\\\_along\\\_axis.html "previous page") previous mlx.core.put\\\\\\\_along\\\\\\\_axis \\\[\\\](https://ml-explore.github.io/mlx/build/html/python/\\\_autosummary/mlx.core.quantized\\\_matmul.html "next page") next mlx.core.quantized\\\\\\\_matmul Contents \\\\- \\\[\\\`\\\`quantize()\\\`\\\`\\\](https://ml-explore.github.io/mlx/build/html/#mlx.core.quantize) By MLX Contributors © Copyright 2023, MLX Contributors.
